{
  "nodes": [
    {
      "id": 0,
      "text": "A systematic literature review can map, taxonomy, evaluate and identify gaps in interpretable and explainable machine learning methods for predictive process monitoring to support trustworthy, transparent predictive process analytics",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        9,
        13
      ]
    },
    {
      "id": 1,
      "text": "We conducted a PRISMA-guided systematic literature review covering 107 studies up to 2025, using queries across ACM, AIS, IEEE, ScienceDirect and SpringerLink with forward/backward search and template analysis tied to eight research questions (RQ1-RQ8)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        7,
        14
      ]
    },
    {
      "id": 2,
      "text": "We present a unified taxonomy distinguishing intrinsically interpretable models (decision trees, rule-based, regression, Bayesian, k-NN, GAMs) from black-box models (deep learning, gradient boosting, random forests) and classify post-hoc explanations by scope (local/global) and relation (model-agnostic/model-specific)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 3,
      "text": "Application domains and benchmark datasets are concentrated: finance, healthcare, customer support and manufacturing dominate; BPIC series and a few others (Helpdesk, Sepsis, Production, Road Traffic Fine) are most frequently used, with BPIC used in 57% of studies",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        7
      ]
    },
    {
      "id": 4,
      "text": "Primary predictive tasks in the literature are process outcome prediction, next-event prediction, time-related PPI prediction and other PPI predictions, with process outcome prediction the most common",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7
      ]
    },
    {
      "id": 5,
      "text": "There is a shift toward complex black-box models for higher accuracy, with many studies relying on post-hoc XAI methods (SHAP, LIME, ICE, PDP, counterfactuals, LRP) to explain predictions",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        7
      ]
    },
    {
      "id": 6,
      "text": "Evaluation of explanations is weak and fragmented: most papers prioritize predictive performance; only a minority perform quantitative or qualitative explanation evaluations and human-grounded studies are scarce",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": [
        7,
        9
      ]
    },
    {
      "id": 7,
      "text": "Empirical evidence from the review: 107 included studies; 64 use interpretable (white-box) models, 59 use opaque models; 61 studies used at least one BPIC dataset; 65 studies examine process outcome prediction, 30 next-event and 32 regression tasks",
      "role": "Evidence",
      "parents": [
        1,
        3,
        4,
        5,
        6
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Post-hoc explanation methods and formats applied in PPM include local methods (LIME, SHAP, ICE, counterfactuals, LRP) and global methods (PDP, aggregated SHAP, feature importance, surrogate models); formats are numeric, rule-based, textual and visual",
      "role": "Result",
      "parents": [
        2,
        5
      ],
      "children": [
        6
      ]
    },
    {
      "id": 9,
      "text": "Key gaps and open issues identified: insufficient evaluation standards, dataset biases and transferability limits, under-explored synthesis of XAI with trustworthy AI aspects (uncertainty, privacy, fairness), and limited human-centered validation",
      "role": "Claim",
      "parents": [
        0,
        6
      ],
      "children": [
        10,
        11,
        12
      ]
    },
    {
      "id": 10,
      "text": "Future research priority: integrate XAI with other trustworthy AI methods—uncertainty quantification, privacy-preserving techniques and fairness-aware explanations—to produce explanations that communicate confidence, protect data and reveal bias",
      "role": "Claim",
      "parents": [
        9
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Future research priority: develop LLM-enabled and conversational explanation systems and retrieval-augmented pipelines to produce natural-language, multimodal, domain-aware explanations for predictive process monitoring",
      "role": "Claim",
      "parents": [
        9
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Future research priority: build low-latency, adaptive explanation methods for streaming event data and create XAI approaches for object-centric process mining to explain cross-object dependencies and multi-entity predictions",
      "role": "Claim",
      "parents": [
        9
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Practical implications: practitioners should match domain, data encoding and explanation style to stakeholders, critically assess fidelity and stability of explanations, and prioritize human-grounded evaluation before deployment",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Limitations of the review: database and query scope, possible omission of non-indexed or non-English work, and reliance on reported evaluation results in primary studies",
      "role": "Limitation",
      "parents": [
        1
      ],
      "children": null
    }
  ]
}