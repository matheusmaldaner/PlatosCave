{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable and interpretable AI methods can improve trustworthiness, transparency and operational integration of predictive process monitoring (PPM) models",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6
      ]
    },
    {
      "id": 1,
      "text": "The literature on explainable and interpretable PPM is fragmented and needs a systematic synthesis to identify trends, gaps and actionable guidance",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 2,
      "text": "We conduct a PRISMA-based systematic literature review covering the last decade (including 2025 works) with structured search queries, eligibility criteria and template analysis to extract evidence",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        7,
        8,
        12
      ]
    },
    {
      "id": 3,
      "text": "We produce a unified taxonomy separating intrinsically interpretable models (rules, trees, regression, Bayesian, k-NN, GAMs) from black-box models that require post-hoc explanations (deep learning, gradient boosting, random forest) and classify explanation methods by scope and relation (local/global, model-agnostic/model-specific)",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 4,
      "text": "The review maps application domains, benchmark datasets and prediction tasks, linking domain characteristics to model selection and explanation needs",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "Key contributions include: consolidated panorama of the field, unified taxonomy, systematic evaluation audit, evidence-based research agenda and practitioner guidance",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        13
      ]
    },
    {
      "id": 6,
      "text": "Final synthesis identifies major research gaps: weak evaluation practices, dataset biases and under-explored directions such as encoding impact, object-centric process mining and streaming explanations",
      "role": "Conclusion",
      "parents": [
        1
      ],
      "children": [
        14
      ]
    },
    {
      "id": 7,
      "text": "Search and selection: queried ACM, AIS, IEEE, ScienceDirect, SpringerLink with combined process+XAI queries, applied inclusion/exclusion rules and forward/backward search",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Data extraction and synthesis used a standardized form and template analysis aligned to eight research questions (RQ1â€“RQ8) to code application domain, datasets, tasks, models, XAI methods and evaluation practices",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Common post-hoc explanation methods observed are SHAP, LIME, ICE, PDP, counterfactuals and model-specific techniques like LRP and DeepLIFT; explanation outputs vary (numeric, textual, rule-based, visual)",
      "role": "Result",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Interpretable models (e.g., decision trees, regression, Bayesian networks) are still used but black-box models (notably LSTMs/DNNs, XGBoost, RF) are prevalent due to higher predictive performance, creating demand for post-hoc XAI",
      "role": "Result",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Empirical landscape: 107 included studies; most-addressed domains are finance, healthcare, customer support and manufacturing; BPIC datasets dominate benchmarking (BPIC 2012 most used), tasks concentrate on process outcome, next-event and time-related predictions",
      "role": "Evidence",
      "parents": [
        4,
        2
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Study corpus and screening: initial retrieval 1,415 records, screened to 136 full texts and 107 included after eligibility assessment following PRISMA",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Actionable guidance matches domain, data availability and explanation needs to suitable XAI approaches to promote transparency, reliability and user trust in real-world PPM deployments",
      "role": "Conclusion",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Evaluations are insufficient: majority of papers do not formally evaluate explanations; only a small subset report quantitative or qualitative assessments and very few combine both or include human-grounded studies",
      "role": "Evidence",
      "parents": [
        6,
        2
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Research agenda and future directions: integrate XAI with uncertainty quantification, privacy and fairness methods; explore LLM-driven conversational explanations and multimodal outputs; develop low-latency streaming explanations and object-centric explanation techniques; standardize evaluation frameworks",
      "role": "Claim",
      "parents": [
        6,
        9,
        10
      ],
      "children": null
    },
    {
      "id": 16,
      "text": "Limitations of this SLR include potential omissions due to chosen databases and search terms and the reliance on reported information in included articles",
      "role": "Limitation",
      "parents": [
        2
      ],
      "children": null
    }
  ]
}