{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable and interpretable ML methods can make predictive process monitoring (PPM) models more trustworthy, transparent and actionable for researchers and practitioners",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        9,
        12
      ]
    },
    {
      "id": 1,
      "text": "Systematic literature review conducted using PRISMA protocol across multiple databases with template analysis to synthesize studies (search, inclusion/exclusion, forward/backward search, data extraction form)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2,
        4,
        5
      ]
    },
    {
      "id": 2,
      "text": "Formal foundations and definitions for PPM were provided (events, traces, event logs, feature extraction, labeling, supervised learning and standard prediction tasks)",
      "role": "Context",
      "parents": [
        0,
        1
      ],
      "children": [
        7,
        11
      ]
    },
    {
      "id": 3,
      "text": "Template-driven qualitative synthesis aligned to eight research questions (RQ1-RQ8) organized evidence into application domains, datasets, tasks, interpretable models, post-hoc methods and evaluation paradigms",
      "role": "Method",
      "parents": [
        0,
        1
      ],
      "children": [
        6,
        9,
        12
      ]
    },
    {
      "id": 4,
      "text": "Corpus selection and descriptive result: 1,415 records screened -> 136 full texts -> 107 included studies (53 journal, 51 conference, 3 arXiv); majority published 2020 onward",
      "role": "Result",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Data extraction and encoding procedures documented: feature extraction functions and labeling definitions formalized for event- and trace-level supervised learning",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        2
      ]
    },
    {
      "id": 6,
      "text": "Application domains in reviewed studies concentrate on finance (most frequent), healthcare, customer support and manufacturing; ~39% evaluated cross-domain datasets, ~42% domain-agnostic",
      "role": "Result",
      "parents": [
        3
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 7,
      "text": "Application tasks distribution: process outcome prediction dominant (~65 studies), next-event prediction (~30 studies), time-related regression (~23 studies) and other PPI predictions; classification prevalent over regression",
      "role": "Result",
      "parents": [
        2,
        6
      ],
      "children": [
        9
      ]
    },
    {
      "id": 8,
      "text": "Benchmark datasets are dominated by BPIC logs (used in 57% of studies) with BPIC2012 most used, producing a dataset-driven bias toward finance and limiting generalizability",
      "role": "Evidence",
      "parents": [
        6
      ],
      "children": [
        9,
        12
      ]
    },
    {
      "id": 9,
      "text": "Model landscape: many studies use intrinsically interpretable models (decision trees, logistic/linear regression, Bayesian networks) while a large portion use black-box models (deep learning, gradient boosting, random forest), creating a trade-off between accuracy and transparency",
      "role": "Claim",
      "parents": [
        0,
        3,
        7,
        8
      ],
      "children": [
        10,
        11,
        12
      ]
    },
    {
      "id": 10,
      "text": "Post-hoc explanation techniques widely applied to black-box PPM models: SHAP, LIME, ICE, PDP, counterfactuals and model-specific methods (LRP, DeepLIFT, TreeSHAP); techniques categorized by local vs global and model-agnostic vs model-specific",
      "role": "Claim",
      "parents": [
        9
      ],
      "children": [
        12
      ]
    },
    {
      "id": 11,
      "text": "White-box approaches: decision trees were the most prevalent intrinsically interpretable models (22 of white-box studies); other transparent methods include rule-based systems, GAMs and k-NN",
      "role": "Evidence",
      "parents": [
        9,
        2
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Evaluation gap and limitations: majority of studies do not rigorously evaluate explanation quality; only ~18 used quantitative or qualitative evaluation, very few human-grounded studies, inconsistent metrics (fidelity, stability, sparsity) and lack of standardized evaluation pipelines",
      "role": "Claim",
      "parents": [
        0,
        3,
        8,
        9,
        10,
        11
      ],
      "children": [
        13
      ]
    },
    {
      "id": 13,
      "text": "Identified challenges and research agenda: need for rigorous, multi-faceted evaluation frameworks, attention to data encoding impacts, support for object-centric and streaming event data, integration of uncertainty/fairness/privacy, and exploration of LLM-driven conversational/multi-modal explanations",
      "role": "Conclusion",
      "parents": [
        12,
        9,
        10,
        8
      ],
      "children": null
    }
  ]
}