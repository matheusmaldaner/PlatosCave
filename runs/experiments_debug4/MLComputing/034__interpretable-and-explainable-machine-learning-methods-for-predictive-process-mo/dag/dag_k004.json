{
  "nodes": [
    {
      "id": 0,
      "text": "A systematic literature review can provide a comprehensive, structured synthesis of interpretable and explainable machine learning methods applied to predictive process monitoring, identify trends and gaps, and propose a research agenda",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 1,
      "text": "We conducted a PRISMA-guided systematic literature review covering the past decade (including works through 2025) using multi-database queries, forward/backward search, eligibility criteria and template analysis to answer eight research questions",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8,
        9
      ]
    },
    {
      "id": 2,
      "text": "Predictive process monitoring (PPM) transforms event logs (events, traces, event logs) into supervised learning datasets via feature extraction and labeling to predict next events, outcomes, remaining time and other PPIs",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 3,
      "text": "The literature on explainable and interpretable PPM is fragmented and rapidly growing, making it difficult to assess methods, datasets and evaluation practices without a structured review",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        1,
        11
      ]
    },
    {
      "id": 4,
      "text": "There is a methodological taxonomy distinguishing intrinsically interpretable models (decision trees, rule-based, regression, Bayesian, k-NN, GAMs) from black-box models (deep learning, gradient boosting, random forest) that require post-hoc explanations (local and global; model-agnostic and model-specific)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        9
      ]
    },
    {
      "id": 5,
      "text": "Common post-hoc explanation methods used in PPM include local methods (LIME, SHAP, ICE, layerwise relevance propagation, counterfactuals) and global methods (PDP, ALE, feature importance, surrogate models), delivered in numeric, textual, rule-based and visual formats",
      "role": "Claim",
      "parents": [
        2,
        4
      ],
      "children": [
        9
      ]
    },
    {
      "id": 6,
      "text": "Search and selection yielded 1,415 candidate records, screened to 136 full texts, with 107 studies meeting inclusion criteria and forming the review corpus",
      "role": "Result",
      "parents": [
        1
      ],
      "children": [
        7
      ]
    },
    {
      "id": 7,
      "text": "Descriptive analysis of the 107 included studies: 53 journal articles, 51 conference papers, 3 arXiv preprints; most publications were from 2020 onward with peaks in 2020, 2022 and 2024",
      "role": "Result",
      "parents": [
        1,
        6
      ],
      "children": [
        8
      ]
    },
    {
      "id": 8,
      "text": "Application domains, datasets and tasks: finance, healthcare, customer support and manufacturing dominate; BPIC and Helpdesk/Sepsis/Production logs are most used; process outcome prediction is most common, followed by next-event and time-related PPI prediction",
      "role": "Result",
      "parents": [
        1,
        2,
        6,
        7
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Observed methodological trends: (a) many studies shifted toward complex black-box models for higher predictive accuracy but rely on post-hoc XAI (SHAP, LIME, ICE), (b) intrinsically interpretable models (DTs, linear/logistic regression, Bayesian networks) remain widely used where transparency is crucial",
      "role": "Claim",
      "parents": [
        1,
        4,
        5
      ],
      "children": [
        10
      ]
    },
    {
      "id": 10,
      "text": "A major evaluation gap: most papers do not formally evaluate explanations; of 107 studies only 18 performed quantitative or qualitative evaluation and only 2 combined both; common evaluation metrics when used include fidelity, stability, sparsity, functional complexity and parsimony",
      "role": "Evidence",
      "parents": [
        1,
        9
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Conclusions and research agenda: future work should prioritize rigorous, multi-faceted evaluation (functional, application-grounded, human-grounded), integrate trustworthy-AI aspects (uncertainty, privacy, fairness), extend XAI to object-centric process mining, streaming event data and LLM-driven conversational/explanations, and address encoding and transferability issues",
      "role": "Conclusion",
      "parents": [
        0,
        3,
        9,
        10
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Limitation: the SLR is bounded by chosen databases, search terms and inclusion rules so some relevant works may be omitted despite forward/backward searches",
      "role": "Limitation",
      "parents": [
        1,
        6
      ],
      "children": null
    }
  ]
}