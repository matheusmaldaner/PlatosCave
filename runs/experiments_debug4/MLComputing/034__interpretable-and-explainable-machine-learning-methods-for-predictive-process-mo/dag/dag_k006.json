{
  "nodes": [
    {
      "id": 0,
      "text": "A systematic literature review can characterize and synthesize methods, datasets, tasks, evaluation practices, and open challenges for interpretable and explainable machine learning in predictive process monitoring to guide research and practice",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6
      ]
    },
    {
      "id": 1,
      "text": "We performed a PRISMA-aligned systematic literature review gathering 1,415 records, screened to 107 included studies (peer-reviewed journals, conferences, arXiv) for analysis",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        7
      ]
    },
    {
      "id": 2,
      "text": "Formal foundations and definitions for predictive process monitoring were established (events, traces, event logs, feature extraction, labeling, supervised learning, task definitions for outcome, next-event, and PPI prediction)",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 3,
      "text": "The literature landscape: finance, healthcare, customer support and manufacturing dominate application domains; BPIC family of event logs and Helpdesk, Production, Sepsis, Road Traffic Fine are the most used datasets",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 4,
      "text": "Model and XAI taxonomy: intrinsic interpretable models (decision trees, rule/regression/Bayesian/k-NN/GAM) versus black-box models (deep learning, gradient boosting, random forest) requiring post-hoc explanations (local/global, model-agnostic/specific)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 5,
      "text": "Empirical findings on tasks and method usage: process outcome prediction is most prevalent (65/107), next-event prediction (30/107), time-related regression tasks (~23), with a roughly balanced use of white-box (64) and black-box (59) approaches among included explainable works",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 6,
      "text": "Evaluation practices and gaps: majority of studies prioritize predictive performance, many omit formal explanation evaluation, only a minority use quantitative or qualitative evaluation and very few perform human-grounded studies leading to a reproducibility and utility gap",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 7,
      "text": "Search strategy and eligibility: multi-database queries combining process mining and explainability terms, inclusion/exclusion criteria and forward/backward snowballing yielded the final corpus",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Formalization supports mapping event-log constructs into supervised learning datasets (feature extraction phi, labeling functions resp_event/resp_trace, dataset partitioning and predictive model definitions)",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "BPIC datasets are dominant (used in 57% of articles), BPIC 2012 most frequent (26/107), dataset availability biases domain focus (finance) and benchmarking; ~39% of papers evaluated approaches on multiple domains/datasets",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Post-hoc XAI techniques commonly used include SHAP, LIME, ICE, PDP, permutation feature importance, counterfactual explanations and model-specific methods (LRP, DeepLIFT, TreeSHAP); explanation outputs are numeric, textual, rule-based and visual",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "White-box approaches most used: decision trees (22/64 white-box), logistic/linear regression and Bayesian networks; black-box prevalence: deep learning (38/59 black-box), gradient boosting (26), random forests (16); many studies combine methods and surrogates",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Common quantitative explanation metrics reported include fidelity, stability, sparsity, proximity, plausibility and functional complexity, but adoption is inconsistent and often limited to a subset of studies",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Qualitative and human-grounded evaluations are rare; when present they include user studies, task difficulty, usability questionnaires and domain-expert appraisal, revealing a shortage of evidence about real-world usefulness of explanations",
      "role": "Evidence",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Identified challenges and open research directions: (a) encoding and data-preprocessing choices fundamentally affect explainability and must be systematized, (b) object-centric process mining requires new multi-object explanation methods, (c) streaming/real-time explanation generation needs low-latency XAI, (d) integrate XAI with trustworthy AI (uncertainty, privacy, fairness), and (e) explore LLM-enabled conversational and multimodal explanations",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Conclusions and recommendations: field shifted from white-box to black-box models with reliance on post-hoc XAI; urgent need for standardized evaluation frameworks, more human-grounded studies, research on encoding, object-centric and streaming scenarios, and integrated trustworthy XAI to make explanations meaningful and actionable",
      "role": "Conclusion",
      "parents": [
        1,
        3,
        4,
        5,
        6,
        14
      ],
      "children": null
    }
  ]
}