--- Page 1 ---
Towards Trustworthy and Aligned Machine Learning:
A Data-centric Survey with Causality Perspectives
Haoyang Liu†,
Maheep Chaudhary†∗, and Haohan Wang
School of Information Sciences,
University of Illinois Urbana-Champaign
{hl57, haohanw}@illinois.edu, maheep001@e.ntu.edu.sg
† equal contribution
Abstract
The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing
various applications and research areas such as robustness, security, interpretability, and fairness.
Over the past decade, dedicated efforts have been made to address these issues, resulting in a
proliferation of methods tailored to each specific challenge.
In this survey paper, we provide a
systematic overview of the technical advancements in trustworthy machine learning, focusing on
robustness, adversarial robustness, interpretability, and fairness from a data-centric perspective, as
we believe that achieving trustworthiness in machine learning often involves overcoming challenges
posed by the data structures that traditional empirical risk minimization (ERM) training cannot
resolve.
Interestingly, we observe a convergence of methods introduced from this perspective, despite their
development as independent solutions across various subfields of trustworthy machine learning. Fur-
thermore, we find that Pearl’s hierarchy of causality serves as a unifying framework for categorizing
these techniques. Consequently, this survey first presents the background of trustworthy machine
learning development using a unified set of concepts, connects this unified language to Pearl’s hi-
erarchy of causality, and finally discusses methods explicitly inspired by causality literature.
By
doing so, we established a unified language with mathematical vocabulary as a principled connection
between these methods across robustness, adversarial robustness, interpretability, and fairness under
a data-centric perspective, fostering a more cohesive understanding of the field.
Further, we extend our study to the trustworthiness of large pretrained models. We first present
a brief summary of the dominant techniques in these models, such as fine-tuning, parameter-efficient
fine-tuning, prompting, and reinforcement learning with human feedback. We then connect these
techniques with standard ERM, upon which previous trustworthy machine learning solutions were
built.
This connection allows us to immediately build upon the principled understanding of the
trustworthy method established in previous sections, applying it to these new techniques in large
pretrained models, openning up possibilities for many new methods. We also survey the current
existing methods under this perspective.
Finally, we offer a brief summary of the applications of these methods and also discuss about
some future aspects relating to our survey1.
1
Introduction
The rapid advancements and widespread adoption of machine learning (ML) have led to its integration
into various applications, ranging from healthcare and finance to autonomous vehicles and social media.
As these applications become increasingly complex and consequential, the trustworthiness of ML systems
has emerged as a crucial factor in ensuring their reliability, safety, and societal impact. Trustworthy ML is
a multifaceted concept, encompassing a broad range of research areas. Over the past decade, substantial
efforts have been made to address these challenges, resulting in a multitude of specialized methods
designed to tackle specific aspects of trustworthiness.
∗work done as remote intern, originally with Bundelkhand Institute of Engineering and Technology, joining Nanyang
Technological University.
1For more information, please visit trustAI.one
1
arXiv:2307.16851v1  [cs.LG]  31 Jul 2023


--- Page 2 ---
label
what a vanilla model learns
Features
“spurious”
“causal”
association
(defined by nature or human)
observable
unobserved
confounder
correlation 
Figure 1: The core problem this survey paper scopes. We use the terms “causal” and “spurious” in
quotes because the nature of these preferred features might depend on the context. For example, in the
robustness generalization setting, the preferred features are usually the “causal” features, but in fairness
discussion, the preferred features do not necessarily have to be “causal” features in the typical causality
sense, although the mathematical tools used can be the same.
While the scope of trustworthy ML encompasses multiple areas, in this survey, we chose to concentrate
on the topics of robustness, security, interpretability, and fairness due to their significant impact as well
as their internal connections. By concentrating on the technical aspects of these topics, we strive to
provide a comprehensive understanding of the state-of-the-art methods and techniques that have been
developed to address these challenges.
While concentrating on the technical aspects, it is important to note the diversity of the methods
from different perspectives. Among a rich set of methods introduced in recent years, our survey primarily
focuses on the machine learning technical aspects of the methods from a data-centric perspective.
Data-centric Perspective
By data-centric perspective, we refer to the understanding that believes
the challenges toward trustworthy machine learning lies in the structure of the available data provided to
train the models. The existence of such structures, such as spurious features [244, 479, 659], confounding
factors [32, 179, 528], dataset bias [256, 517, 518], often leads to the consequence that vanilla training
with empirical risk minimization (ERM) will often generate models that capture such undesired signals
from the data.
Following this data-centric perspective, we introduce a concrete example for the following-up discus-
sions throughout this survey.
Example 1. In a classification task, considering the problem of classifying images of sea turtles and
tortoise [540]. To address this task, images are labelled for both types of animals by human annotators
reading the images. However, sea turtles typically live in the sea while tortoises live in various environ-
ments. As a result, most images of sea turtles will have a blue sea background while images of the other
class can have various backgrounds, creating strong correlations between the color of the backgrounds
and the label. Thus, an ERM trained model will likely pick up the backgrounds of the images due to
the strongly correlated signals, while, on the other hand, a marine biologist will suggest we classify the
images through their feet or shell.
Example 1 is a straightforward demonstration for the widely-accepted remark “Correlation is not
Causation”. While this remark is not new to the statistics and machine learning communities, numerous
efforts have nonetheless been devoted to improve the model’s predictive performances over benchmark
datasets, resulting in techniques selected and favored regardless of whether the models are learning causal
features or spurious features. Then the consequence should be easy to anticipate: when these trained
models are deployed in the real-world where the spurious features can be different from those in lab, the
models will underperform.
There exist several explanations that potentially account for the performance disparities between the
benchmark settings and the real-world settings [487, 527, 635], but here we will devote the discussion
to the data-centric understanding: the performance gap is due to the fact that the models are learning
2


--- Page 3 ---
through the spurious features in the datasets, resulting in some over-estimated performances through
the benchmarks [163, 547, 561].
Illustrated in Figure 1 and as previously hypothesized by [547], the scope of this survey considers the
root of the undesired performances in the real-world applications lies in the data: within the collected
data used to train the supervised machine learning models, there exist some spurious features that are
associated with the label, therefore, if a vanilla statistical model cannot differentiate the spurious features
from the desired ones, the model will learn both the features and result in some undesired performances.
It is also worth mentioning that the turtles vs. tortoise is not the only example used to highlight this
issue in the literature. To the best of our knowledge, the first example in the deep learning era is the
discussion on how the snow background is playing a role in Husky vs. wolf classification [427], and then
the community also often motivates their discussion with examples such as how the habitat plays a role
in frog vs. animals without swamp scenes classification [36] or camel vs. cow classification [503], how
the fisher is significantly correlated with fish classes in ImageNet classification [60].
The Role of Causality
Fortunately, for the problem in Example 1, the solution is fairly apparent:
causal analysis is a field that studies the systematic way of understanding the causal relationship from
data while staying least influenced by the statistical signals raised by spurious features or confounding
factors.
Thus, it seems to incorporate the established solutions of causal analysis into current deep
learning solutions is a direction to solve the problem.
Before we dive into the world of causal analysis, we find it beneficial to first clarify the role causality
plays in machine learning with static datasets that are already collected beforehand. With the example
of image classification, a question is often raised on “whether the pixels causes the label or the label
causes the pixel.” We consider neither of these directions is nature enough to serve the discussion, and
we tend to formalize the problem in a way that it is the pixels of the images that cause the human
annotators of the dataset to label each image in a certain way.
With this setup of the problem, we can leverage the established concepts and solutions in causal
analysis for machine learning problems. This survey aims to serve the role of summarizing the recent
works incorporating the concepts and techniques from causal analysis for improving the robustness or
interpretability of machine learning models, either when these works explicitly mention the inspiration
or when these works potentially invent these techniques independently. As we will demonstrate soon,
while most of the methods are invented independently, they converge to the same statistical language,
and the same statistical language is connected to Pearl’s causal hierarchy.
Survey Contents
Therefore, our survey has the following main components (Figure 2):
• In Section 2, we will recapitulate the current state of the machine learning and deep learning
techniques with an emphasis on the evidence suggesting the need of trustworthy machine learning
in the real-world. The section also serves the goal of delineating the background problems we are
interested in surveying the techniques for, including
– Robustness in generalization: In this survey, we further branch it into categories such as
domain adaptation, domain generalization, and learning with the existence of dataset bias.
We scope this topic as the study of how to maintain the predictive performances over additional
datasets that human users consider similar.
– Adversarial Robustness (Security): In this survey, we scope the topic regarding security mostly
under the discussion of adversarial attack and defense, a study about how to carefully crafted
certain noises imperceptible to human users but able to alter the model predictions, and a
study about how to defend such noises.
– Interpretability: In this survey, we use the term exchangeably with explainability, and consider
it a study about how to translate the statistical decision process of models to users. We mostly
focus on explaining the importance of features used by models.
– Fairness: In this survey, we will discuss both fairness under the categories of outcome discrim-
ination and quality disparity. We focus mostly on the technical aspects of the designing of the
models given an established fairness criterion, instead of the design of such fairness criterion.
While introducing these machine learning methods under each topic, we also condense the ideas
behind each method to its mathematical backbone and demonstrate a conceptual principled un-
derstanding of these methods through mathematical unification.
3


--- Page 4 ---
• Section 3 attempts to give the reader a thorough overview of various causality notions and a
summary of the deep learning methods explicitly inspired by such notions. Overall, this section
has been divided mostly into three key parts:
– The background of causality is summarised in Section 3.1 using the structural causal model
(SCM) and Pearl Hierarchy. Our vision is primarily focused on the debate surrounding the
“confounders,” who are the principal villains in the quest for trustworthy machine learning.
Additionally, we outline the fundamental issue with machine learning that makes confounding
variables more likely to appear in the observed data and describe its effect on the estimated
probability of output.
– Several causal notions that fall under the second level of causation, L2, are defined in Sec-
tion 3.2; these concepts include the randomized controlled trial (RCT), instrument variable
(IV), backdoor method, and front door method. Furthermore, we discuss the different works
under the umbrella of the technique employed by these works to de-confound the machine
learning model.
– Finally, we define the ideas that make up the third level of causation L3 in Section 3.3. We
also describe the ideas of treatment effects because they are partially based on L3. These
notions are defined to a greater extent by exploring their employment in different machine
learning works.
Along with the introduction of these causality concepts, we will also delineate the machine learning
methods that are explicitly supported by these concepts, and link these ideas back to the principled
understanding in the previous section.
• In section 4, we put our discussions from the above two sections into the context of large pretrained
models.
– We first shift the views from the standalone models into a new paradigm of large pretrained
models by offering summaries of the techniques dominating in this paradigm, such as fine-
tuning, parameter-efficient fine-tuning, and prompting. Following this introduction, we will
condense the fundamental concepts behind these techniques into the mathematical language
of a typical ERM loss.
– The essence of this summary allows us to apply the techniques discussed in prior sections to
the realm of large pretrained models. To a certain extent, our mathematical summary holds
the potential to predict future trustworthy machine learning methods that will be invented in
the context of large pretrained models. We also offer a survey of the existing methods before
our summary predicts.
• Section 5 summarizes the application of these techniques, mainly categorized as vision, language,
and vision-language applications.
• In Section 6, we will conclude with more explicit discussions of the sections above, and briefly
discuss the potential future aspects under each perspective, and the unification language as a
whole.
Notations
Throughout the survey, we aim to expand the narrative with two intertwined main threads:
one is an intuitive explanation of the high-level ideas that can help the readers to quickly understand the
core innovation from each paper, and the other is a formalized discussion that aims to offer a rigorous
delineation of the methods through our master equation summarising all the papers under each section.
representation/embedding
In our
survey paper,
we will use the
terms representation and embed-
ding
interchangeably,
and
use
both of them to refer to the in-
termediate results generated by
the model, which encode raw data
into a more abstract form.
Thus, to serve our second goal, we first introduce the notations
here. Due to the nature of our paper, we aim to expand the discussion
from both machine learning perspective and the causality perspective,
we will introduce the notations used in each domain separately. From
the machine learning perspectives, our notations mostly serve the pur-
poses to describe how to train the models with regularizations over
the datasets used. Thus, We will use (X, Y) to denote a dataset of n
samples, with each samples denoted as (x, y). We will use P to denote
distributions of random variables. We will use ℓ(·, ·) to denote a generic loss function, and we will use
4


--- Page 5 ---
argmin
!
1
(
)
(#,%)∈((,))
max#!;, #!,# -.*(, -/; / , 2)
argmin
!
1
(
)
(#,%)∈((,))
*(,(-; /)) , 2) −4*(ℎ(,* -; / ), 6) 
argmin
!
)
(#,%)∈((,))
9(-, 2, /)*(/ - , 2)
Domain Generalization
Fairness
(Outcome Discrimination)
Beyond Domain Generalization 
(bias in data)
Domain Adaptation
Fairness
(Quality Disparity)
Adversarial Robustness
Interpretability
(gradient-based)
Interpretability
(removal-based)
Master Equation
Master Equation
ERM
argmin
!
)
(#,%)∈((,))
*(/ - , 2)
Master Equation
Association
Intervention
Counterfactual
Figure 2: Summary of the major topics surveyed in this paper. Blue boxes: machine learning topics in
trustworthy ML scoped by our survey; Red boxes: core techniques and master equations we summarized;
Green boxs: the causality layers these techniques build upon.
category and rule
example
notation explanation
relationships
data
X
data space
x ∈X
Y
label space
y ∈Y
(X, Y)
a dataset of features and label
(x, y) ∈(X, Y)
(x, y)
a sample of features and label
C
set of indices of causal features
¯C
the complement set of C
ϵ
exogenous variables acting as noise in
the generative models
(Z, ∅)
a dataset of feature set Z with corre-
sponding labels unavailable
Z ⊂X
z
a sample of features
z ∈Z
model is denoted with a
small-case letter followed
by input placeholder · and
a greek letter for its
parameters
f(·; θ)
a model with parameters θ
f(·; θ) : X →Y
fk(·; θ)
first k layers of f(·; θ)
dom(f0(·; θ)) means X
h(·; ϕ)
a model with parameters ϕ
h(·; ϕ) : dom(fk+1(·; θ)) →Y
g(·; ψ)
a model with parameters ψ
see below
F(·)
a random function
random variables are
denoted by capitalized
letters
X
random variable for features
Y
random variable for label
C
random variable for causal features
¯C
random variable for non-causal features
˜C
random variable for non-causal, but statistically related features (confounder)
G
Directed Acyclic Graph
U
exogenous variable in the graph
V
endogenous variable in the graph
PA
parents of a variable in the graph
values are denoted by
small case letters
x
value of X
y
value of Y
c
value of C
¯c
value of ¯C
˜c
value of ˜C
probability
P(X)
distribution of a random variable X
P(X = x)
probability of X = x for discrete random variable X, also denoted as P(x)
symbols denoting special
conditions
A ⊥⊥B
A is independent of B
G ¯
X,¯Y
all arrows out of Y are removed and all the arrows coming to X are removed
in the original graph G
Table 1: A summary of major notations we use throughout this survey. Due to the limitation of space,
we define g(·; ψ) here as g(·; ψ) : dom(fk+1(·; θ)) →dom(fk′(·; θ)), where k′ ≤k.
5


--- Page 6 ---
f(·; θ) to denote a function with the parameters θ, with the entire hypothesis denoted as Θ. Correspond-
ingly, we will use h(·; ϕ) to denote another model that usually plays the role to assist the training of
f(·; θ), and the input of h(·; ϕ) is usually fk(·; θ) which means the kth layer’s output (i.e., the represen-
tation/embedding) from f(·; θ); in addition, we will use g(·; ψ) to denote a model that usually plays the
role of generating data or internal representations, which is a function whose output space is the same
as fk(·; θ), and we use f0(·; θ) to denote the input (data) of f(·; θ).
On the other hand, when we discuss in the context of causality topics, we will use capitalized letters
to denote the variables, where some variables are reserved for special meanings.
For example, X is
reserved for variables corresponding to input features, Y for variables corresponding to labels, C for
variables corresponding to causal features, and therefore, ¯C will be reserved for variables corresponding
to non-causal features. Also, within non-causal features, there are also features that are statistically
correlated with the label (i.e., confounding factors), and we will use ˜C to denote them. We will use small
letters such as x, y, c, ¯c, ˜c to denote the values of these random variables. As a simple example to show
how some of these notations can be used, we denote the standard empirical risk minimization (ERM) as
the following
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y)
(1)
Definition of Trustworthy and the Role of Stakeholders
Before we dive deeper into the introduc-
tion of the technical contents of the paper, we need to first clarify the definition of trustworthy. Different
communities might have different understanding of the concept. In this survey, we use the term trust-
worthy machine learning to refer to the development, deployment, and use of machine learning models
that are reliable, ethical, and transparent, and thus a trustworthy machine learning model is designed
to be fair, accurate, and robust, and it is developed using transparent and explainable methods, as well
as with security and privacy in mind.
Therefore, we consider trustworthy ML as an umbrella term to cover various aspects of machine
learning such as fairness, security, privacy, explainability, and robustness. However, as we discussed pre-
viously, this survey will only discuss the topics of robustness, security (adversarial robustness), fairness,
and explainability.
It is also worth noting the role of the additional elements, other than major components that are
in ERM machine learning study (i.e., the statistical model and the data), plays in the definition of
trustworthy ML.
For example, machine learning robustness studies the topic of how to maintain the predictive perfor-
mances of variations of the data distribution shifts, usually in the form of additional datasets that users
consider similar to the training dataset, or perturbations that users consider should not alter the model’s
prediction. In other words, while the topic studies performance against variations of data, the variation
needs to be specified, instead of being arbitrary. To put simply, robustness study must specify what the
model is robust against.
To further illustrate this point, Figure 3 (left) is created from the inspiration of two previous works
[542, 644] discussing machine learning robustness in the context of domain adaptation. In our example,
the model can learn two possible decision boundaries from the data to classify triangles vs. circles, and
which one is considered useful (or “causal”) depends on where/what the model is used for. Figure 3
(right) is an intuitive illustration to explain the example on the left.
Similarly, security study must specify against what, and sometimes to what degree. Also, interpretabil-
ity study must specify to what is considered interpretable to the stakeholders, For example, as we will see
later in detail, different assumptions in interpretations, such as “the smaller set of features identified,
the better” or “the more connected the features identified (smoothness), the better” will lead to distinct
evaluations of interpretation methods, as well as distinct regularizations as part of the methods.
In summary, with the above argument, we believe that trustworthy machine learning, as least within
the scope of this survey that covers concrete directions such as robustness, security, fairness, and inter-
pretability, is a topic that cannot be studied without specifications of the requests of stakeholders. As
a result, the design of the methods will certainly require additional knowledge from such requests as
part of the regularization or data augmentation procedure. We will see tons of evidence to support this
argument later in this survey. Occasionally, there might be methods that do not use such prior knowl-
edge explicitly, but we notice that these methods usually implicitly build upon different assumptions
regarding the requests of stakeholders. Thus, Trustworthy machine learning cannot be studied without
6


--- Page 7 ---
0
0.5
1.0
1.0
1.8
2.8
0.0
Train Domain
Test Domain 1
•
“Spurious” for Test
Domain 1
•
“Causal” for Test
Domain 2
•
What a vanilla model
is likely to learn
•
“Causal” for Test
Domain 1
•
“Spurious” for
Test Domain 2
1.2
2.2
Test Domain 2
Test Domain 3
Training dataset
Task: what is the animal
Task: where is the animal
Task: is the animal currently at its
mostly assumed environment?
Figure 3: Illustrations to support the argument of in the main text that robustness study must specify
what the model is robust against. Left: examples created combining the examples used in [542] and [644]
for a classification over triangles vs. circles; the labeling functions (decision boundaries) in the training
domain are colored according to Test Domain 1 (the bottom right domain). Right: intuitive examples
created to illustrate the point of the left.
prior knowledge. There might be communities who do not agree with our assertion, then it is most likely
because the community defines trustworthy ML differently from ours.
Contributions
Due to the extensive body of literature surrounding the topic of trustworthy machine
learning, our survey diverges from the structure of traditional survey papers. While conventional survey
papers tend to passively document various methods within each category, we take a more proactive ap-
proach by delving into the statistical underpinnings of each technique and summarizing their overarching
design principles. This consolidation allows us to group multiple methods under a unified framework,
facilitating a broader analysis and revealing that numerous methods within different trustworthy ML
topics share a common rationale. In summary, our contributions can be outlined as follows:
• We extensively surveyed papers under the umbrella of trustworthy ML, including robustness, se-
curity (adversarial robustness), fairness, and interpretability. We primarily focus on the technical
aspects of the methods, and from a data-centric perspective.
• We summarized each method down to its statistical backbone with its conceptual rationale, offering
a principled understanding of trustworthy ML. The principled understanding allows us to build
high-level connections between methods within and across topics under the umbrella of trustworthy
ML, offering readers an efficient way to navigate through the vast sea of this field.
• We connected the principled understanding to the well-established study of causality under Pearl’s
perspective, showing the connections between trustworthy ML and causality. Many trustworthy
ML methods have been implicitly using the popular concepts in causality, suggesting new views
for trustworthy ML by leveraging frontiers from causality.
• We put our discussions in the context of large pretrained models. By summarizing the core ideas of
the techniques of the large pretrained model’s paradigm and connecting them to ERM. With this
summary and the principled understanding in previous sections, we are able to potentially predict
some of the future methods in the context of the large pretrained models over trustworthy machine
learning topics.
• We also offer intuitive languages to explain the ideas behind our mathematical works.
• We also attempt to offer suggestions for the future development of trustworthy machine learning.
7


--- Page 8 ---
2
Trustworthy Machine Learning Topics from Data Perspective
With the significant results machine learning achieves on benchmark datasets on various application
scenarios in the lab, the community is excited about extending its power into real-world applications.
However, when it comes to the real world, the numerical performance (such as prediction accuracy) is not
always the only important thing, especially when the real data are not as well prepared as the benchmark
datasets. As a result, many other metrics of machine learning models are valued and studied.
This survey paper scopes its focus along three dimensions of these other valued metrics, namely the
model’s robustness, fairness, and interpretability, and the term “trustworthy” is used to refer to a model
being robust, fair, and interpretable. Although in other literature, the term “trustworthy” might be used
to refer to the models being resilient to label noises (label shift) [308, 487, 635], or private [2, 191]. we
do not consider these or other merits part of the discussions in this survey paper.
The remainder of this section is structured in a way that we will first discuss each of these three major
focuses in trustworthy machine learning, with detailed setup of the problems and high-level descriptions
of the solutions in each of the focuses. Then, with our summary of the problems and the solutions, we
propose a hypothesis that an underlying common issue of all these challenges is that these models are
not learning what the models are expected to learn i.e., the models are not learning the desired features.
2.1
Robustness
In modern machine learning communities, robustness is usually used to refer to the property that a model
can maintain its performances over perturbed test data when the perturbed data has some “tolerable”
shifts from the original data and the causal features remain intact during the perturbation.
Domain Adaptation
The study of machine learning robustness in this regard has a long history.
Domain adaptation [45, 46], as one of the pioneers, studies the problem of how to maintain the model’s
predictive performances when the test are from a domain that is similar but different from the domain
used to train the models.
domain: in our survey, we follow the convention of using the
word “domain” to refer to a specific context or environment
from which the data comes or in which the model is applied.
In statistical studies, a domain is usually associated with
a specific data distribution; in practical studies, a domain
is usually considered a specific collection of data.
source domain: the domain with which the model is trained.
target domain: the domain with which the model is tested.
The study of domain adaptation has inspired
a long line of research in both theoretical perspec-
tives [45, 46, 122, 165, 330, 638] and empirical per-
spectives [261, 392, 441, 524]. The empirical eval-
uation is usually set upon a test scenario that the
model is trained with one dataset from one distri-
bution and evaluated from another dataset from
another distribution that is considered similar but
different to the training one. This “similar but different” property is usually defined by human factors
during the collection of the datasets used for domain adaptation study.
For instance, under the setup of Example 1, one domain adaptation study is to train the image
classification model on photos of sea turtles vs. tortoise, and then test the model on sketches of the
depicted animals without background. A ideal model is expected to perform well on sketches of animals
even if it was trained on photos because an ideal model is supposed to be able to capture the causal
features of depicted animals from photos, just as how a human will recognize the images in either photos
or sketches because a human understands the true differences between a dog and a cat. Covariate shift
[180] is a formalization of this problem setup of domain adaptation.
The theoretical discussion of domain adaptation has been expanded over decades [45, 46, 122, 165,
330, 638], probably pioneered by [45, 46]. Recent works such as [122, 543] conceptually summarized the
main idea of the generalization error bounds into two components (an estimatable term regarding the
divergence between the source and target distributions, and an non-estimable term about the nature of
the problem).
Thus, most of the empirical methods devoted to this topic seeks to improve the models’ performances
by introducing regularizations to force the learned representations to be invariant across training and
testing distributions, with the pioneering example of domain adversarial neural network (DANN) [13],
which introduces a “domain classifier” to differentiate two domains at the representation/embedding
space, and then a representation that offers minimum information to this “domain classifier” (i.e. a
representation invariant across domains) is considered good for domain adaptation.
Inspired by the
theoretical discussions above, mostly from [45, 46], domain adversarial neural network has popularized
8


--- Page 9 ---
train
test
unsupervised domain adaptation
train
test
domain generalization
train
test
modern OOD generalization
Figure 4: Conceptuall illustration about the differences between unsupervised domain adaptation, do-
main generalization, and more modern settings of OOD generalizations. The shape denotes the domain
of the data, and the color denotes the label. The illustration is for the availability of data during the
training time.
the following formulation of training a model
arg min
θ
1
n
X
(x,y)∈(X,Y)∪(Z,∅)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(2)
from which the parameters ϕ are obtained from
arg min
ϕ
X
(x,y)∈(X,Y)∪(Z,∅)
l(h(fk(x; θ); ϕ), d),
(3)
where (Z, ∅) denotes the dataset from target distribution with Z denoting features and ∅denoting the un-
available labels, and d denotes “domain IDs”, an label-functioning variable that encodes the information
of whether x is from the source domain or target domain.
In summary, we refer to the equation set of Equation 2 and Equation 3 as a DANN structure solution.
This name is chosen following the fact that domain adversarial neural network (DANN) [13] is one of
the most popular techniques using this set of equations.
activition:
the output of a node
in a neural network, calculated by
applying an activation function to
its inputs.
The community later proliferative progressed along this line to in-
troduce many methods for the invariance across domains/distributions,
with extensions of the representation-learning-model (i.e., encoder) to
two copies [524], extensions with additional alignment of activation dis-
tributions between source and target domains [523], extensions through
additional domain-relevant but task-irrelevant data [392], and many others that target learning invariant
representations across domains e.g., [59, 262, 666].
Another branch of domain adaptation techniques aim to introduce the invariance across training
domain and test domain in a more explicit manner by directly augmenting the training data to match
the marginal distributions of the test data. For example, a popular approach is to generate the data
with source domain semantics (i.e., p(Y |X)) but target domain style (i.e., p(X)), with a simple master
equation
arg min
θ
1
n
X
(x,y)∈(X,Y)∪(Z,∅)
Ez=g(h(x;ϕ);ψ)l(f(z; θ), y),
(4)
where we use g(h(·; ϕ); ψ) to denote a data generation model, where h(·; ϕ) maps the raw sample into
embedding space, and g(·; ψ) maps the embedding space back to the data space.
A mainstream choice is to use GAN or its variants as g(h(·; ϕ); ψ) to generate the data to boost the
performance for domain adaptation, such as [58, 209, 262, 354].
While the above summarizes the main approaches in domain adaptation, it is worth mentioning that
there are also works arguing that Equation (2) will not solve a domain adaptation problem sufficiently,
but most of these works assume the label shift setting such as [592, 645], and thus they are not in the
scope of our discussions.
In addition to the relative fixed train-test distributions split scenario, there are also works using
intermediate domains (distributions) to help the adaptation process, usually terms as multi-step domain
adaptation [504, 505] or gradual domain adaptation [83, 276, 544]. For a more dedicated summary and a
general primer of domain adaptation, we refer readers to several focused literature reviews [106, 554, 577].
Domain Generalization
An often asked question regarding the study of domain adaptation is what
if we do not know the distribution the model to be tested with during training. In reality, this concern
9


--- Page 10 ---
seems legitimate since after we build a model, we will expect it to perform consistently in future data
distributions that we are still unaware at this moment. Thus, as an answer to this question, the com-
munity starts to work on domain generalization [353], for which a model is trained on a collection of
distributions of training data and then tested on new distributions unseen during training.
Different from domain adaptation research, the development of domain generalization techniques
in this deep learning era rarely build upon pioneering theoretical discussions.
Instead, most of the
development efforts can extend upon the empirical efforts in the domain adaptation field to extend the
“invariance between source vs. target distributions” technique to the more suitable “invariance among
multiple training distributions” techniques. Therefore, a large trunk of empirical works converge again to
the common theme of being invariant across multiple distributions, despite being creative and innovative
as each individual publication, such as
1. direct extension of DANN to multi-domain case [292] and further extensions like conditioning on
the label [298] or through divergence terms [650] or others [16, 69, 158, 193, 352, 366, 414, 545],
with a master equation
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(5)
where d stands for the domain ID that is part of the dataset by the definition of domain gener-
alization. Due to the similarity between Equations 5 and 2, h(·; ϕ) can be estimated in the same
way as in Equation 3.
2. enforcing invariance with generated corresponding samples in other domains [171, 220, 462, 620,
661]. At a high level, the main idea is essentially data augmentation, with
arg min
θ
1
n
X
(x,y)∈(X,Y)
Ez=g(h(x;ϕ);ψ)l(f(z; θ), y),
(6)
which almost the same as Equation 4, except for the definition of the marginal distribution one
is interested in generating: domain adaptation aims to generate data following the marginal dis-
tribution of the target domain data, while domain generalization aims to generate data following
the marginal distributions of other training (source) domain data. As some concrete examples
for domain generalization, [462] perturbs data through the gradient with respective to the data
to fool both the label and the domain classifiers (i.e., an adversarial attack process that we will
discuss in the next part), and then use the generated data for training (i.e., an adversarial training
process that we will discuss in the next part). [220] introduced a bidirectional learning idea that
involves the learning in both spatial domain and the frequency domain of an image, with domain
randomization on the frequency domain as an augmentation. [414] builds multiple extensions upon
[292] with different blocks for global domains and local sub-domains.
3. learning the same classifier across all the domains (e.g., invariant risk minimization) [12, 27],
although with some counterpoints on the effectiveness of this thread [248, 433].
It is also worth mentioning another thread of domain generalization works following the assumption
domain-specific features can also help the empirical performances in domain generalization [63, 135, 457],
although this thread is not in the scope of our discussion.
In recent years, although it is fairly intuitive that using the extra domain partition information will
benefit empirical performances, the community continues to seek to free this last constraint of domain
generalization to a more realistic scenario where the training datasets are not necessarily partitioned into
multiple distributions/domains with clear boundaries during training [223, 224, 515, 539, 540]. It seems
the community is using the terminology Out-of-domain Domain (OOD) Generalization to largely
refer to Domain Generalization. For more detailed discussions of topics in Domain Generalization and
Out-of-domain Domain (OOD) Generalization, we refer the readers to more dedicated surveys [468, 550].
Countering Spurious Features (Dataset Bias)
Another thread of research that usually falls into
the scope of machine learning robustness is motivated by the concept of spurious features [532], con-
founding factors [337], or bias-in-data [519]. Overall, in comparison to the topics discussed above, this
thread of works centers more explicitly around the story in Example 1 about the fact that the models
10


--- Page 11 ---
might learn some undesired features (like backgrounds) other than the ones that are semantically aligned
with the human perception of the data.
As there are multiple lines of works suggesting that a fundamental challenge for the model to learn
“semantic” (or causal) features instead of the spurious features lies in the construction of the dataset
[207, 243, 547], or the existence of the confounding features (Example 1).
Therefore, most of these
methods are designed following the same procedure: first identify the spurious features, and then analyze
the statistical properties of the spurious features to build a reguarlization and/or a training process for
the model to avoid learning these features.
For example, [540] investigates the problem that a vanilla computer vision model tends to learn the
texture features from an image [162, 460], and build a side model that focuses particularly on learning
textures and force the main model to learn information invariant to this side model. Following the similar
main structure, [539] studies the problem that sometimes model tend to learn a local patch of information,
ignoring the idea from the whole images, and constructs a side model that particularly focuses on learning
patches of images to help the learning of the main model. Further, [36] introduces a more systematic
view to counter the bias of data with concrete examples such as CNN with smaller receptive fields for
texture bias. More recently, aiming to break the requirement of a prior model design, there is a line of
works investigating the statistical properties of certain datasets, and conclude that, for these datasets,
the spurious features are usually the ones that are easier to learn. Following this observation, [358]
considers the features learnt at an early stage of the training spurious, and [109] considers the features
learnt by a shallow network spurious, then they can take advantage of these properties to counter the
main model’s learning of the spurious features.
The above thread naturally converges to a master equation,
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(7)
with ϕ to be estimated with
arg min
ϕ∈Φ
X
(x,y)∈(X,Y)
l(h(fk(x; θ); ϕ), y).
(8)
As one might notice, Equations 7 and 5 are exactly the same, however, the differences lie where Equation 8
is compared to Equation 3. The main difference lies that, where 8 does not require explicit domain labels
d, however, it usually requires dedicated chosen hypothesis space Φ such as the models that only learn
the texture of images etc.
There is also some exceptions, for example, [259] assumes prior knowledge of bias information is
available in the form of labels, and then directly reuses the DANN structure from Equation 2 and 3.
Adversarial Robustness
Another widely studied topic under the robustness category is adversarial
robustness, which studies the model’s reaction to samples that are transformed under certain criteria.
The research field is popularized by the “intriguing properties of neural networks” [173, 501] through
showcasing that we can generate samples that are perceptually indistinguishable to the original samples
but completely alter the models’ decisions.
Figure 5: One of the most widely known
illustrations about adversarial robustness,
from [173]. It tells the story that one can
inject (carefully crafted) noises to the im-
age, creating a resultant image that ap-
pears identical to the original image, but
deceive the model to predict it to be some-
thing else.
The community names these samples adversarial sam-
ples, the process of generating them attack, and correspond-
ingly, the process of maintaining the model’s prediction over
these generated samples same as over the original ones de-
fense. Also, it is critical to note that there is usually a con-
straint regularizing the generation of adversarial sample in
terms of the distance between the resultant adversarial sam-
ple and the original sample, otherwise, the research will be-
come meaningless if the adversarial sample can be arbitrarily
different from the original sample. Usually, we denote such
a constraint as d(x′, x) ≤ϵ, where x′ is the generated sam-
ple and d(·, ·) is the distance metric of choice, with the most
popular choices usually being ℓp norms.
The discovery of the intriguing property of adversarial
sample inspires long lines of studies to innovation along the attack methods [24, 31, 40, 68, 84, 87, 96,
99, 105, 112, 184, 200, 257, 279, 300, 316, 349, 350, 363, 372, 382, 383, 384, 436, 448, 491, 553, 580, 581,
11


--- Page 12 ---
634, 654, 664, 667] as well as the defense methods [70, 99, 125, 152, 175, 181, 208, 213, 241, 242, 252,
275, 286, 303, 321, 322, 323, 355, 364, 367, 380, 381, 385, 412, 434, 488, 496, 558, 559, 560, 568, 571, 576,
585, 604, 607, 626].
adversarial example/adversarial sample refers to the data
that has been modified slightly in a way that is intended
to cause a model to make a mistake.
attack refers to the process or method used to generate ad-
versarial examples.
defense refers to methods to make a model more robust
against attacks.
While the community has progressed signifi-
cantly along both the attack methods and the de-
fense methods directions, the research efforts re-
cently seemingly converge to the most powerful
attack methods and its associated defense meth-
ods: for a while, PGD [325] is considered as the
most powerful attack methods. Intuitively speak-
ing, PGD can be considered as an opposite process
of training a model with the gradient descent: when we train a model with the gradient descent, we
usually iteratively update the model parameters following the gradient to decrease the model’s loss over
the fixed data, when we use PGD, we iteratively update the data following the gradient to increase the
fixed model’s loss over the resultant data.
With a most powerful attack method, the most powerful defense method is to simply train with the
adversarial samples generated with the attack at each iteration along training [325], which leads to the
following equation
arg min
θ
1
n
X
(x,y)∈(X,Y)
max
x′;d(x′,x)≤ϵ l(f(x′; θ), y).
(9)
As Equation 9 once again aligns well with the one of the central themes above (e.g., Equation 6),
one might wonder that whether we can use the other major theme (e.g., Equations 2 and 5) to improve
the method for adversarial robustness. In fact, there are indeed some works using, again, the DANN
structure to improve adversarial robustness by considering the setting as a domain adaptation problem
[211, 421], but these methods do not seem to have shown its significance.
Interestingly, a more significant thread of methods have demonstrated its empirical strength is closely
tied to the Equations 2 and 5 but in a much more simplified manner because of the nature of adversarial
examples. Since the generation of adversarial examples is essentially a data augmentation process, thus
there is a natural one-to-one correspondence between the original sample x, and the augmented sample x′
we do not really need a discriminator (i.e., the h(·; ϕ)) to push for the invariance between the embeddings
learnt from these two samples, we can simply regularize the distance between these two embeddings, as
the following:
arg min
θ
1
n
X
(x,y)∈(X,Y)
max
x′;d(x′,x)≤ϵ

l(f(x′; θ), y)

+ λD(f(x′; θ), f(x; θ)),
(10)
where D(·, ·) stands for a distance metric of choice over the embeddings of samples fed into the model.
Equation 10 corresponds to adversarial training [325] when λ = 0, TRADES loss [633] when D(·, ·) is
KL divergence, and ALP loss [251] when D(·, ·) is squared ℓ2 norm. Other notable methods under this
category are MART [572] and Consistency [502].
In addition to the development of defense methods for the statistical perspectives, the community
has also been seeking to offer an intuitive understanding of the underlying causes of such “intriguing
properties”. One answer is points to the nature of data by showing the the existence of such features
that are imperceptible to human but also predictive [229, 547]. The existence of such features have also
been validated by multiple other works showing that deep learning model has a tendency in learning
the texture of images [162, 207, 243, 460, 547], not necessarily in the context of adversarial robustness.
These evidence credits the challenges of adversarial robustness to the perspective of data.
Further, it is worth mentioning that, despite the popularity gained through adversarial robustness in
the deep learning community recently, the statistical techniques of how to maintain a model’s prediction
toward certain distribution shifts equivalent to perturbing features within a ℓp ball has been studied over
decades in the statistics community under the name Distributional Robustness Optimization [413].
Some of these studies in recent years interestingly connects the regularization in loss terms [458, 597] to
the adversarial robustness behaviors in ℓp norms in linear models.
There are many other related topics in adversarial robustness. For example, targeted adversarial
attack is an extension of adversarial attack, in the sense that it does not only use the resultant image
to deceive the model to predict into something, but direct it to predict into a specific class [14]. Many
of the vanilla adversarial attack methods above can be extended to its targeted version, as surveyed by
12


--- Page 13 ---
[14]. Another popular extension is to extend the adversarial training into embedding space [111, 224],
where the above adversarial training idea (e.g., Equation 9) is used, but, instead of at the raw data level,
it is used at the embedding/representation level. In other words, instead of augmenting x into x′, it
augments fk(x; θ) into its perturbed counterpart.
Connections of OOD Robustness and Adversarial Robustness
While we have shown that the
Equation 9 has the same format with one of the major threads of methods in OOD robustness (such as
domain adaptation and domain generalization), it is worth mentioning that the format of Equation 10
is also not unique. It has been studied in the robustness literature in the name of consistency loss or
alignment regularizations [28, 186, 301, 444, 461, 541, 590, 595, 642, 655]
These connections inspire us to think in a more high-level of what robustness means: robustness refers
to the study of whether the model can maintain its performance under the shifts when the stakeholders
do not consider these shifts should lead to degradation of the model’s performance, either the shift is
more salient such as from color image to sketch (thus OOD generalization) or more subtle such that the
stakeholders cannot observe the shift (thus adversarial robustness).
2.2
Fairness
When a machine learning model is robust against various shifts, it might be perform into the real-
world without a noticeable performance drop on various situations. However, this does not necessarily
mean that the machine learning models are ready to be deployed to serve all different tasks, especially on
certain tasks where there are some sensitive attributes from the data that the models better neglect while
building the statistical relationship. For example, a hiring[183] prediction software is not supposed to
use ethnicity or gender as an attribute to avoid discrimination against certain populations. As another
example, a face recognition algorithm is supposed to perform stably over all genders and skin colors
[8, 22, 197, 249, 555].
While there are evidence that the machine learning models are suffering various challenges above
[72, 132], fortunately, the community is actively proposing powerful methods to mitigate these issues,
and the research community usually refers to this thread of topics the study of machine learning fairness.
While there are multiple topics the ML fairness study focuses, the problem are mainly categorized
into two problems according to [132]: the outcome discrimination and the quality disparity.
• Outcome Discrimination: it refers to the scenario that the ML model uses certain attributes to
predict, such as learning the association between the ethnicity and the salary outcome
• Quality Disparity: it refers to the scenario that the ML model fails to generalize to samples with
certain properties because of their lack of representation in the data, e.g., a model trained on
Caucasian faces might not perform well on Asian faces.
One might already notice that, the outcome discrimination problem corresponds to the spurious
feature setup that we discussed in the robustness section, although the techniques largely use the domain
adaptation or domain generalization ideas with the availability of the sensitive variables; while the quality
disparity is more conceptually related to the domain adaptation or domain generalization topics, but
because the distributions are more explicitly defined here with sample disparity, there are usually more
direct methods to align the distributions than a mathematical alignment that is often seen in the domain
adaptation/generalization works.
Outcome Discrimination
Due to the similarity of the mathematical construction of the problem,
the many solutions can also be categorized into what has been used in the above, such as
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(11)
where d now denotes the label of sensitive variable, with ϕ to be estimated with in the same way as in
Equation 3.
GAN-style We use this term to broadly refer to modules
that can generate samples like a GAN. This survey does
not intend to analyze the detailed differences among data-
generation models used.
For example, as early-stage works, [7, 74, 139,
535] mostly reuses DANN model with domain id
replaced with sensitive attribute, [50] adopts a
similar idea, but only uses a small amount of data.
13


--- Page 14 ---
Further, [149, 324] uses GAN-style model, which is then extended by [596] with the idea to use multiple
GANs. [630] builds the adversarial component from prediction of the main model to the sensitive vari-
able. Further, [134] trains a classifier for each group, and then use domain adaptation to connects each
classifier of the group. [373] builds a model to predict sensitive attribute, and then augment the data to
remove the part of the features that can predict the sensitive attribute. [629] maintains the insensitivity
to the sensitive variable through reconstruction.
Another branch of efforts is to train with explicit fairness constraints, along which, [73] introduces
a framework that connects to multiple existing fairness definitions and handle multiple existing fairness
constraints. [102] introduces a proxy-Lagrangian formulation for optimizing non-convex objectives with
non-differentiable constraints that also connects to multiple fairness and other policy definitions. Along
this thread, there is also a proliferation of methods focusing on explicitly building constraints into the
process of learning such as [9, 49, 124, 170, 221, 362].
Quality Disparity
On the other hand, quality disparity, due to its explicit construction of problem
in terms of the model’s lack of attention to samples that are not sufficiently represented in the data,
the corresponding techniques usually directly weigh the samples to push the models to emphasize the
minority samples.
For example, there is a major line of solutions focusing on weighting the samples differently, for exam-
ple, [169] increase the weight of the minority samples, [236] sample-weighting method that corresponds
to multiple different fairness measures, [272] introduces a process that iteratively adapts training sample
weights. There is also a line of research working on the similar issue under a “group-DRO” term, such
as [215, 358, 443]
The sampling-weighting theme can also be potentially aligned to the central theme of this paper, as
discussed in [542], although the discussion involves many more assumptions.
There is also a branch of papers that is leveraging the domain invariance techniques to solve the
problems of Quality Disparity, essentially, to align the distributional differences between the training
distributions (where there is low density for the minority samples) and the testing distributions. For
example, [546] generates data distribution that minimizes the disparity (generate target domain data, of
data distribution that minimizes the source/target domain differences) [239] aligns the two distributions
with Wasserstein distance. Therefore, once again, these methods can lead to the same central equation
as used in Domain Adaptation (2).
Evaluation of Fairness
While we are presenting a summary of the techniques that have been invented
and proved useful in various topics under the “machine learning fairness” category, it is worth noting
that we believe the research about machine learning fairness involves many directions other than the
development of machine learning methods.
For example, one crucial topic is probably the actual meaning of being “fair”, which has inspired
multiple lines of discussions on either the societal aspects of “fairness” or the evaluation metrics of
“fairness”.
As a technical survey, we do not intend to offer discussions on these aspects, readers of
interest can refer to more dedicated surveys of relevant discussions [72, 340]
2.3
XAI: Interpretability and Explainability
Definitions and Evaluations
Another big branch of the study of trustworthy machine learning is
the investigation of the techniques to unveil the blackbox nature of the deep learning models, aiming to
explain the working mechanisms of the stacked layers of matrices to the users with human comprehensible
terms. This field is often described as to study the interpretability, explanability, or even understandability
of the neural networks, with subtle differences in the definitions of each [42, 131, 307]. Here in this paper,
we will not dive deep to analyze the exact definitions of each, but to follow some other customs [639] to
use these terms exchangeably: we use these terms to describe the study of techniques that report a set of
features the models use to make the predictions, which corresponds to the data perspective theme of our
survey. On the other hand, the branch of works aiming to explain how the building blocks of matrices
are wired together for a model to make predictions is not in the scope of our discussion.
The diverse set of the definitions leads to a diverse set of evaluation metrics. As one might expect,
one of the ideal evaluations in terms of performance is to test whether the interpreted results (i.e., the
features identified by the interpretability methods) can directly speak to the users (domain experts) in
a comprehensible manner [131]. However, this evaluation is probably also the least favorable choice in
terms of efficiency as it involves human evaluation (i.e., surveying users to vote out a rank of methods).
14


--- Page 15 ---
As alternatives, there is a list of other evaluation methods introduced to evaluate the interpretability
methods by quantitatively measuring certain properties of the identified features.
For example, one
branch is to masking out the identified features and then test for the model’s performance degradation
of the same model [446] or retrained models [6, 210]. There are also other evaluation methods that
emphasize on other properties, for example, [131] emphasizes the sparsity of identified features, while
[346] emphasizes the the smoothness of the identified features.
Methods in Interpretability and Explanability
In a nutshell, we notice that different evaluation
methods can directly inspire the design of methods. For example, many methods directly support the
formula of a “main equation” for interpretability (identification of features) regularized with a constraint,
where the constraint can regularize the identified features to be sparse, smooth, etc, dependent on the
evaluation metrics. As one might expect, the “main equation” once again converges to a central theme
that we will present after we discuss the techniques in details in the following paragraphs.
We will start by iterating the argument in [123] about the features being “minimally and sufficiently
present” and “minimally and necessarily absent”.
In short, they search for the features that if not
perturbed, the prediction will not change, and if perturbed, the prediction will change. While there are
multiple other efforts to define the importance of features in a similar manner [1, 103, 178, 320], as one
may expect, such definitions will directly guide a golden strategy of locating such features for model
explanation: perturbing the features of interest and then compare the model’s output under certain
metrics of interest (such as whether the prediction shifts).
This main idea of perturbing features and then comparing the output for explanation has been
considered as a central theme of model explainability or interpretability by [104], which summarizes
multiple relevant techniques such as IME [489, 490], SHAP [320], SAGE [103], LIME [427], and many
others.
One can refer to detailed discussions [104] for a summary of more methods on this theme.
However, one shall notice that summaries like [104] considers the first step of the explanation routine as
“removing” of features, whereas here we refer to “perturbation”, which we consider is more general, and
fits the central theme of our entire survey better.
One difference between “removal” and “perturbation” is that “removal” fixes the feature values one
can use, usually to be zeros [397, 455, 628], default values [108, 427], or certain values according to the
(conditional) marginal distribution [103, 320, 669]; meanwhile, “perturbation” does not have clear values
to set, thus allowing more methods to be categorized into this theme.
For example, activation maximization [143, 477] perturbs the features to maximize the output of a
model (e.g., the activation of a certain class of the prediction layer) to search for patterns of input that
are most responsible for the class. In practice, it can also be implemented in a way that the users start
from an existing image of a certain class and apply the perturbation to convert the image to maximize
the activation of another class [142], so that the patterns that are responsible for the prediction will
be more visually recognized. This usage in practice will probably remind the readers of the adversarial
attack methods discussed in previous sections, which essentially is about perturbing the features of the
data to alter the prediction of the model. However, adversarial attack constraints the perturbation to
be invariant to a human’s perception (usually favors high-frequency perturbations), while interpretation
methods usually constrain the perturbation to be meaningful to a human’s perception (usually favors
low-frequency perturbations) [328, 365, 614].
Finally, the activation maximization usually uses the gradient information of the model to perform
the perturbations, which seems a natural idea given the popular connections between the gradient and
its input, as well as the important role the gradient has played in adversarial attack methods. Further, it
is worth mentioning that the usage of gradient has played a significant role along in the thread of model
interpretation, known as gradient-attribution methods. Methods such as GradCam [456, 657] have been
widely used by the community. Other works [639, 657] have also categorized other popular methods such
as DeepLIFT [475], LRP [33], and integrated gradient [497] as gradient-based methods. In addition,
we believe the connection between perturbation (or removal) based methods and gradient-attribution
methods is probably more mathematically fundamental: the definition of “gradient” is the evaluation of
the function output of the infinitesimal shift (i.e., perturbation) of the input (i.e., features).
In addition, it seems natural that the perturbation (or removal) based methods can be accelerated
by the gradient-attribution methods. However, we do not see published papers that explicitly connnect
these two threads.
Overall, as a summary of the techniques discussed above, we aim to attempt a master equation that
15


--- Page 16 ---
outlines the techniques into one equation:
x∗= arg max
x′:d(x′,x)≤ϵ
e(f(x′, θ))
(12)
where x∗denotes the explanation of the input x, e represents the evaluation function discussed above
(such as change of the prediction or negative activation function), d represents the constraint discussed
above (usually those favoring low-frequency components), and the choice of features and the target
values.
Different from the above master equations, Equation (12) does not seem to offer an elegant enough
mathematical guidance for the methods in this section, in comparison to the ones in previous section.
For example, while both equations (9) and (12) use d(x′, x) ≤ϵ, it will take more mathematical efforts
to correspond the constrain to each method in this section, while it barely requires additional efforts to
correspond it to most methods in the adversarial robustness section. Regardless, the master equation
should still offer an adequate conceptual summary of most of the methods, which will be enough for
us to continue to the next part of this survey.
Along the preparation of this manuscript, we also
notice concurrent works that connect interpretability and adversarial robustness from more technical
perspectives [311].
Connections Between Interpretability and Robustenss
Despite the expansive set of promising
techniques that aims to continue to improve the explanation techniques, it is worth mentioning that
many of these explanation techniques are fairly easy to be fooled. For example, [206] fine-tunes the
model with additional regularizations to shift the attention maps, and [126] leverages the gradient of
model with respect to the image to perturb the image features to manipulate the explanation, which is
a process highly relevant to (targeted) adversarial attack. One can refer to a more systematic discussion
on this regard [526].
However, the above discussion of the possibility of fooling an interpretation method leads to another
question: whether it is the issue of the interpretability method or the issue of the model itself.
In
fact, there is a long line of methods that has been discussing whether a robust model that has been
trained on the right features naturally has multiple desired properties. For example, [141] showed that
an adversarially robust vision model has a chance to perform well on a variety of different vision tasks by
learning a representation that is better aligned with the human visual system. The merit of adversarial
robust models on learning a representation that is more aligned with the preferences of the stakeholders
has been supported directly or indirectly by many works of different nature, such as [82, 439, 509, 547,
612, 637]
Despite a long line of work suggesting that a more robust model tends to behave better with the
stakeholders, the conclusion of whether a robust model is enough is not clear at this moment. This line of
debate nonetheless validates one point: a robust model is more favorable than a vanilla model, although
it might not be ideal enough, which we conjecture is because the robust models are not robust enough
yet.
2.4
A Theme of Trustworthy Machine Learning from Data Perspective
With separate discussions of multiple threads of different topics in trustworthy machine learning and the
mathematical and conceptual summarization of the main ideas, we hope we have convinced our readers
that many of the methods discussed here, although innovative and powerful in other aspects, converge
to an interesting theme of trustworthy machine learning.
As we can see, two significant formulations repeatedly appear in the discussion of methods across
different aspects of trustworthy machine learning topics: the first one, probably popularized by the
domain adversarial neural network, is
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(13)
where choices of h(·; ϕ) and d depend on the exact applications, as we discussed above; the second one,
probably popularized by adversarial training in adversarial robustness literature, is
arg min
θ
1
n
X
(x,y)∈(X,Y)
max
x′;d(x′,x)≤ϵ l(f(x′; θ), y),
(14)
16


--- Page 17 ---
label
data
cross-entropy loss
prediction
backpropagation
model
label
data
cross-entropy loss
prediction
backpropagation
prediction
(modality)
label
(modality)
cross-entropy loss
backpropagation
(negative gradient)
backpropagation
label
data
cross-entropy loss
prediction
backpropagation
weight calculator
label
data
cross-entropy loss
prediction
backpropagation
gradient
additional
generator
generated
distance checker
cross-entropy
consistency
a
c
b
d
c.1
Figure 6: A summary of methods in our converged theme of trustworthy machine learning. (a) standard
ERM loss. (b) DANN structure construction of model, corresponding to master equation 13. (c) worst-
case data augmentation strategy, corresponding to master equation 14. (c.1) shows an upgraded version of
the loss design of (c) that usually leads to an improved empirical performance. (d) is sample-reweighting
method that corresponds to master equation 15, and it can be plugged onto all previous methods.
where choices of d(·, ·) and ϵ will depend on the exact applications. One might consider the generation
of x′ will also vary and depend on the applications, however, in our formulation, we consider that when
d(·, ·) and ϵ are well defined, the generation of x′ under a maximization will also be determined.
In addition, there is also a plug-in component that one can directly inject onto Equations 13 and 14
to weigh the samples differently. For example, we can denote the weighting factor as α(x, y, θ), and the
standard ERM loss function Equation 1 can be directly upgraded to the following
arg min
θ
1
n
X
(x,y)∈(X,Y)
α(x, y, θ)l(f(x; θ), y),
(15)
the same technique can be directly plugged onto Equations 13 and 14.
Further, in one of our previous works, we have shown that, even for these views of trustworthy
ML, there is a higher-layer converged understanding of trustworthy machine learning from the data
perspective [542]. In our formulation, we showed a unified generalization error bound that can lead to
the above two formulations of methods. Our unified generalization error bound essentially suggests that
a path to developing such methods is the identificaiton of those features that are statistically correlated
in a dataset, but spurious in practical settings, and informing the models about these features with either
regularizations or augmentations.
3
Trustworthy Machine Learning in Causality Perspectives
The previous section provided an overview of trustworthy machine learning across multiple topics, reveal-
ing a common theme of data-centric machine learning techniques that involve discarding or perturbing
certain features to achieve trustworthiness as defined by human experts. This core technique has a con-
ceptual connection to the topic of causality in Pearl’s language, particularly in the connections between
feature perturbation and the intervention and counterfactual concepts in Pearl’s causal hierarchy. In the
second half of this survey, we will offer another overview of recent trustworthy machine learning papers,
but this time from the perspective of Pearl’s causal hierarchy. We will organize the papers that explicitly
mention the causality terms and associate them with the levels of Pearl’s causal hierarchy.
Before delving deeper into the core techniques of intervention and counterfactual causation, we will
first provide a brief overview of the concepts and terminologies of causality.
17


--- Page 18 ---
3.1
Background in Causality
Causality is a fundamental concept in many fields.
In its most basic form, causality refers to the
relationship between cause and effect, where a cause is an event or condition that produces an effect.
However, establishing causal relationships is often challenging, as many factors can influence an outcome,
and it can be difficult to distinguish between causal and non-causal relationships.
3.1.1
Background: Confounding Variable and SCM
In causal inference, we are often interested in finding the causal effect of a variable X (“treatment
variable”) to another variable Y (“outcome variable”). Such causal effect cannot be estimated in general
from the statistical association between X and Y in observational data, due to the spurious correlation
brought by one or more variables. More formally, according to the associational criterion in [389], we say
that X and Y are confounded if there exists a variable Z which is not affected by X but is associated
with both X, and Y conditional on X. We refer to Z as a confounder, or confounding variable, for
the relationship between X and Y . Although in the context of causal diagrams or Bayesian networks,
“confounder” is often used to refer only to variables that causally influence both X and Y [389], we follow
the terminology in more general discussions [402, 649] and use the word “confounder” interchangeably
with “confounding variable” to incorporate a broader range of variables that create spurious correlations
between X and Y . For example, following the story in Example 1, we can consider the sea environment
as the confounder.
(a)
(b)
Figure 7: (a): The graphical model representing the data generation process for the sea turtle vs. tortoise
classification. An arrow in C →X means C is a direct cause of X. A dashed double-arrow arc means
there is unobserved confounder between C and ˜C. (b): An informal, illustrated version of (a)
Intuitive as it might seem, it is not easy to formalize the notion of “cause” and “effect”, or tell which
correlations are “spurious”, using the standard language of probability theory. Even with a fully specified
population density function, we are still unable to make predictions about a hypothetical distribution
where a new treatment is imposed unless we make some assumptions about the generating mechanisms
of the variables [389]. To better study the causal relationship between variables, the community has
introduced a formal language called Structural Causal Model (SCM).
Definition 3.1: Structural Causal Model
A structural causal model is a 4-tuple (U, V, F, P(U), where U = {U1, U2, ..., Un} is a set of exogenous variables,
which account for factors or influence from outside the model, and are not caused by any other variables in the
model. V = {V1, V2, ..., Vn} is a set of endogenous variables, whose values are determined by other variables in
the model. F = {F1, F2, ..., Fn} represents a set of functions such that Vi = Fi(PAi, Ui) for i = 1, . . . , n, where
PAi ⊆V represents the parents of the variable Vi. P(U) is a probability distribution over the exogenous variables
U.
Every SCM is associated with a graphical representation that illustrates the causal relationship among
variables in the SCM, referred to as the Graphical Causal Model, or “graphical model”. We assume that
exogenous variables are mutually independent, and omit them from the graphical model for simplicity.
Otherwise, if U1 and U2 are dependent, we add a dashed double-arrow curve between the endogenous
variables V1 and V2 in the graphical model, meaning they are confounded.
Let Figure 7(a) be the graphical model of the training data for the sea turtle vs. tortoise image
classification in Example 1. Among the endogenous variables, C represents the biological feature of the
animal, such as the shape, color and texture of its shell and feet; ˜C represents the background of the
image; X and Y are the image and label respectively. Exogenous variables represent external factors,
e.g., UC may represent the individual characteristics of the animal, and UX may be the photographic
18


--- Page 19 ---
conditions. The causal diagram encodes our assumptions about the data generation process, that C and
˜C cause X, that Y is only caused by C except the error term UY , etc. Although the biological feature
C and the background ˜C do not cause each other, there is spurious correlation between them due to
the data collection process. For example, the data may only include photos of animals in their natural
habitats at certain places. We consider the correlation spurious because it may change in another set
of photos collected from different places (e.g. a thermal transportation box), or in images of cartoon
and art paintings where animals can appear in any background. A model that constantly performs well
should be one that makes the prediction based only on the biological features.
Given a causal diagram, one can infer the conditional independence and dependence relations between
variables from graphic patterns such as chains, forks, colliders and the d-separation criteria. For a detailed
introduction of them, please refer to [168]. Suppose we are interested in the relation between C and X,
we can infer from Figure 7 that ˜C is a confounder between them through the path (C, ˜C, X).
In Section 3.1.2, we will go beyond this single example and discuss the connections between machine
learning and causal inference in a broader setting.
3.1.2
Connections to Machine Learning Development
We now shift back to the discussions of the machine learning topics. In Section 2, we have discussed a
summary of trustworthy machine learning covering multiple different topics and converged the topic to
a shared data-centric theme. Here, we continue to study this theme. We believe the challenge is mainly
caused by a major non-robust assumption taken for many machine learning models: (x1, y1), ..., (xn, yn)
are realizations of random variables that are i.i.d. (independent and identically distributed) with joint
distribution P(X, Y ) [394].
In other words, previous machine learning models are usually evaluated
based on the same dataset distribution used for the training, which often does not reflect the true testing
scenario in practice.
Therefore, the community has investigated a long line of research focusing on
topics that the testing scenario is different from the training scenario, as discussed in the “robustness”
in Section 2.
This disparity between training and testing has not been emphasized by the machine learning models
for a long time, probably because the concentration on improving accuracy over i.i.d data is one of the
fastest ways to facilitate method development. As a result, machine learning development does not often
recognize the underlying causal model of the data-generating process. Therefore, the models’ so rich
ability helps them capture all kinds of patterns in the data correlated with the output (termed as “curse
of universal approximation” in certain prior work [542]), including both causal features C and non-causal
features ˜C. There is a pressing need to eliminate confounders’ impact on the result (more on this in
“3.2” and “3.3”)
(a) Assumed in this survey
(b) Unbiased
(c) Alternative
Figure 8: Different graphical models for the data generation process. (a) is the main graphical model
in our survey representative of many different settings; (b) is the graphical model of an ideally unbiased
dataset that doesn’t contain ˜C; (c) is an alternative graphical model which adds a causal link ˜C →Y to
(a), but cannot be distinguished from (a) by the machine learning model.
Recent works at the intersection of causal inference and machine learning [145, 146, 267, 563, 565, 623,
631] have introduced different graphical models encoding various assumptions about the data generation
process. After extensive investigation, we use Figure 8(a) as the main graphical model for our survey
because it is most representative of a wide range of settings. It is the same as Figure 7(a) except that now
we assign much broader meanings to the variables. C represents the causal features often related to the
aim of the task, such as object appearance and location in an object detection task, or reviewers’ attitude
towards an item in a sentiment classification task, etc. ˜C represents the non-causal features that should
not be leveraged by the model for predictions. In fairness considerations, ˜C often denotes demographic
information, while in domain generalization and adaptation, it typically reflects domain-specific biases
19


--- Page 20 ---
and is usually categorical or can be approximated as such. ˜C can also be challenging to model, when it
is multi-dimensional and continuous, as in adversarial attacks, or involves high-level concepts difficult to
separate from causal variables C [146].
As shown in Figure 8(a), we assume that a datapoint X is generated by causal features C and non-
causal features ˜C. Some unmeasured variables produce non-causal correlation between C and ˜C. While
some works consider annotation artifacts [329] or incomplete information in the causal features about
the label [565], which implies a causal link from ˜C to Y , we assume in our survey that the dataset is
carefully prepared such that Y is the ground truth label that only depends on C.
Suppose there is an ideally unbiased dataset, whose graphical model is given in Figure 8(b). Based
on the equation P(y|x) = P
c P(c|x)P(y|c), and assuming a fixed P(y|c), a model trained to estimate
P(y|x) on the data distribution would learn a good estimator of the causal feature ˆP(c|x). In Figure 9(a),
however, ˜C confounds the relationship between C and X through the path (C, ˜C, X). More specifically,
the path produces a non-causal association between C and X, where the association between C and ˜C
is often a bias in the dataset.
Consider the alternative graphical model of the data generation process in Figure 8(c), where ˜C is
a direct cause of Y . While one may notice that it is not observationally equivalent to the graphical
model in Figure 8(a), it is impossible for a statistical model observing only X and Y to differentiate
between those two graphical models. Instead, in Figure 8(a) the path ( ˜C, C, Y ) produces a non-causal
association between ˜C and Y , which the model may capture regardless of the causality. In practice, ˜C
is often shallow features that can be learned in the first few layers of a neural network [540], or at the
early stage of training [358], which may exacerbate the model’s tendency to use ˜C for prediction.
SCM provides us with a convenient tool to identify the confounders and qualitatively analyze the
undesired behaviors of machine learning models when deployed in non-IID settings. In the remainder of
this section, we will see that causal inference provides us with a lot of powerful tools to more quantitatively
estimate causal effect and remove the influence of confounders, which has been increasingly used in recent
works across different topics of trustworthy machine learning. In fact, many other works discussed in
Section 2 which did not explicitly use causal tools can also be revisited and understood from a causal
perspective.
3.1.3
Background: Levels in Pearl’s causal hierarchy
We now continue to offer the background in causality literature. These discussions might be perceived
as overly detailed for some readers with a working knowledge of causality. However, we believe these
discussions are essential as we later will map this causal hierarchy to current machine learning methods.
We hope such mapping will immediately help set the expectations of what current trustworthy methods
can achieve.
Pearl’s causal hierarchy (PCH) [388, 390, 391] provides a unified framework for discussing different
aspects of causality. The hierarchy consists of three levels of causation (L1, L2, and L3). The first
level is associational causation, which works on conventional statistics and does not incorporate any
causal techniques to identify the causal relationships between the variables. In other words, L1 does
not distinguish between “correlation” and “causation”, and seeks to identify correlations in the observed
data.
Unlike L1, the remaining two levels of causation (L2, and L3) adhere to the principle that “correlation
is not causation”. Although the causal mechanisms behind a system are often unobservable, they leave
observable traces in the form of data that can be analyzed. The second level of causation is typically
referred to as intervention, while the third level is known as counterfactuals. The primary difference
between intervention and counterfactuals lies in their ability to consider scenarios that contradict the
observed data. Intervention involves asking and answering questions about the effect of an action on
the resulting distribution of the overall observed data. In other words, intervention often operates on a
population level to estimate the effect of the intervention. In contrast, counterfactual reasoning involves
considering hypothetical scenarios at an individual level, including those that did not actually occur, to
quantify the effect of an intervention.
First Level (L1)
The first level of causal hierarchy deals with the question: “How likely is Y given
that one observes X?”. This level focuses on measuring statistical associations between variables, repre-
sented by the conditional probability P(Y |X). However, such associations alone cannot establish causal
relationships, as they can be influenced by confounding variables not accounted for in the analysis. For
example, data might reveal that ice-cream and sunglasses sales are highly correlated in a certain region,
20


--- Page 21 ---
but this might simply reflect the influence of a confounding variable - hot weather, which boosts both
ice-cream and sunglasses sales [225]. Most of the conventional statistical and machine learning methods
primarily seek to find correlational patterns in data.
Although this might be useful under the i.i.d.
assumptions, these patterns often fail to generalize to new scenarios. This is because they do not provide
insight into the underlying causal mechanisms generating the data.
(a) Before intervention
(b) After intervention
Figure 9: The effect of intervention on the graphical models of the data generation process.
Definition 3.2: do-Calculus
The do−calculus is a system that replaces the conditional distribution with an intervened distribution forcing the
value of a variable, such that it is randomly assigned without any influence of its parents. It consists of three
schemes that provide graphical G criteria for when certain substitutions may be made.
Rule 1. P(Y |do(X), Z, W) = P(Y |do(X), W), if (Y ⊥⊥Z|W, X)G ¯
X
Rule 2. P(Y |do(X), do(Z), W) = P(Y |do(X), Z, W), if (Y ⊥⊥Z|W, X)G ¯
X,¯Z
Rule 3. P(Y |do(X), do(Z), W) = P(Y |do(X), W), if (Y ⊥⊥Z|W, X)G ¯
X,Z(W )
Second Level (L2)
The second level of Pearl’s hierarchy, known as interventionist causation, involves
hypothetical or “conditional” questions such as “How likely would Y be if one were to make X happen?”.
This level aims to understand the implications of intervening in a system.
In Pearl’s framework, an intervention is an action that changes the value of a variable by some external
mechanism. For example, in Figure 9, if we intervene on variable C, we enforce a value to it regardless
of the original mechanism that generates C. This intervention effectively removes all incoming edges to
this node. The causal effect of this intervention is then gauged by the consequent change in the outcome
variable. For example, to see whether there is a causal effect of local ice-cream sales on sunglasses sales,
we may implement a policy to close all the ice-cream shops and see the change [168]. In Example 1, if we
want to know whether a machine learning model erroneously uses color information as a heuristic (such
as the blue color in an ocean background for sea turtles), we may convert all images to grayscale to see
how the distribution of predictions change.
For interventional experiments, a gold standard is the randomized controlled trials. By randomizing
the assignment of treatment, we make sure that any change in the outcome is only a result of the treat-
ment, which helps us make sound scientific conclusions and informed decisions. However, this method
might not always be practical, ethical, or economically feasible. In causal inference, intervention is for-
malized by the do-operation, and the do-calculus provides a set of rules for manipulating expressions
involving do-operations. There exist various techniques to estimate the effect of interventions from ob-
servational data, such as adjustment methods, Inverse Probability Weighting, and Instrument Variables,
to name a few. In recent years, there is a growing interest in the machine learning community to apply
these techniques to find robust patterns in the data that capture the causal relationship between vari-
ables. This is fueled by the potential these techniques have in enhancing the trustworthy properties of
machine learning models.
Third Level (L3)
The third level of causation, known as counterfactuals, also deals with hypothetical
scenarios. It allows questions like “Given that one observed X and Y , how likely would Y have been if
X′ had been true?”, where X′ may contradict the observed event. In practice, it is difficult to directly
observe counterfactual outcomes, and so counterfactual causation relies on statistical methods to estimate
these outcomes. Counterfactual and intervention might appear similar. However, in interventions, we
focus on what will happen on average if we perform an action on overall observed samples, whereas in
21


--- Page 22 ---
counterfactuals we focus on what would have happened if we had taken a different course of action in a
specific situation, given that we have information about what actually happened.
For example, imagine we have a machine learning model that decides whether to grant loans to
someone based on income, employment status, credit score, and age. Suppose the model rejected the
loan application from an individual with medium income, part-time job, good credit score, and older
age. The person would be interested in finding a counterfactual explanation such as “What if I had
a full-time job instead of a part-time job?” If the model would have approved this application in this
hypothetical setting, it could motivate the applicant to find a full-time job. From a fairness perspective, if
the model’s prediction would have flipped by merely changing the demographic group of the applicant, we
know that the model may contain social bias that needs to be addressed. As the top level of causality,
counterfactual analysis enables us to answer causal questions that span across various hypothetical
scenarios, and isolate the treatment effect by different mechanisms. Compared to intervention, it often
requires stronger assumptions and more accurate specification of the causal model, because it often
involves extrapolation outside the support of the observed data.
From the above text, we can see the differences in information-richness among the three levels of
causation: higher layers Li encode more information than the lower layers, forming a hierarchy L3 >
L2 > L1 [391]. Therefore, to answer questions related to Layer i, knowledge of Layer i or higher is
necessary. Further, it is highly unlikely for the layers of PCH to collapse, meaning that L1 contains
answers to L2 and L2 contains answers to L3, as it requires capturing the exact representation of the
population in the samples, which is difficult to achieve [391]. This forms the basis of the development of
PCH and is stated in the Causal Hierarchy Theorem [41].
Overall, Pearl’s causal hierarchy provides a comprehensive framework for reasoning about causality
in a variety of contexts. By understanding the different levels of the hierarchy, it is possible to develop
more reliable and trustworthy machine-learning techniques that take into account the complexities of
causal relationships, and it will also help us set up the expectations for what degree of trustworthiness
a method can eventually achieve, despite it might perform well on certain benchmarks. In the following
sections, we will discuss how Pearl’s hierarchy has been used in recent research on trustworthy machine
learning. Following the seminal books on causal inference [168, 389], we assume that all variables are
discrete in the math derivations in this section, to ensure consistency in notation. These derivations can
be easily extended to the continuous case through integration on probability density functions.
3.2
Intervention: the second level
Observational data often provides limited insight into the structural causal model that generated it,
partly due to the difficulty of discerning whether observed associations between variables reflect causal
relationships. This challenge arises because many variables’ roles - causal or merely correlational - remain
unclear. By external control and manipulation of these variables, we can investigate their potential causal
influences more effectively. This active manipulation and observation of effects is a key component of
the second level of causation (L2) and allows us to construct a more accurate representation of the true
data-generating SCM.
In causal inference, quantitative measurement of causal effect is facilitated by the do-operator, which
forces a variable X to take the value x, denoted as do(X = x) or do(x). Formally, given a structural causal
model M, the intervention do(x) is defined as the substitution of structural equation X = FX(PAX, UX)
with X = x.
Intervening on a variable is different from conditioning on it, which can be explained via the example
in Figure 9. In Figure 9(a), the path (C, ˜C, X) produces spurious correlation between C and X. By
conditioning on C, we narrow our focus to part of the sample space where C = c in the distribution. If we
change the value of C to condition on, ˜C is also likely to change due to the statistical association between
them. In contrast, by intervening on C, we change the distribution by removing all edges pointing to
C and assigning a value c to it. If we change the value of c for intervention, the change will not be
transmitted to ˜C.
Note that Figure 9(a) is the main causal diagram in our survey, and our above example has im-
plications in the context of machine learning. If we conduct the stochastic intervention [389] on the
data distribution P by assigning a distribution P(C) to the causal feature C, we can generate a new
distribution Pm where the marginal distribution of C remains the same but ˜C no longer confounds the
association between C and X. Further, we will show that such intervention will remove the confounding
effect of ˜C on the association between X and Y , and hence de-confound the prediction of a machine
learning model. Consider the probability of Y conditional on X for distributions P and Pm,
22


--- Page 23 ---
P(y|x) = P(x, y)
P(x)
=
P
c P(x, y, c)
P(x)
=
P
c P(c)P(x|c)P(y|x, c)
P(x)
=
P
c P(c)P(x|c)P(y|c)
P
c P(c)P(x|c)
(16)
Pm(y|x) =
P
c Pm(c)Pm(x|c)Pm(y|c)
P
c Pm(c)Pm(x|c)
=
P
c P(c)Pm(x|c)P(y|c)
P
c P(c)Pm(x|c)
=
P
c P(c)P(x|do(c))P(y|c)
P
c P(c)P(x|do(c))
(17)
Intuition:
Equations 16, 17 can be under-
stood as following: Suppose the presence of
objects in images (e.g., sea turtle features) is
the causal feature we care about.
If we in-
tervene on the objects in the data generation
process (e.g., the photographing process), and
then train a model on the intervened data dis-
tribution, the model will learn to pick up the
object information without being distracted by
other factors (e.g., the background).
where we use the condition that the generating mechanism
for Y is not changed, i.e.
Pm(y|c) = P(y|c).
Comparing
Equations 16, 17, it can be seen that the relationship be-
tween X and Y is confounded by ˜C only through the fac-
tor P(x|c). Because P(x|do(c)) removes the confounding, a
model trained to fit the statistical association between X and
Y on the interventional distribution Pm will not be affected
by the confounder ˜C.
While the above derivation gives a conceptual direction,
directly intervening on C is often impractical. C is the un-
derlying causal feature of the data examples which we usually don’t have access to and is hard to model.
Luckily, a series of methods in causal inference and statistics related to the notion of intervention has
enabled us to overcome the technical difficulties. In the remainder of this subsection, we will introduce
these methods and review recent works that apply them to topics in trustworthy machine learning.
Among them, recent works using adjustment methods are often based on a set of causal assumptions
different from that in our main graphical model, which will be detailed in Section 3.2.1, 3.2.2. We will
also revisit recent works discussed in Section 2 to understand them from a causal perspective.
Randomized Controlled Trial
Randomized Controlled Trial (RCT) is a scientific methodology that
operates on the principle of random assignment or collection of samples in different classes, ensuring that
any observed differences in outcomes are due to the intervention rather than confounding variables. For
example, in Example 1, rather than collecting images where sea turtles are mostly beside the sea, we
carefully collect a balanced set of images where sea turtles occur in all kinds of backgrounds according
to the marginal distribution of background, and similarly for the images of tortoises. However, it is often
impossible to collect such dataset due to the prohibitive cost and sometimes unattainable conditions (e.g.
a sea tortoise near a crater).
In the machine learning literature, data augmentation is often used to get an enlarged dataset where
the confounding effect of ˜C is removed. This can be seen as a RCT method from a causal perspective.
During this process, we first identify the confounder ˜C and its distribution. For each training datapoint
x, we generate a minimally perturbed version of x, denoted as x˜c, by setting the value of its confounder ˜C
to a value ˜c while keeping everything else the same in the process where x was generated. We repeat this
process by sampling different values ˜c from the marginal distribution P( ˜C) independently of the causal
feature C to get different x˜c’s, resulting in a randomized dataset. Standard training on this dataset gives
the loss function as in Equation 18.
arg min
θ
1
n
X
(x,y)∈(X,Y)
E
˜c∼D ˜
C
l(f(x˜c; θ), y),
(18)
Data augmentation that randomizes confounders is equivalent to intervention in terms of effects. To
show this, we can have a more formal look at this process.
Let P and P ′ denote the probability
distributions of the original data and the randomized data respectively.
Assume that the marginal
distribution of C and C′ remains the same but they become mutally independent after randomization,
i.e. P ′(c) = P(c), P ′(˜c) = P(˜c), P ′(c, ˜c) = P ′(c)P ′(˜c). The structural equations for X and Y should
remain the same, i.e., P ′(y|c) = P(y|c), P ′(x|c, ˜c) = P(x|c, ˜c),
23


--- Page 24 ---
P ′(x|c) = P ′(x, c)
P ′(c)
=
P
˜c P ′(x, c, ˜c)
P ′(c)
=
X
˜c
P ′(x, ˜c|c)
=
X
˜c
P ′(˜c|c)P ′(x|c, ˜c) =
X
˜c
P ′(˜c)P ′(x|c, ˜c)
=
X
˜c
P(˜c)P(x|c, ˜c) = P(x|do(c))
(19)
Intuition:
Equation 19 can be understood
with the follows.
If we randomize the exis-
tence of objects in images (e.g., mismatching
the animals with backgrounds randomly be-
fore taking photographs), we break the natu-
ral tendency of certain objects to occur in cer-
tain backgrounds.
Then model trained with
such data will be able to pick up the object
information for prediction, without being in-
fluenced by the background.
where the last equation is the backdoor adjustment formula,
which will be introduced in Section 3.2.1. This shows that
the statistical association between X and C in the random-
ized distribution P ′ captures the causal effect between them
in the original, biased distribution P, as if we had conducted
the intervention do(c) on the original dataset. Then, we can
analogously derive P ′(y|x) following Equation 17 and con-
clude that a model trained on P ′ will not be affected by the
confounder ˜C.
A large body of work using data augmentation for trust-
worthy properties (Section 2) can be understood from the perspective of Randomized Controlled Trials,
despite that the concrete design of the augmentation method varies with the problem settings.
In
domain generalization and domain adaptation, ˜C is often the domain category associated with domain-
specific bias (e.g. style or texture features).
˜C is often implicitly assumed to conform to a uniform
distribution, because different domains are considered equally important. Recent work has conducted
data augmentation on source (training) domain images by matching the style of the target domain for
domain adaptation [58, 209, 262, 354], or the styles of other training domains for domain generaliza-
tion [171, 220, 462, 620, 661]. In the fairness literature, ˜C may be demographic categories, and recent
works have used data augmentation to alleviate bias and discrimination in machine learning models
[386, 465, 648, 670].
When ˜C is multi-dimensional or continuous, estimation of the inner expectation in Equation 18 is
expensive. An alternative formulation has been proposed, which finds the worst-case perturbation on
the confounder ˜C and then minimize the loss.
arg min
θ
1
n
X
(x,y)∈(X,Y)
max
˜c
l(f(x˜c; θ), y),
(20)
Because max˜c l(f(x˜c; θ), y) is an upper bound on E˜c∼D ˜
Cl(f(x˜c; θ), y), solving Equation 20 has similar
effect to Equation 18 on the model.
It emphasizes the worst-case scenario where the confounder ˜C
breaks the prediction of a model when it is expected to be invariant to ˜C. It has been extensively used
in adversarial robustness studies. From a causal perspective, ˜C is a attack-related feature representing
whether the data is attacked, the attack algorithm and configurations, the initialization of the noise,
etc. The counterfactual x˜c is often realized as a perturbed version of x, where an ℓp norm constraint is
placed on the perturbation to ensure that it does not change the causal feature of the image perceived
by humans. Then Equation 20 converges to Equation 14 in our discussion of the common theme of
trustworthy machine learning in Subsection 2.4. Unlike Equation 18, this formulation does not require
prior knowledge on the distribution of D ˜
C, which is difficult to get in this scenario and some others.
This worst-case formulation goes beyond adversarial robustness studies and has been used in fairness
[552, 573] and domain generalization [318, 462] research as well.
Apart from RCT, several other options exist for de-biasing a model, such as Instrument Variable,
Backdoor adjustment, and Front-door adjustment method. We will continue our discussion with Instru-
ment Variable (IV) [38, 583].
Instrument Variable
It is a variable that is associated with the treatment variable, and influences
the outcome variable solely through the treatment variable. The Instrument Variable method is often
used to estimate the causal relationship between variables in the presence of unobserved confounders.
For example, assume we cannot observe ˜C in the graphical model in Figure 10. We may choose Z as the
instrument variable to estimate the causal effect of C on X.
24


--- Page 25 ---
To achieve this, we first use Z to predict the value of C, and then use the estimated value of C
to predict the value of X, resulting in an unbiased estimator of the causal effect. In the case of linear
models, this method is known as the two-stage least squares (2SLS) method [512].
(a) Prior Instrument
Variable(IV) Learning
(b) Post Instrument Variable(IV)
Learning
Figure 10: An example of instrument variable. (a): The original graphical model, where ˜C is unobserved.
(b): We may use Z as the instrument variable, to get an unbiased estimate of the causal effect of C on
X.
Definition 3.3: Instrument Variable
Instrument Variable Z is an exogenous variable introduced such that it affects X and has no independent effect
on the outcome variable Y . Z should satisfy the following properties:
1. Z is associated with X, i.e. P(X|Z) ̸= P(X);
2. Z is independent of Y given X, i.e. Z ⊥⊥Y |X
Recent works studying IV in machine learning settings have mainly focused on low-dimensional struc-
tured data. For example, [400] proposed a method to identify sparse causal effect in linear models in
presence of unobserved counfounders. They developed graphical criteria for identifiability of the causal
effect, and proposed an estimator based on the limited information maximum likelihood (LIML) estima-
tion [21, 23]. [584] studied the estimation of treatment effect when the data is collected from different
sources without access to source labels. They modeled the latent source labels as Group Instrument
Variables (GIV), and used a Meta-EM algorithm to iteratively optimize the data representations and the
joint distribution for GIV reconstruction. [255] developed parametric and non-parametric methods to
estimate the average partial causal effect (APCE) of a continuous treatment using instrument variables.
[440] focused on the independence of the IV with outcome variable conditioned upon input variable.
They used this independence to improve the identification and generalization of causal effects using the
proposed HSIC-X estimator. Recent works [345, 510] have studied IV methods for random processes.
[510] used the conditional instrument variable method by identifying sets of variables at different lags,
while [345] integrated the covariance matrix over time to find moment equations. For high-dimensional
data such as image or text, some works [218, 260, 508, 562] have considered adversarial or random per-
turbations on the input data or features as the instrument variable, to improve the robustness of deep
learning models.
Intervention on the feature level
In real-world scenarios, it is often difficult to do experimental
intervention. Counterfactual data augmentation is also challenging when ˜C is elusive or involves high-
level concepts. One line of work instead intervene on the feature level [146, 238], to learn a representation
that captures C for the downstream task while providing minimal information about ˜C. From a causal
perspective, this shares the spirit of “process control” [389], i.e., intervening the process influenced by the
treatment variable. For example, [238] intervene on the feature representation variable by normalizing
it for each confounded data point, modifying the representation of all data points with reference to one
particular distribution, leaving no effect of the confounder.
A large body of work on domain adaptation [59, 262, 392, 523, 524, 666], domain generalization [16,
69, 158, 193, 292, 298, 352, 366, 414, 545, 650] and fairness [7, 50, 74, 139, 535] discussed in Section 2
aligns the distribution of the representation of data at different strata of the confounder ˜C, which falls
into this category from our causal perspective. Many of these methods do the feature-level intervention
in a min-max game, iteratively training a side model to capture the confounder and the main model to
25


--- Page 26 ---
be invariant to it, based on Equation 21
arg min
θ
1
n
X
(x,y)∈(X,Y)
l(f(x; θ), y) −λl(h(fk(x; θ); ϕ), d),
(21)
When we have pairs of data with the same causal feature C, it is often more effective to do sample-wise
alignment [327]. This is connected to data augmentation, if we consider model output as the last-layer
feature.
For example, assume that ˜C follows a Uniform distribution over the set of possible values
{˜c1, . . . , ˜cn}. Assume l is a loss function that can be considered as a distance metric (e.g. ℓp loss for
p ≥1), so it should satisfy the triangle inequality. Then for i ̸= j, the below relation holds
l(f(x˜ci; θ), f(x˜cj; θ)) < l(f(x˜ci; θ), y) + l(f(x˜cj; θ), y)
(22)
where we use f(·; θ) to denote the model’s output after softmax.
Summing over all (i, j) pairs results in the following
X
1<i<j<n
l(f(x˜ci; θ), f(x˜cj; θ)) < (n −1)
n
X
i=1
l(f(x˜ci; θ), y)
(23)
This shows that minimizing the loss on the augmented data also minimizes an upper bound on the
pairwise distance between outputs corresponding to different strata of ˜C.
3.2.1
Backdoor Adjustment
The backdoor adjustment is a commonly used method to estimate the causal effect of a treatment variable
X on an outcome variable Y . By conditioning on a set of properly chosen variables Z (“adjustment
variables”), we can remove their confounding effect and get the causal effect of X on Y from observational
data alone without actually conducting the intervention. To achieve this, Z need to satisfy a set of
conditions, often known as the backdoor criterion. The backdoor criterion and the adjustment formula
are defined in 3.4.
Definition 3.4: Backdoor Adjustment
A set of variables Z satisfies the Backdoor criterion relative to {X, Y } in a DAG, if no node in Z is a descendant
of X, and Z blocks every path between X and Y that contains an arrow into X. Then the causal effect of X on
Y is given by
P(Y = y|do(X = x)) =
X
z
P(Y = y|X = x, Z = z)P(Z = z)
(24)
The backdoor criterion can be intuitively understood from its graphic implications on the SCM. Z
should be chosen such that:
• It blocks all spurious paths between X and Y ;
• It leaves all directed paths from X to Y unperturbed;
• It doesn’t create new spurious paths.
These three conditions ensure that conditioning on Z blocks and only blocks the spurious paths
between X and Y . For a formal proof of Equation 24, please refer to [389].
(a) Backdoor
(b) Frontdoor
Figure 11: Graphical models of the data generation processes underlying recent machine learning methods
that use backdoor and front-door adjustment.
26


--- Page 27 ---
Based on the causal assumptions encoded in the graphical model in Figure 8(a) and discussed in
Section 3.1.2, it is difficult to directly apply backdoor adjustment to machine learning tasks because
C is unobserved. Instead, recent works using adjustment-based methods have implicitly made causal
assumptions with a different conception of the causal feature C: instead of the latent factors that generate
X, they consider C as the information conveyed in and decoded from X. This results in a reversal of
the causal link between C and X and a graphical model in Figure 11(a). Now their is a causal relation
between X and Y through the path (X, C, Y ), and ˜C satisfies the backdoor criterion relative to {X, Y }.
This eliminates the need to model C and makes it more practical to apply adjustment methods to
high-dimensional data such as image.
Because backdoor adjustment provides a principled way to remove the confounding effect without
interventional experiments, it has gained popularity in recent years in machine learning research [91, 93,
94, 119, 222, 299, 463, 563, 565, 566, 623, 631]. These works studied problems from diverse background
but with a common aim to learn the causal effect of the input X on the output Y . In these methods, the
first step is to identify the confounder and its distribution in the dataset. For example, [93] found that
the individual habits of facial muscle movement is a confounder for facial action unit recognition, and
[631] found that the co-occurrence relationship among objects is a confounder for image classification
models to produce correct activation maps.
˜C is often assumed to be a discrete variable with a set of
possible values {˜cj}m
j=1, each corresponding to a class label or a sample group (such as facial images
of the same individual).
˜C is often assumed to conform to the Uniform distribution, or a distribution
estimated from the training set.
Next, the model architecture is modified to incorporate ˜C as a covariate, getting an estimator of
P(y|x, ˜cj) as below
ˆP(y|x, ˜cj) = fy((x, α(x,˜cj)˜cj); θ)
(25)
where fy(·; θ) is the output probability of the model corresponding to label y, ˜cj is the representation of
˜cj, and α(x,˜cj) is the weight assigned to ˜cj for the sample x, representing the probability that x belongs
to the stratum ˜cj.
Then, backdoor adjustment is applied to get an estimator of ˆP(y|do(x))
ˆP(y|do(x)) =
m
X
j=1
ˆP(y|x, ˜cj)P(˜cj) =
m
X
j=1
fy((x, α(x,˜cj)˜cj); θ)P(˜cj)
(26)
To improve the efficiency, the Normalized Weighted Geometric Mean [599] is often used to move the
outer summation into the feature level.
ˆP(y|do(x)) = fy

x,
m
X
j=1
α(x,˜cj)P(˜cj)˜cj; θ


(27)
Finally, the model is trained with maximum likelihood estimation, based on the following equation
arg min
θ
1
n
X
(x,y)∈(X,Y)
l

f

x,
m
X
j=1
α(x,˜cj)P(˜cj)˜cj; θ

, y


(28)
Following the early work [563, 623, 631], different implementation choices have been made to adapt to
different problem settings. To get the representation {˜cj}m
j=1, a common method is to average the features
of all samples corresponding to the same ˜cj [93, 94, 222, 299, 360, 463, 563, 566, 623, 631]. This approach
helps to gain the average features containing the confounding information, even if the confounders are
unobservable and there is little knowledge about them. However, it does not explicitly separate causal
from non-causal features. To overcome this limitation, recent works such as [94, 119, 404, 463, 563] modify
this methodology by only including context (e.g. background) features corresponding to each group. For
instance, [463] used class activation maps (CAMs), [404] built the confounder representation based on
query type in a multi-model scenario, [563] took the arithmetic average of the region of interest features of
associated objects, while [119] took weighted average of the sample features based on output probability
of the model. In addition, [406] used the observed confounders to training an unbiased classifier, which
was then used to stratify the remaining confounders. [75, 370] did not stratify the confounders, but
identified them based on temporal dependencies between confounders and other features.
27


--- Page 28 ---
Regarding the weight α(x,˜cj), [360, 566] took the simple approach to set α = 1, [93, 222, 299, 563, 631]
used an attention mechanism [529] to model the alignment of the sample to the confounder, while
[119, 623] used the model’s output probability corresponding to the label. In addition, there are different
ways to fuse the confounder representation with the sample, such as concatenation [631], simple addition
[360, 404], or they can be processed by different layers before added together [93, 566].
Before we conclude this section, it is worth mentioning that according to [406], some of the non-causal
features may provide useful contextual information for an image, which benefit the generalization of a
machine learning model. They suggest retaining these features in backdoor adjustment.
3.2.2
Frontdoor Adjustment
In the above text, we discussed adjustment variables and how we require a backdoor criterion to ensure
that we are able to adjust the right variables, to estimate the true effect of the treatment on the outcome.
However, there might be some cases where the backdoor criterion does not get satisfied, such as when the
confounding variable is unobserved. In these cases, front-door adjustment can be used to de-confound
the model.
Front-door adjustment method works using the two consecutive applications of backdoor adjustment
to estimate the causal effect of X on Y (i.e., P(Y = y|do(X = x))). It introduces a variable Z that is
a mediator between X and Y , with no backdoor path from X to Z. This means that the correlation
between Z and X is equal to the causal effect from X to Z, i.e., P(Z|do(X = x)) = P(Z|X). The
front-door adjustment chain together two partial effects, i.e. X on Z and Z on Y to estimate the overall
causal effect of X on Y , as given in Equation 29.
P(Y = y|do(X = x)) =
X
z
P(Y |do(Z = z))P(Z|do(X = x))
(29)
We can write the expression P(Z|do(X = x)) = P(Z|X) as described above. Meanwhile, for the
other partial effect expression P(Y |do(Z = z)), we can observe a backdoor path between Z and Y , i.e.
(Y, C, ˜C, X, Z). Therefore, we can properly quantify the effect of Z on Y only if the backdoor path is
blocked between Z and Y . This can be achieved by adjusting for the variable X, which is observable in
our backdoor path, arising the expression P(Y |do(Z)) = P(Y |Z = z, X = x′)P(X = x′) as represented
in the Equation 30.
P(Y = y|do(X = x)) =
X
z
P(Z = z|X = x)
X
x′
P(Y |Z = z, X = x′)P(X = x′)
(30)
Based on the graphical model in Figure 11(b), the intermediate feature of the model fk(X; θ) is
considered as the mediator Z [295, 608]. Compared to the 3.2.1, methodologies that use the front-door
adjustment do not require the observation of confounders. This enables some recent works [295, 608]
to use the observed data examples to eliminate spurious patterns, where the intermediate feature is
taken as the mediator. [608] proposed a causal attention framework. They used Normalized Weighted
Geometric Mean (NWGM) approximation [599] to absorb the outer summation on Z and X into the
feature level, and used in-sample and cross-sample attention mechanisms to calculate embeddings for Z
and X respectively. [295] used the gradient information of each example X to model its confounding
effect on Z →Y , and used a clustering-based method to efficiently estimate Equation 30 on the whole
dataset.
3.2.3
Inverse Probability Weighting
In 3.2.1 and 3.2.2 we have introduced two adjustment approaches to estimate the causal effect of X on Y
in the presence of a confounder Z. However, both methods require considering each value or combination
of values z and estimating P(y|x, z) separately, which may bring practical challenges. First, if the set
of possible values is large, it is computationally expensive to estimate all the associated conditional
probabilities. Second, some combinations of (x, z) may be missing or scarce in the dataset, in which
case it is difficult to give a reliable estimate of P(y|x, z). Inverse probability weighting is an alternative
approach that creates a pseudo-population where the confounder is independent of the treatment variable.
Assume, for example, that Z satisfies the backdoor criterion relative to {X, Y } in a graphical model.
28


--- Page 29 ---
P(y|do(x)) =
X
z
P(y|x, z)P(z) =
X
z
P(y|x, z)P(x|z)P(z)
P(x|z)
=
X
z
P(y, x, z)
P(x|z)
(31)
As comparison,
P(y|x) =
X
z
P(y, x, z)
P(x)
(32)
The interventional distribution P(y|do(x)) differs from the original distribution P(y|x) in that it
replaces the constant P(x) on the denominator with P(x|z). P(x|z) is often referred to as the “propensity
score”, which captures how likely the treatment variable is given the confounder. If we can reliably
estimate the propensity score (often using a parameterized model), we can weight each datapoint to
remove the confounding effect of z. This approach provides lower variance estimation than adjustment
methods when the distribution of ˜C is complicated and the dataset is relatively small.
A line of works discussed in Section 2 weight data examples to counter spurious features for robustness
[215, 359, 442], or improve learning on under-represented subpopulation for fairness [169, 215, 236, 272,
358, 443], which fall into this category of methods from a causal perspective. Based on our assumptions
in the graphical model in Figure 8, ˜C satisfies the backdoor criterion relative to {C, X}. And according
to our previous discussion on Equation 16, 17, estimating P(x|do(c)) removes the confounding between
X and Y . Following Equation 31,
P(x|do(c)) =
X
˜c
P(x, c, ˜c)
P(c|˜c)
(33)
Intuition: A larger propensity score indicates
that the causal feature of the datapoint is more
easily explained by the confounder, or the dat-
apoint is “bias-aligned”. Otherwise, the data-
point is said to be “bias-conflicting”. By giv-
ing more weights to the bias-conflicting sam-
ples, we balance the distribution and remove
the bias in the dataset.
Consider the relationship P(y|˜c) = P
c P(c|˜c)P(y|c). For
a fixed y, a higher P(y|˜c) implies higher propensity scores
between ˜c and the causal feature value c’s corresponding to
the label y. This tells us that P(y|˜c) is a good surrogate
for P(c|˜c) to weight the data samples, which is very useful
because C is usually not observed. We can train a side model
h(·; ϕ) to estimate P(y|˜c), and train the main model f(·; θ) on
the weighted training data, where smaller weights are given
to datapoints whose labels are more easily predicted from the bias, and vice versa. This gives us the
following equation
arg min
θ
1
n
X
(x,y)∈(X,Y)
1
hy(x; ϕ)l(f(x; θ), y),
(34)
where hy(·; ϕ) is the element of Softmax(h(·; ϕ)) corresponding to label y. Similar ideas may be imple-
mented in different ways, as in [109, 358]. One advantage of weighting-based approach is that it does
not require explicit modeling of the distribution of ˜C. Instead, the assumptions about the distribution
of ˜C is encoded in the architecture of h(·; ϕ) or its learning algorithm.
3.3
Counterfactuals: the third level
In this section, we will explore counterfactuals, which are considered the third level of causation L3.
Typically, counterfactuals involve a hypothetical scenario or antecedent, where the question is posed
with “if”, and the condition after “if” may be untrue and contradicts the observed event.
29


--- Page 30 ---
Definition 3.5: Counterfactuals
Counterfactual analysis deals with the assessment of events that would have happened under an alternative condi-
tion X = x′, given that the event has already occurred under the actual condition X = x with the outcome Y = y.
This can be defined mathematically as Equation 35
E(YX=x′|Y = y, X = x)
(35)
There are three major steps in estimating a counterfactual scenario:
Abduction- “given the fact that X = x and Y = y”, i.e. the observed values of endogenous variables in V are
used to infer the posterior distribution of exogenous variables U.
Action- “had X been x′”, i.e. the causal model M is modified by replacing the structural equations for X with
adequate functions making X = x′, resulting in a modified model Mx′
Prediction- “what Y would have been”, i.e. the modified model Mx′ and the inferred distribution of exogenous
variables U are used to compute the counterfactual outcome Yx′.
3.3.1
Data augmentation
Counterfactual analysis is often used in data augmentation methods. Based on our assumptions in the
graphical model in Figure 8, when we generate additional samples by perturbing the non-causal features
˜C, we are essentially answering the causal question of “what the input X would have been had ˜C been set
to a different value, with everything else been the same”. If we follow the three steps of counterfactual
analysis, but assign to ˜C a distribution P( ˜C) instead of a constant at the “action” step, we get the
following loss function on the augmented data:
Laug =
E
(x,y)∼P (X,Y )
"
E
u∼P (U|x,y)
"
E
˜c∼P ( ˜
C)
[l(f(XM˜c(u); θ), y)]
##
(36)
where u is a realization of the exogenous variables, u = (uX, uY , uC, u ˜
C). XM˜c(u) is the value of X in
the intervened causal model M˜c at U = u, XM˜c(u) = FX({c, ˜c}, uX) = FX({FC(∅, uC), ˜c}, uX), where
FX and FC denote the structural equations for X and C respectively.
Recent generative methods for data augmentation, such as those based on Variational Autoencoders
(VAEs) [263], often use an encoder to estimate P(U|x) and a decoder to generate XM˜c(u). Further,
if we assume that U can be uniquely determined by X, we can denote their mapping as U = U(X).
Then every pair of (x, ˜c) uniquely determines a counterfactual sample, denoted as x˜c, by the equation
x˜c = XM˜c(U(x)). Equation 36 can be simplified as below:
Laug =
E
(x,y)∼P (X,Y )
"
E
˜c∼P ( ˜
C)
[l(f(XM˜c(U(x)); θ), y)]
#
=
E
(x,y)∼P (X,Y )
"
E
˜c∼P ( ˜
C)
[l(f(x˜c; θ), y)]
#
(37)
which corresponds to the training objective in Equation 18.
Intuition: In data augmentation, we are im-
plicitly following the three steps of counterfac-
tual analysis to find out what the input object
would have been under a different condition.
For example, what if the turtle is placed into
a bird nest before taking photos.
Many deep learning techniques use generative [449] and
latent variable-based methods [387, 500, 606] to extract the
exogenous variables. These generative models, such as GAN
and VAEs, treat the exogenous variables as noises in their re-
spective models, as shown in the Figure 12. The works based
on them intervene on these noises to generate a counterfac-
tual sample in the majority of the scenarios. [500, 606] used
a variational autoencoder to extract independent causal mechanisms or exogenous variables in a latent
space, converting them to endogenous variables. [449, 500] were based on the assumption of “Indepen-
dent Causal Mechanism” (ICM) [289, 453], which states that “the causal generative process of variables
in a system is composed of autonomous modules that do not inform or influence each other”[394]. In
ICM, “independent” does not mean that different ICMs are statistically independent of each other, but
that they do not causally influence each other, i.e. intervening on one mechanism will not have any effect
on other mechanisms. Additionally, [387] performed the abduction step using normalizing flows and
variational inference, and then intervened on the obtained exogenous values as shown in Figure 12(a).
Some other works [57, 137, 178, 343, 371, 530, 557, 652, 673] have used generative or latent-space
based models to generate counterfactual images belonging to a different target class. They achieved this
30


--- Page 31 ---
Noises
Generator
(a) Similarity between Generative
Adversarial Network (GAN) and a
Structural Causal Model (SCM)
Encoder
Latent
Space
Decoder
(b) Similarity between Variational Auto-encoder (VAE) and a
Structural Causal Model (SCM)
Figure 12: Similarities between SCM and latent variable models, with GAN and VAE as examples. (a)
demonstrates the similarity between GAN and SCM, where exogenous variables (U) correspond to noise
used by the GAN to generate the output.
Meanwhile, endogenous variables (V ) are neurons of the
network and the image is the generated output (X). (b) Similarly, for VAE, exogenous variables (U)
correspond to the latent vector [500] extracted by the encoder. The encoder acts as an inverse function
(F−1(·)) of the structural equations, and the decoder uses the latent vector to reconstruct the input
variable X.
by minimally perturbing the causal features of the original image, and some also used text associated
with the original image [651]. This procedure aims to identify the features treated as causal to categorize
the sample into the original class. The works [530, 578] segregated the image into segments to improve
the perturbation quality and let the image remain in the prior distribution. [578] introduced the concept
of causal over-determination, which probes two or more causal factors. Generative adversarial network
(GAN) [172] is the most widely used technique to generate counterfactual samples in this direction
[57, 137, 343, 371, 482, 570, 651, 652, 673].
In addition to latent-space based methods, some works conducted minimal perturbation on the pixel
level to generate counterfactual samples [178, 227, 246]. Aside from explaining the network behavior to
predict a sample in a class, [227, 482] focused on generating a counterfactual sample to correct the model’s
prediction. Rather than relying on the original latent space of the generative models, [606] introduced an
SCM as a prior by adding a causal layer, which maps the exogenous variables to endogenous variables.
Elements in this layer can be intervened with do−calculus to generate the counterfactual samples.
Furthermore, a plethora of works [226, 231, 428, 603, 673] generated counterfactual samples for
explainability. [673] generated counterfactuals to increase the explainability of the network to identify
the network’s behavior under some specific conditions not present priorly in the dataset. [428] aimed to
find spurious features that produce non-trivial explanations. [603] generated counterfactuals for attribute
informed latent space. [226] used an invertible network to identify how inputs can be altered to change
the prediction of a classifier, and generated the changed input image to provide a visual explanation for
prediction change. [371] proposed a method to generate a multi-way counterfactual explanation, i.e.,
change the attributes of the sample with respect to any class resulting in a change in the prediction of
the sample.
As alternative methods, some recent works treat causal features as endogenous features, and non-
causal features as exogenous features. [627] proposed to generate images by considering the non-causal
properties in the image as the exogenous variable U. [270, 621] generated new counterfactual images
by extracting the exogenous values using the three-step process defined in 3.3. They substituted the
value of an object with another object. [3] extracted the exogenous variables from samples to generate
counterfactual scenarios. Several other works [78, 85, 269, 284, 302, 643] used the term factual to describe
the process of augmenting a sample by removing non-causal features, and counter-factual to describe
the process of retaining the non-causal features after removing the causal features. The goal of this
augmentation is to let the machine learning model associate its output only with causal features and
disregard any dependence on non-causal features. In addition, [151, 401, 432, 449, 611] also aimed to
generate counterfactual samples to expand the observed dataset for robust learning. They achieved it by
leveraging various techniques such as adversarial learning to interpolate complex samples [151], diffusion-
based data-generation [449], and inserting scenarios in the dataset containing counterfactual situations
[611]. [401, 432] did binary classification on the existence of an object in the image. [432] employed the
process of counterfactual generation in two steps: defining the sample into different causal categories;
and replacing the causal variable in a sample with that of another sample.
31


--- Page 32 ---
3.3.2
Treatment Effects
The term treatment effect refers to the difference in potential outcomes resulting from the application
of a specific treatment or intervention. Since it is not possible for the same unit to be both treated and
untreated, counterfactual analysis is often required in observational studies to estimate the potential
outcomes.
(a)
(b)
Figure 13: Graphical models as an example of mediation, and how the natural direct effect is measured.
(a): X influences Y through the paths (X, Y ) and (X, Z, Y ). W is an arbitrary confounder. (b): When
we change X from x to x′, we hold {Z, W} to their pre-treatment distribution, and only transmit the
effect of the change through the path (X, Y ).
There are different types of treatment effects such as Natural Direct Effect (NDE), Natural Indirect
Effect (NIE), and Total Effect (TE). Consider the graphical model in Figure 13(a), in which X has
influence on Y both directly and through a mediator Z. Regarding the change in Y induced by changing
X from x to x′, different treatment effects can be defined using the counterfactual notation as below
[168]:
Definition 3.6: Treatment Effects
NDEx,x′(Y ) = E[Yx′,Mx −Yx,Mx]
(38)
NIEx,x′(Y ) = E[Yx,Mx′ −Yx,Mx]
(39)
TEx,x′(Y ) = E[Y |do(X = x′)] −E[Y |do(X = x)] = E[Yx′,Mx′ −Yx,Mx]
(40)
where M is the set of parent variables of Y except X, i.e., M = PAY \ {X} = {Z, W}, and Mx′ is
the value M would have attained under the condition X = x′. The direct influence of the treatment
variable X on Y , or the “direct effect”, is mingled with the mediating effect from Z. In order to uncover
the direct influence of the treatment, we hold M constant and change X from x to x′, as shown in
Figure 13(b). The resulting change in outcome variable is considered as the “Natural Direct Effect”
(NDE), as in Equation 38. The indirect effect refers to the anticipated change in the outcome variable
Y under the circumstance where the treatment variable X remains unchanged, but M is altered by the
treatment’s influence to become Mx′. To quantify the “total effect” of X on Y , we can directly use the
intervention do(x), or use the relation between different treatment effects. From Equations 38-40, it can
be seen that
TEx,x′(Y ) = NDEx,x′(Y ) −NIEx′,x(Y )
(41)
which means that the total effect of a transition is equal to the difference between the natural direct
effect of the transition and the natural indirect effect of the inverse transition [389].
Recent deep learning research has incorporated treatment effect method to either account for the
impact of certain variables, or remove the influence of certain variables [216, 481, 566]. Z often represents
secondary cues used to predict Y , which are influenced by the primary cues. For instance, [420] considered
Z as the refined features resulting from the use of soft attention [599], and used a network to learn
basic feature maps and attention separately to create the structural causal model (SCM) displayed in
Figure 13(a). Several approaches have been employed to address Z, considering it a bias resulting from
a specific type of occurrence between particular objects or features [368, 507], or events observed in
sequential data [80, 493, 566, 636]. For example, some studies [423, 507, 602, 636] consider the variable
Z as the variable carrying bias in the output and used the natural direct effect. [80, 216, 291, 566]
quantified the effect of a variable on the output by subtracting the nullified effect (the effect measured
by fixing Z to be 0) from the original effect. On the other hand, some works [368, 420, 426, 493] involve
32


--- Page 33 ---
problem settings where Z should be considered the main cue for predicting the output Y , and X is the
spurious variable. In such cases, the natural indirect effect was utilized to assign greater weightage to
Z. [481] utilized both the direct and indirect effect to determine the significance of different features in
the output.
Many of the works in this section have focused on using different training objectives aligned with
principles of causal inference, to prevent the model from learning biases in the training data. However,
some other works [79, 160, 161] have taken a more model-driven approach by incorporating causal prior
into the model. For instance, [79] view a neural network as an SCM, and quantified the impact of features
on the output via Average Causal Effect. [161] aligned representations in a neural network with variables
in an SCM, and trained the network to mimic the counterfactual behavior of the causal model on the
same base input. Similarly, [160] proposed a method to align the different sets of neurons in the neural
network as nodes of an SCM. As such, it mimics intervention on the SCM by assigning values to the
neurons artificially, supporting reasoning at the neuron level of the network.
3.4
Summary
In this section, we introduced Pearl’s Ladder of Causation, various techniques of causal inference, and
discussed their connections to trustworthy machine learning techniques. It can be seen from our discus-
sion that the techniques we identified in Section 2, despite been based on various statistical theories or
intuitions, can be grounded in the causal inference framework. When a new regularization or augmenta-
tion technique is proposed, it is important to be aligned with the causal assumptions of the corresponding
task. In addition, the direct application of causal inference to deep learning methods is challenged by
the problems of unobserved variables and the entanglement of causal and non-causal features. For the
communities who are interested in grounding trustworthy ML with causality, our results suggest future
works can aim to alleviate these problems so that deep learning methods can more strictly follow the
procedures of causal inference.
4
Trustworthy Machine Learning in the Context of Pretrained
Models
In previous sections, we discussed the technical advancements in trustworthy machine learning, synthe-
sized these developments into a central theme, and connected it to the concept of causality. Now, we
transition our focus to large, pretrained models. These models are usually trained on extensive web-scale
data and thus will potentially introduce unique challenges related to trustworthiness.
Certain literature posits that the immense variety and volume of training data might mitigate some
trustworthiness issues.
However, an expanding body of research suggests that these trustworthiness
challenges persist in pretrained models. Further, the complexity of these models and the opaque nature
of their training processes may make these issues harder to address.
The proposition of retraining large models using techniques previously discussed could potentially
alleviate some challenges. Nevertheless, this approach is often impractical due to the substantial param-
eter and data sizes. Fortunately, there are techniques such as fine-tuning, parameter-efficient tuning, and
prompting. While these methods were initially designed to improve model performance and efficiency,
they also provide opportunities to enhance the models’ trustworthiness.
In the remaining part of this section, we will recap the advancements in large models and present
different opinions on their trustworthiness.
Then, we will review commonly utilized techniques for
large models, aiming to establish a connection between these techniques to the ERM training strategies
that previously discussed methods seek to improve. We believe the discussion will continue to connect
the techniques of large pretrained models into the converged theme. Finally, we will survey techniques
explicitly devised to make large models more trustworthy, which will demonstrate that our central theme,
while being established within the context of standalone model training, remains applicable in the context
of large, pretrained models.
4.1
Large Pretrained Models and Its Trustworthiness Challenges
In recent years, we have witnessed an exceptional surge in the field of machine learning research, par-
ticularly with regards to pretrained models. As of the date this survey was drafted, this area of study
can arguably be considered the most prominent in AI research and applications. Models like ChatGPT,
33


--- Page 34 ---
for instance, have transcended the boundaries of machine learning communities, becoming the nucleus
of numerous discussions at various events [133, 253, 445].
Concurrent with its widespread practical influence, the core concept behind large pretrained models
is elegantly simple and widely comprehensible: as we increase the model parameter size together with
the volume of data, many machine-learning challenges could eventually be addressed. Even in instances
where specific challenges persist, fine-tuning the model with distinct datasets typically proves more
effective than training an isolated model from scratch.
While the utilization of pretrained models has seen a dramatic upswing in popularity in recent times, it
is probably not a recent invention. We hypothesize that earlier works likely sought to employ parameters
trained on one dataset to augment performance in other areas, as seen in transfer learning [53, 114, 379].
However, such ideas gained traction in the context of deep learning with the advent of AlexNet [274] in
computer vision and Word2Vec [344] in NLP.
In computer vision, following the triumphant runs of AlexNet and other CNN models in the ImageNet
competition, a segment of the community began enhancing their performance in other applications like
object detection [166, 167, 424] and segmentation [167, 431]. This methodology became so prevalent
that the primary ImageNet-trained model was often referred to as the “backbone” in their method. In
addition, the “backbone” can conveniently adapt to the latest architectures [67, 154, 665] along with
ImageNet research progression.
In NLP, pretrained models gained popularity with the introduction of word vectors via Word2Vec
[344] or GloVe [393]. These context-encoding word representations swiftly supplanted the discrete word
input in other NLP models. More recently, the BERT [121] pretraining regime, capable of excelling
in various downstream NLP tasks with minimal fine-tuning, became popular. Subsequent to BERT’s
success, numerous language models were introduced [43, 62, 100, 204, 280, 315, 378, 411, 495, 610], with
models like ChatGPT significantly impacting communities beyond AI research.
It is crucial to note that the GPT family of models, including ChatGPT, are not the sole large models
making substantial impacts beyond the AI community. For instance, AlphaFold [245] has demonstrated
considerable practical value in predicting protein’s 3D structures; CLIP [408], the core technique under-
pinning DALL-E, has also served as pretrained models in numerous vision-language learning tasks; and
more recently, vision models like SAM [264] are also emerging as dominant players in various vision tasks.
Their progress in their respective fields has been so significant that a portion of the research community
has proposed naming them “foundation models” [56].
In this survey, our primary focus will remain on vision and NLP applications. Consequently, our
survey will encompass techniques and discussions on the trustworthiness of NLP models such as the
GPT family, BERT [121], PaLM [26, 97], and LLaMA [520, 521], vision models like SAM [264], DINOv2
[375], SEEM [671], and vision-language models like CLIP [408], DALL·E [418, 419], Flamingo [19], BLIP
[293], and GPT-4 [374].
It is worth mentioning that the core strength leading to these models’ superior performance varies.
For instance, while models like CLIP, SAM, and early-stage language models primarily benefit from data
volume, Reinforcement Learning with Human Feedback (RLHF) [98, 486] has significantly enhanced the
practical performance of recent language models.
RLHF, or more broadly, machine learning with human feedback, is a process that enlists humans
to guide and evaluate the outputs of AI systems. Initial guidance comes from demonstrations, where
humans provide instances of correct behavior or desired outcomes (such as annotators’ understanding of
an image class). After the AI system generates its outputs, humans provide feedback by ranking output
quality or providing direct corrections. This feedback is then used to refine the AI model, in a cycle that
continues until satisfactory performance is achieved.
It is interesting to draw conceptual connections between machine learning with human feedback and
our general definition of trustworthy machine learning. Specifically, the need for human feedback arises
primarily due to discrepancies between the statistical loss that is typically trained and what humans
find useful [62, 148, 409, 410, 514]. Hence, machine learning with human feedback could potentially
bring models closer to the trustworthy attributes we desire. This might be one reason why a part of the
community has started to view these large models as possessing multiple trustworthy merits.
4.1.1
Positive Opinions on Trustworthy Properties of Large Models
Despite the feast celebrated by the public and media about the strengths of these pretrained models, we do
not find an abundance of rigorous academic publications supporting that these models are equipped with
desired trustworthy merits. In the few examples we found, the discussions are also fairly objective stating
34


--- Page 35 ---
that the pretrained models tend to have much higher performances on various benchmarks [62, 121, 409],
even on out-of-distribution data (defined as datasets released after the date the model is trained) [548], or
causal inference benchmarks [258], and stating that these performances are not perfect and need further
improvement.
4.1.2
Negative Opinions on Trustworthy Properties of Large Models
On the other hand, as the community continues to explore the technical advancements of these large
models, there is a growing recognition of their potential limitations, fueling concerns and driving further
investigation.
Lack of Domain-specific Robustness
Given the sheer volume of the data used to train the large
pretrained models, it will be hard to test the models’ robustness performances against traditionally
defined out-of-domain data: since the models might have already seen the domains during training.
However, in certain application fields, where the data are of a different nature to the daily language
and images, these large models are still suffering a fairly large performance drop, although still better
than the previous standalone models.
For example, the community has shown limited performances of SAM over medical images [336, 470],
of GPT over medical texts [113, 445], of Stable Diffusion [430] over medical images and text [77]. Other
domains, such as legal texts [459] and financial texts [588] are also facing challenges to directly applying
the generic large models. The challenges of applying pre-trained models to specific domains have become
significant, and there are surveys dedicated to the discussions [306].
With these specific challenges of the model’s understanding over the knowledge in specific domains, it
seems fine-tuning the pretrained models on these specific domains is a natural solution. However, it seems
a direct fine-tuning of the pretrained models on certain datasets only appear to solve the problems but
will reveal more weakness when more sophisticated testing are performed [182, 582], which is potentially
due to the catastrophic forgetting issue of large models [88], or in general, machine learning models [116].
Lack of Fairness (equal representation)
It is widely observed that machine learning models fre-
quently generate outputs that perpetuate stereotypes and discrimination against marginalized groups.
These models not only reflect, but sometimes exacerbate the biases present in the dataset [369]. When
it comes to these large pre-trained models, these issues are seemingly getting worse, as the construction
of larger models typically requires a larger volume of data, which can result in reduced data quality.
The issues exist in both discriminative models and generative models. A classifier may erroneously
exploit demographic information for prediction [39, 265, 304], and a text generation model could directly
express prejudiced views against certain demographic groups [356, 369, 452]. For instance, bias pertaining
to race, gender, and other demographic attributes has been discovered in models such as GPT-2 [469]
and BERT [506]. [5] identified persistent bias towards Muslim people in GPT-3. [647] discovered gender
bias in earlier models like ELMO [395], and [319] identified gender stereotypes in more recent models
like GPT-3. Although the RLHF technique improves the model’s sentiment towards all social groups, it
does not appear to reduce the disparities among them [378]. Furthermore, biases have been assessed in
pre-trained models across different languages [76, 551], and systematic evaluations have identified bias
pertaining to nine demographic attributes in masked language models [250, 361]. These studies reveal
the significant and widespread nature of bias in pre-trained language models [47].
The papers discussing such issues are more than what our survey can cover. For a more comprehensive
overview, one can refer to other surveys dedicated to discussions of the critical need for fairness in pre-
trained language models [54, 150, 278]. Some studies [188, 282, 464] focused on improving fairness in
PLMs. For instance, [464] eliminated gender bias using instruction text. [188] alleviated the transfer of
bias from PLMs to student models during the distillation process. [282] employed the approach of PEFT
to eliminate biases from the model. Meanwhile, several studies such as [334] have focused on enhancing
bias quantification metrics. It has been discovered that current metrics inherently incorporate bias [494]
and are occasionally unreliable [118].
Hallucination
Another challenge for large pre-trained models is hallucination. In general, it refers
to the state that the machine learning models, in particular, natural language generation models, can
generate texts that are plausibly correct, but wrong [233].
While [306] associated hallucination with the inability of the model to understand domain-specific
knowledge, the broader scope of the community tend to consider it more generic. For example, it can
35


--- Page 36 ---
happen in many generic text-generation task such as machine translation [422] and image captioning
[52, 429].
More formally, hallucination is defined as “the generated content that is nonsensical or unfaithful to
the provided source content” [233, 335]. There are also different definitions available [130, 285, 422, 513].
In summary, we believe the key differences of these definitions lie in the agreement on where the factuality
should be aligned to, which again aligns with our general theme of trustworthy machine learning in this
paper: the complete definition must be specified by the stakeholders.
4.2
Techniques with Large Models
This section introduces several techniques commonly used with large models. In Sections 4.2.1-4.2.3,
we introduce fine-tuning, prompting, and parameter-efficient fine-tuning, respectively. These techniques
were initially designed to improve the performance or efficiency of large models. Section 4.2.4 introduces
RLHF [98], a technique that aims to align the behaviors of language models with human values.
4.2.1
Fine-tuning
Fine-tuning refers to the process of modifying the weights of a pretrained model for a downstream
task.
Recent work has shown that fine-tuning changes the underlying representation of data by in-
creasing the distance between different labels, and thus, leads to better generalization [663]. However,
a significant challenge in fine-tuning is the lack of robustness in the adjusted weights compared to the
pre-trained models [196, 342, 351]. To increase the robustness of fine-tuned models, researchers have
suggested different approaches [235, 237, 277, 317, 516, 594, 640, 641], which can be broadly categorized
as architecture-driven and data-driven techniques.
Architecture-driven approaches [237, 317, 516] deal with fine-tuning the model with a different ap-
proach compared to the conventional one. For example [237, 516] discard the idea of fine-tuning all the
parameters of the pre-trained model. ROSE [237] select parameters based on dropout using adversarial
perturbation and tuning parameters that are less aggressively updated to avoid overfitting on spurious
patterns. Instead of selecting parameters for updates or layers, [516] automatically imposes the con-
straint between the model weights and projection radii, i.e. the distance constraints between the layers
of the fine-tuned and pre-trained model through weight projections to retain robust features in the fine-
tuned model. Furthermore, [317] retains the pre-trained features by fine-tuning the pre-trained models
to the downstream task by training dual networks (pre-trained and fine-tuned) with shared parameters,
excluding the batch normalization layers. Without changing the architecture, [277] suggests employing
linear probing followed by fine-tuning gives good results on Out-of-Distribution fine-tuning. Similarly,
[89] proposes GNOME to penalize the representation related to semantic and non-semantic shifts move,
in case it moves away from their respective mean. It argues that fine-tuned pre-trained language models
(PLMs) work on a trade-off between the semantic and non-semantic shifts. Furthermore, they find out
that fine-tuning PLMs on redistribution data benefits detecting semantic shifts (OOD consists of unseen
classes in the in-distribution task) but severely deteriorates detecting non-semantic shifts (OOD consists
of a change in background information, but has same classes). Similarly, to infuse robustness in the fine-
tuned model, the work [201] focuses to preserve the pre-trained features to help calibrate the fine-tuned
PLMs by maintaining consistency between their trained features.
On the other hand, the data-driven approaches [235, 594, 640] achieves robustness for fine-tuned
models by data-oriented techniques, like [235] insert additional hallucinated data with pseudo labels to
provide the fine-tuned model with a more diverse dataset to avoid overfitting on the few-shot learning
task. On a similar token, [640] generate adversarial text via curriculum learning method [48] to also
allow intermediate text as the sample becomes more uncertain. [61] discusses how covariate shift affects
the performance of BERT poorly and how infusing covariate drift augmentation transforms to make the
fine-tuned model distributionally robust, while also improving its performance. Furthermore, it focuses
to learn glymph and phonetic features via adversarial graphs as these features are usually perturbed
to generate adversarial text.
[594] utilize the third level of causation 3.3 to generate counterfactual
images by masking semantic or non-semantic features and filling them with randomly sampled images.
Meanwhile focusing to maintain consistency between the prediction of the pre-trained and fine-tuned
model to transfer robustness from the pre-trained model. Based on these works, infusing robustness in a
hybrid mode, i.e. using both the data-oriented and model-oriented way seems like a desirable solution.
One such work, [641] executes it for backdoor attacks by cleaning the potential backdoor embeddings,
which is done by aligning the embedding of top words with that backdoor model of the pre-trained
36


--- Page 37 ---
model. It also proposes a strategy to mix the backdoor model weights and pre-trained weights to train
them on a small set of clean data.
Another branch of works [287, 601] focuses on solving the problem of catastrophic forgetting knowl-
edge gained in the pre-training phase. From a broad perspective, the researchers have tried to solve this
by selectively fine-tuning different parameters/layers in the model, as fine-tuning additional parameters
on small datasets can cause forgetting[287]. For example, [287] selectively fine-tunes the layers of the
model based on the distribution shift, also unlike conventional methods, which selectively fine-tunes the
last or last few layers. It also gives liberty to only fine-tune the model using initial layers. Similarly, to
prevent unstable fine-tuning [601] focuses on balancing the unbalanced gradients across different com-
ponents thought the “component-wise gradient norm clipping” method to adjust the speed for different
type of parameters. To make the fine-tuned model more trustworthy [472] aims to protect privacy leakage
for the sensitive tokens defined in a policy function by fine-tuning large-transformer-based models. It
achieves it by fine-tuning the model twice, i.e. firstly by masking the sensitive tokens to fine-tune the
model on in-distribution data, giving a good initialization to the second phase of training on original
in-domain data using a private optimizer [2].
Apart from these works, there exist other works [64, 120, 176, 266, 326, 472, 609, 616, 632] which
focus on refining the fine-tuning technique to make it data-efficient and more effective.
They [266]
employ it by proposing new loss functions including additional loss functions by non-linearly combining
losses corresponding to different auxiliary tasks. Furthermore, [632] proposes two new regularizers to
accomplish the isotropic property of feature space for intent classification. [176] argues that one should
not abandon the fashion of pre-training and fine-tuning ought to be done in the same manner.
It
accomplishes it by employing the contrastive loss between the class-descriptive prompt embedding and
image embedding.
Similarly, to make the process more efficient [64] harness the idea of curriculum
learning [48], i.e.
by learning easier concepts and progressing to harder ones.
It fine-tunes smaller
language models on these sub-tasks to learn a target task. [120] harnesses reinforcement learning for
zero-shot fine-tuning an out-of-box image neural captioner by updating the language part. It uses the
reward from the retriever, which selects the original image from which the caption was generated.
To make the process of fine-tuning more data-efficient [121] innovates upon active annotation. AC-
TUNE fine-tunes BERT [121] based on pseudo-labels obtained for low-certainty examples while actively
annotating the high-uncertainty samples. Furthermore, [326] proposes not to only have minimum label-
ing cost while randomly selecting the data but also focus on low acquisition latency. Additionally, it
focuses on both diversity and uncertainty by selecting the cluster centers of acquired data points and
selecting the ones having low-confidence prediction or high entropy to gain uncertain examples.
4.2.2
Prompting
Prompting, or prompt learning, involves the process of using a template to alter the input, which can
be done manually or automatically. The template is tailored to suit the specific task requirements. By
making the form of the downstream task more similar to the pre-training task, we hope to better leverage
the capabilities of the pre-trained model so that the downstream task can be performed with reduced or
no need for additional training.
In prompt learning, we fill the input into a fixed location of the template, and let the model generate
output at the reserved location, usually at the middle of the text for bi-directional language models,
or at the end of the text for auto-regressive models. However, sometimes the model output cannot be
used directly as the answer. In this case, another step is often taken to map the output to the answer
space, often known as verbalization. Prompts can be obtained by manual engineering [62, 396, 451]
or automated search [240, 473].
In manual prompts, we manually define an appropriate prompt to
guide the PLM towards completing a task. This process is more interpretable, but it relies more on
human expertise and may not yield optimal results [240] compared to automated search. In automated
engineering, various algorithms are employed to discover the most effective prompt for a given task.
Prompts can also be categorized as “discrete prompts” [44, 115, 155, 199, 240, 474, 536] and “continuous
prompts”. Discrete prompts are composed of words and are represented as strings, whereas continuous
prompts pertain to prompts that exist within the embedding space of the model. Continuous prompts
are considered as independent parameters and can be optimized from scratch or initialized using discrete
prompts [192, 405, 474, 656]. They can be incorporated into the sentence at the beginning [290, 296, 522]
or at more flexible locations [194, 313] as embedding vectors.
Instead of using single prompts to arrive at the desired answer, recent works [20, 155, 192, 240, 290,
405, 450, 450, 450, 451, 452, 452, 618] used multiple prompts of the input.
These multiple answers
37


--- Page 38 ---
were used by some works [240, 405, 450, 450, 452, 618] to predict the output. Meanwhile other works
[20, 155, 450, 452] leveraged different small models as intermediaries corresponding to each of the multiple
prompts to get trained and predict different outputs, which can then be used to predict the final output.
There has been a growing interest recently in the research of a special kind of prompt learning, known
as in-context learning (ICL) [62]. In ICL, a few input-answer pairs are incorporated into the prompt as
demonstration data, helping the pre-trained model adapt to the new task without any training. ICL has
shown to effectively improve the performance of PLMs in a variety of relatively complex tasks [62, 575].
Several works have tried to understand the working mechanisms of ICL. [18] found that transformer
possesses the ability to implement learning algorithms for linear models implicitly in their hidden states,
and update these models according to the demonstration samples.
Other works [17, 110, 294, 533]
discussed the theoretical connections between ICL and fine-tuning, as well as their similarities in empirical
behaviors.
Mathematically, the method of in-context learning can be defined using Equation 42
ˆP(y|x) = f(y|x, {(xi, yi)}i=n
i=0, I; θ)
(42)
where I is a natural language instruction of the task, and (xi, yi) is the ith pair of demonstration
input and answer. The ICL ability can be improved using model warm-up, which acts as an intermediary
fine-tuning stage between the pre-training and ICL inference stages. Based on the finding that ICL
performance benefits from the diversity of pre-training corpus, [86, 95] fine-tuned the parameters of the
language models such that they outperformed language models with larger sizes. [574] found that adding
instructions while pre-training acts as a performance booster. On the other hand, many other works
have explored the formatting or selection of demonstration examples. [310, 310, 437, 484] focused on
selecting examples such that there is high information sharing [310, 437] between the query input and
the example based on different metrics such as distance or mutual information [484]. Similarly, example
order plays a crucial role in ICL performance [310], where the rightmost example should be highly similar
to the input sample.
4.2.3
Parameter-efficient Tuning
Fine-tuning involves modifying the weights of a pretrained model for a specific task. However, fine-
tuning all parameters in a large language model is highly resource-intensive due to the exponential rise
in the parameters from LMs like BERT [121] with 350 million parameters (released in 2018) to GPT−3
[62] containing 175 billion parameters. It is not feasible or cost-effective to train the entire language
model from scratch for each task.
As a result, a sub-field called “Parameter-Efficient Fine-Tuning”
(PEFT) has emerged, which aims to minimize the computational, memory, and storage demands for
fine-tuning. The community has proposed various ways [127, 138, 185, 212, 214, 217, 309, 398, 399, 438,
498, 499, 534, 569, 625] to accomplish PEFT, with the common ground of involving fewer parameters for
updates during fine-tuning. The main challenge lies in identifying the optimal and minimal weights for
fine-tuning the model. Certain methods [138, 214] attempt to address this by identifying the low-rank
representation of current weights of LM model, and hence infusing them with the pre-trained weights as
trainable parameters, while keeping the original weights frozen. Research [10] supports this approach by
considering pre-trained models as low-dimensional intrinsic models that can learn effectively even when
projected onto a random subspace.
In addition to obtaining the low-rank representation for fine-tuning, certain studies [127, 185, 499, 534,
625] identified the most suitable parameters [185, 499, 534, 625], and selectively fine-tuned layers [127]
of the model. For instance, [625] manually selected specific parameters for update, such as the “bias”
parameter. On the other hand, [185, 534] adopted a model-agnostic approach by masking parameters
during training. In [534], parameters were categorized as trainable or non-trainable, while [185] masked
parameters to determine their relevance during training. To reduce the computation cost of finding the
important parameters, [499] proposed to pre-compute the crucial parameters to be fine-tuned during
training using the Fisher Information.
Furthermore, several studies [212, 217, 309, 398, 399, 438, 498, 569] have explored the incorporation
of additional parameters into neural networks. These extra parameters can only be re-parameterized
during the fine-tuning process while the existing weights remain unchanged. The concept of adapters
[212] represents an early example in this category, where adaptive training modules are added within
the transformer layer.
Subsequent research [217, 398, 399, 438, 569] has expanded on this concept
to further improve the efficiency of fine-tuning. For multi-task learning, AdapterFusion [398] trained
an adapter module for each task and utilized a fusion module combining these adapters to tackle a
38


--- Page 39 ---
specific task. AdapterDrop [438] improved the memory efficiency of [398] by pruning adaptors in lower
transformer layers. [217] integrated several methods such as Series adaptor [212], Parallel adaptor [399],
and LoRa [214] into large models for various tasks. AdaMix [569] incorporated multiple adapter layers,
randomly selected for each forward pass, to enhance performance. Mad-X [399] further introduced an
invertible adapter architecture, which used the same set of parameters for adapting both input and
output representations for cross-lingual transfer.
In addition to adapter methods, other techniques [309, 498] also focus on introducing additional
parameters for task-specific fine-tuning. [309] added parameters that scale the activations in the trans-
former layers based on the inferred task. LST [498] trained a small side network detached from the
pre-trained model, and improved memory efficiency by avoiding backpropagation through the backbone
network.
4.2.4
Fine-tuning with human feedback signals
Large language models are often pretrained to predict the next token given the context in a corpus, which
is different from the objective to “follow the user’s instructions helpfully and safely” [62, 148, 409]. This
misalignment in objectives may be the primary reason behind the widely observed phenomenon that
Pretrained Language Models (PLM) sometimes produce uninformative, toxic, or misleading response
[29, 378] despite the ever-increasing size of model parameters and pretraining corpus. To alleviate this
problem, Reinforcement Learning from Human Feedback (RLHF) [98, 486] has recently been used to
steer a pretrained language model towards intended behaviors by incorporating human preference signals
into the training pipeline. RLHF was originally proposed to train agents in relatively simple simulated
environments [35, 98, 228], and was later used in specific language tasks such as text summarization
[55, 486, 587, 668], machine translation [34, 273], dialogue system [195, 232, 613], semantic parsing
[283], and story telling [662]. As large language models show strong cross-task generalization abilities
with in-context learning [62, 129] and instruction tuning [574], the community has recently focused on
using RLHF to align a language model with human values to perform a wide range of tasks [378], as a
general-purpose chatbot or language assistant [37, 153].
In a typical RLHF process [37, 378, 486], a dataset is carefully curated by collecting user-generated
prompts paired with human-written answers as demonstration. This dataset is used to fine-tune the
PLM in an initial warm-up stage. Then, for each prompt, different answers are collected from the output
of different versions of the PLM, the human demonstration, and various baselines. A group of annotators
were asked to rank the quality of K answers (K ≥2) to the same prompt, according to some predefined
human values, and all the
 K
2

pairs from each prompt are used to construct a comparison dataset. A
reward model is trained to predict which answer in a pair is preferred by human by assigning a scalar
reward to each answer according to the below formula
loss(θ) = −1
 K
2

E
(x,yw,yl)∼D[log(σ(rθ(x, yw) −rθ(x, yl)))]
(43)
where rθ(x, y) is the output of a reward model for prompt x and answer y with parameter θ, yw is the
preferred answer in the pair of yw and yl, σ(·) is the Sigmoid function, and D is the distribution of the
comparison dataset. The output of the reward model is then normalized to have a mean of 0 across the
dataset [486, 668].
With the learned reward model, we can sample answers with the PLM and optimize the expected
reward using the PPO algorithm [454]. The auto-regressive generation of the PLM can be seen as a
Markov Decision Process, where the state is the concatenated string of the prompt tokens and previously
generated answer tokens, and the action is to predict the next token and append it to the current string.
At the end of the generation, a reward is produced by the reward model and assigned to each token,
with a KL loss to prevent the current policy from deviating too much from the original policy.
R(x, y) = rθ(x, y) −β π(y|x)
π0(y|x)
(44)
where π are π0 are the current and original policy models respectively, and β is a coefficient. The collection
of comparison data and PPO retraining of model can be done iteratively. [37] update the reward models
and RL policies with fresh human data on a weekly basis to continuously improve the quality of the
dataset and models. Recent works have also tried to improve the RL algorithm. [417] employ a policy
mask by selecting the top p tokens, which reduced the action space and stabilized training. They also
developed an open-source library and a benchmark to optimize and evaluate language models with RL.
39


--- Page 40 ---
[483] extend the Implicit Q-learning method [271] to language generation tasks and show improved
stability and performance.
Reinforcement learning is not the only way to incorporate human feedback signals in the training
process. [128] learn a reward model from human preference and fine-tune the PLM on the highly ranked
samples. [619, 653] align the model-produced probabilities of answers with their human rankings. [598]
fine-tune a diffusion model towards better scores from a reward model for text-to-image generation. [658]
used vanilla fine-tuning instead of RL on a small amount of instruction data, and got similar performance
to RLHF method. This is differnt from the empirical finding in [417] that RL-based methods are 5 times
more data-efficient than vanilla fine-tuning for aligning a PLM. The comparison and relationship between
RL and fine-tuning remain at the forefront of active discussions.
Note that these methods of optimizing a PLM toward the preferences of a group of annotators still
exhibit certain limitations at the current stage. According to the self-evaluation of recent works [37, 378],
although RLHF brings significant improvements in truthfulness, informativeness and toxicity, it does not
help reduce bias towards social groups, and is not designed for robustness towards distribution shifts or
adversarial attacks. [37] self-identified lack of robustness of their reward model, which leads to overfitting
of the policy model during training. Some later works [92, 549] did systematic evaluation and found that
current large models fine-tuned with RLHF show improved robustness but still suffer from significant
performance degradation under adversarial attacks and distribution shifts.
4.2.5
Connections to ERM
We have discussed three prominent techniques tailored for large models, namely, fine-tuning, prompting,
and parameter-efficient fine-tuning.
All these techniques impose some restrictions on the hypothesis
space to effectively preserve the “knowledge” in the pretrained model. Their training objectives also
have a lot of commonalities.
Fine-tuning adopts the following objective
arg min
θ
E
(x,y)∼P (X,Y ) l(f(x; θ), y)
(45)
where θ0 = θpretrained, meaning we initialize the model with the parameters of a pretrained model.
Typically, we set a small learning rate and train the model for fewer epochs compared to a standalone
model, so that the solution ˆθ∗is close to θ0.
Parameter-efficient fine-tuning adopts the following objective
arg min
θ
E
(x,y)∼P (X,Y ) l(f(x; [Θ; θ]), y)
(46)
where Θ represents the parameters to be fixed and θ represents the parameters to be trained.
In
particular, for adaptor-based methods, Θ represents all parameters of the pretrained model, and θ
represents the parameters of the adaptor. θ usually has far less parameters than Θ, i.e., |θ|≪|Θ|.
Prompting adopts the following objective
arg min
θ
E
(x,y)∼P (X,Y ) l(f(h(x; θ); Θ), y)
(47)
where h(·; θ) is an encoder that modifies the input to make it more easily processed by the pretrained
model. A special case is “hard prompt”, where θ consists of embeddings of natural language tokens.
Again, |θ|≪|Θ|.
It can be seen that although these techniques differ in their parameterizations, they all adopt the
objective to minimize the empirical risk on the training data. In Section 2, we have discussed various
improvements to the ERM objective to equip a standalone model with trustworthy properties, and we
will see that they are applicable to large pretrained models as well.
In other words, since the master equations Equations 13, 14, and 15 can all be built upon the ERM
(Equation 1), these equations should also be built upon the above extensions of ERM (Equations 45, 46,
47) to boost their trustworthiness properties.
4.3
Trustworthy Solutions for Large Models
In this subsection, we present techniques aimed at enhancing the trustworthiness of large models. In
Sections 4.3.1, 4.3.2 and 4.3.3, we review recent methods that extend the trustworthy methods originally
40


--- Page 41 ---
developed for standalone models, to large pretrained models with different parameterizations of the ERM
objective. Lastly, in Section 4.3.4 we introduce other techniques that boost the trustworthiness of large
models.
4.3.1
Domain-invariant representation learning
One line of work focuses on learning representations that exhibit certain invariance across environments.
We consider this line of works building upon Equation 13, although these works might not be invented
through this process explicitly.
For instance, [234, 586] used DANN on the representation of a prompt token to align the features
across different domains for text classification tasks. [234] additionally minimized the KL divergence of
predicted probability distributions of tokens for each category across different domains. [146] designed
adversarial fine-tuning tasks that perform gradient reversal on token representations associated with a
chosen concept, to remove bias in the model related to that concept. They also compared the predictions
of the original and debiased models to evaluate the treatment effect of the concept. [624] used DANN for
the domain adaptation of complex word identification models in multilingual and multi-domain settings.
[622] used the MMD loss on the token representations for the domain adaptation of question answering
models. [593] used the MMD loss to align the feature distribution of the new and old data for continual
learning. [615] partitioned the samples to different domains using an unsupervised method, and then
used IRM to improve the OOD performance of several language understanding tasks.
[425] used a
combination of inter-domain and intra-domain contrastive learning, to push closer the representations
of samples within the same category but different domains, for the domain generalization of image
captioning models.
4.3.2
Data augmentation methods
A large body of work uses data augmentation to enhance the trustworthy properties of pretrained models,
which corresponds to Randomized Controlled Trial in causal inference (Section 3.2) and master equation
Equation 14.
For example, [187] used a bias-inducing prompt to compare the different predictions of the PLM only
at the change of the demographic words, and aligned the predicted distributions of the masked token
to mitigate bias. Similarly, [203] generated counterfactual samples by gender word substitution, and
used contrastive learning and regularization to align the counterfactual pairs. [564] used interpretability
methods to find the tokens important to the model’s prediction, and identified the bias tokens among
them by cross-corpora analysis and knowledge-aware perturbation. They found that masking the tokens
at training or inference time improved the robustness of models. [594] masked patches of the images
based on the class activation map and refilled them with randomly sampled images. [65] masked domain-
related terms in the source domain text input, and use a T-5 model to reconstruct the counterfactual
text corresponding to the target domain. Similarly, for open-domain dialogue system, [376] identified
and replaced the keyword corresponding to the response perspective, and used BART to reconstruct a
response with different semantics for the same dialogue history. Data augmentation often comes with
task-specific designs. For example, for the continual learning of relation extraction models, [556] used
a mix-up style augmentation to interpolate between labels, and reversed the asymmetric relations to
create new relation types. For entity typing, [600] prompted a PLM to identify six types of biases in
the dataset, and devised counterfactual augmentation for each kind of bias. For factual probing, [297]
considered the symmetry of tasks (subject prediction and object prediction), and designed a symmetric
prompt enhancement method for prompt learning.
Sometimes it is easier and more flexible to augment the data in the representation space. [622] used
data augmentation for the domain adaptation of question answering models. They sampled augmented
embeddings of question tokens from the convex hull spanned by the 2-hop synonyms of the token. [646]
improved adversarial robustness of language models by data augmentaion in the embedding space. [117]
proposed a method to generate label-flipping counterfactual text representation. They used the mean
difference between the text representation of both labels as the displacement vector to perturb the
original text representation. For domain adaptation, [318] generated domain-adversarial perturbation
on the sample representation that fools the domain classifier into predicting the target domain, and
used contrastive learning to push the counterfactual pair closer, using some other samples in the original
domain as negative samples. [205] sampled counterfactual and factual image features and used contrastive
learning to learn more robust prompts.
41


--- Page 42 ---
4.3.3
Sample weighting methods
Some other works have used techniques to weight training samples based on their difficulty or biased
content, capturing the principles of group-DRO [443] to model the worst-case distribution shift, or inverse
probability weighting (Section 3.2.3) to establish independence between the confounder and treatment
variable. This line of work corresponds to the line of works we summarize in master equation Equation 15.
For instance, [617] clustered the input queries using K-means, and used the implicit DRO loss to
make dense retrieval models generalize better to unseen queries. [202] weighted training data with the
Focal Loss [305] for the imbalanced classification in bug detection. As a special case of weighting, some
works selected part of the samples for training and discarded the rest. [492] studied active learning for
domain adaptation tasks where the source domain data are not available. They selected target domain
samples with a large entropy from the source domain model, and manually annotated them for fine-
tuning. [61] clustered the samples and then selected samples in the k most difficult clusters, to train a
model that is robust to covariate shift for spoken language understanding. For generation tasks, some
works identified the biased samples and removed them for fine-tuning a large model. [128, 653] filtered
the samples using a reward model trained with human feedback data. [591] used z−filtering to remove
bias-aligned samples and fine-tuned a GPT-2 model towards generating unbiased training data for the
NLI task. They extract spurious features based on prior knowledge of the task, and used z−statistics to
measure correlation between the bias and the label.
In Section 4.2.4 we discussed RLHF, which can also be used to boost the trustworthiness of large
models. However, it seems that RLHF and the above-mentioned methods are orthogonal in the aspects
of trustworthiness they are good at. RLHF is good at the human-centered concepts such as politeness
or adherence to user’s instructions, which are hard to be mathematically defined. On the other hand,
robustness and fairness can be more effectively approached with statistical and causal inference methods
(Section 2, 3). We hypothesize that a combination of both family of methods would endow large models
with more trustworthy properties.
4.3.4
Other trustworthy methods
We will also briefly introduce some other trustworthy methods which do not fall into the master equa-
tions of our survey. As large language models have shown stronger generative abilities, recent works
have focused on a new approach to explainability known as self-rationalization, in which the model pro-
vides rationales for its own predictions [579]. There are primarily two kinds of rationales: extractive
and abstractive.
Extractive rationales [288] involve extracting fragments from the input to serve as
explanations, while abstractive rationales [579] generate free text for explanations.
For instance, [81] demonstrated that training a language model to produce extractive rationales can
potentially enhance its robustness against adversarial attacks. [332] used prompt learning to fine-tune
a language model in a few-shot setting, enabling it to generate abstractive rationales. They observed
that the quality of rationales and the performance on end-tasks both improved as the scale of the model
increased. [331] proposed a technique where an image is sent with a learned “why prompt” as input to
help the visual model not only recognize the object in the image but also explain why that object is
predicted.
Self-rationalizatin is closely related to another line of work that focuses on enhancing the reasoning
abilities of language models. For instance, in chain-of-thought prompting [575], reasoning steps were
incorporated into demonstration examples for in-context learning, prompting the model to generate
reasoning steps before predicting the answers to target questions. This led to consistent performance
gain on arithmetic, symbolic, and commonsense reasoning benchmarks. Another study [268] discovered
that using a simple prompt such as “Let’s think step by step” could elicit reasoning steps and enhance
performance on reasoning tasks. Subsequent works such as [90, 537, 660] have explored alternative ways
of prompting or more reasoning tasks.
A growing body of evidence suggests that training or prompting a model to provide rationales or
reasoning steps can improve its performance on various tasks that require reasoning [415, 575], and
also enhances out-of-distribution robustness [25, 575, 660]. Self-rationalization also provides a way to
potentially improve the reliability of outputs by verifying the truthness of rationales [101] or ensembling
different rationales [567]. For a more comprehensive overview, we direct interested readers to the survey
papers on language model reasoning [219] and rationalization [189].
There are other trustworthy techniques for large models that are not included in our survey. One line
of work augments the large model with retrieval or web search functionality [190, 230, 312, 357, 416] so
that it can make predictions based on the grounding text. This helps the model to access truthful and
42


--- Page 43 ---
up-to-date knowledge and reduce hallunication [476]. Some other works modify the model’s behaviors
at inference time for more reliable predictions [159, 339, 452, 525, 672].
While these methods have
great application values, in this survey we focus on techniques that make large models inherently more
trustworthy.
4.4
Summary
In this section we discussed the trustworthy challenges, common techniques, and trustworthy solutions
for large models. We discussed prompting, fine-tuning and parameter-efficient tuning, and unified them
under the ERM objective, thereby showing that the master equations discussed in Section 2 can be
extended to large models. The RLHF technique seems to be complementary to our master equations in
the aspects of trustworthiness it improves. As new techniques for large models are being continuously
proposed, we believe their combination with the master equations has the potential to further enhance
the trustworthiness of large models.
5
Applications Where Trustworthy Machine Learning Play Im-
portant Roles
In this section, we will focus on application scenarios where trustworthiness of models plays an important
role. We introduce recent efforts that evaluate the performance of large models in these scenarios, or
discuss their limitations or possible solutions. We will also give a brief overview of recent works that
develop trustworthy methods focusing on the application perspective. We categorize our discussion into
vision applications, language applications, and vision-language applications.
5.1
Vision
In vision applications, we often want a machine learning model to understand the semantic information of
images, whereas the non-semantic factors such as the texture, background, viewpoints, lighting conditions
and all kinds of noise can act as confounders that hurt the trustworthiness of vision models.
Some
application scenarios also suffer from the data scarcity and long-tail distribution problems. Different
methods have been proposed to tackle the challenges in a variety of tasks including image classification
[15, 78, 137, 178, 401, 406, 482, 508, 530, 538, 565, 651, 652], object detection [222], object localization
[463], co-saliency detection [270], facial recognition [93], emotion recognition [94], semantic segmentation
[466, 631], meta-learning [466, 621, 623], sequential tasks [80, 314, 370, 493, 636], dataset preparation
[270, 338, 423, 611], knowledge distillation [119, 216], and medical imaging [71, 174, 231, 333, 343, 343,
371, 377, 403, 447, 480, 481, 578].
Many works have employed data augmentation methods to extract causal features or remove con-
founders [78, 136, 270, 401, 449, 508].
A subset of the literature approached the data scarcity problem by robust few-shot learning, inter-
vening on features, normalizing them, or using synthetic images [466, 621, 623]. Several studies aimed to
mitigate bias in sequential tasks, often through treatment effects [80, 370, 493, 636]. For knowledge dis-
tillation, [119] used backdoor adjustment (Section 3.2.1) to eradicate the confounding effect of knowledge
prior which determines the context or background features of the image. Additionally, [216] quantified
the effect of an intervention, to eradicate the forgetfulness problem in student models by enforcing the
same representation from the student model.
Furthermore, several works [15, 137, 178, 482, 530, 538, 651, 652] tried to enhance the interpretability
of the models, mostly by generating an image that flips the label to emphasize the important features
responsible for the original label. For example, [482] aimed to measure robustness in robot control by
quantifying the change required to generate a counterfactual image that has a ground-truth label but is
predicted wrongly by the robot.
Additionally, some studies have created vision datasets for training trustworthy models [270, 338, 423].
For example, [338] provided a tool for constructing data with specified characteristics for use with self-
driving cars. The tool offers a high-fidelity simulation environment that can be used for causal and
temporal reasoning, providing control over variables such as environment characteristics, vehicles, and
traffic lights.
43


--- Page 44 ---
Medical Imaging
Machine learning applied to medical imaging holds great potential and societal
value. By automatically recognizing the patterns in imaging data, machine learning models have the
potential to greatly reduce the workload of radiologists and alleviate the strain in medical resources.
They may also detect the subtle abnormalities reflecting the early stages of disease which may not be
easily recognizable by humans [144]. However, the high-stake nature of medical imaging places high
demands on the trustworthiness of machine learning models, because a wrong prediction could bring
severe outcome to the patient’s health. The model should be interpretable enough for medical experts to
easily validate and adopt its predictions. Distribution shifts are common in imaging data due to different
medical devices, imaging procedure and individual characteristics. High-quality labeled data are often
scarce, due to privacy concerns and the expensive cost of annotation, which may lead to overfitting
problems [71].
Recent works have evaluated the performance of vision foundation models on medical imaging tasks.
[471] evaluated the SAM model on medical images in different applications including dermatology, oph-
thalmology, and radiology. The author discovers that SAM is not able to perform optimal for certain
structured targets like continuous branch structures. However, fine-tuning the model on small amount of
this data leads to several improvements. Similarly, [336] evaluated the SAM model on medical imaging
tasks using prompt learning. They discovered that prompt performance varies on datasets and does
perform poorer on ambiguous segmentation tasks. Furthermore, the author discover that when prompts
are provided iteratively their performance does not improve as much as the previous models. Besides
numerical evaluation, [198] took a societal view and highlighted the marginalization of people with rare
diseases in the dataset, which may cause unintentional discrimination.
Different methods have been proposed to improve the trustworthiness of models for medical imag-
ing.
[71] discussed the importance of causal features to alleviate issues arising from domain-shift.
Other studies such as [377, 403] focused on delineating invariant features. [403] distinguished between
causal and contrast features for COVID-19 CT scans, whereas [377] employed augmentation and pseudo-
correlations to address domain shift issues in cross-modality (CT-MRI) abdominal image segmentation,
cross-sequence (bSSFP-LGE) cardiac MRI segmentation, and cross-center prostate MRI segmentation.
Moreover, [481] employed treatment effects (Section 3.3.2) to perturb various clustered units representing
human-understandable concepts, aiming to identify units that serve as causal features. For interpretabil-
ity, the study also trained a decision tree to map output to an explanation, thereby creating a rule-based
explainer.
Techniques of counterfactual image generation have been utilized by [174, 231, 333, 343, 371, 403,
480, 578] to enhance the interpretability of medical models. Counterfactual images are generated to un-
derscore causal features [371], and features contributing both negatively and positively are revealed [333].
However, the task of image generation often results in poor quality. To mitigate this, [343, 480, 578]
focused on improving the quality of generated images by emphasizing textural and structural information
[343] for pneumonia classification from X-rays. Additionally, [447] evaluated the reconstruction quality
of different segments of chest X-ray images, along with object retention. Yet, focusing solely on intensi-
fied discriminatory pixels can compromise the interpretability of less-intensified pixels that nonetheless
contribute significantly to prediction. To address this, [578] proposed using a regression equation to
quantify the contributions of different image segments rather than relying solely on visual appearance.
This work also incorporated the factor of overdetermination to provide a broader view of the factors
contributing to a label.
Counterfactual explanations for medical images with high uncertainty occasionally produce blatantly
incorrect explanations, but these can be improved [231] through reliable elucidation of the intricate
relationships between image signatures and the target attribute. Nevertheless, all the aforementioned
techniques might not perform well with medical images exhibiting high variability in data-shift.
As
a result, [174] sought to discover the statistical relationship between inter-species images, where the
DL model fails due to its low generalizability properties. This work disentangled the information of
medical images using a Directed Acyclic Graph to generate counterfactual images based on low-level and
high-level features.
5.2
Language
Large language models have significant application potential in multiple areas such as medicine, edu-
cation, software engineering, law, and finance. In the medical domain, large models can quickly digest
and interpret the vast amount of medical literature, helping clinicians and researchers keep up with the
latest research findings and treatment options. This is particularly helpful given the exponential growth
44


--- Page 45 ---
of publications [511]. Based on the research literature and the latest medical guidelines, large models
can also aid in clinical decision-making [478]. Medical chatbots can be developed to automate certain
aspects of patient communication, such as answering frequently asked questions of patients [164]. Large
models can also provide initial analysis on mental health issues [605], improving the accessibility of men-
tal health support. In the education domain, large models can aid in producing learning content and
facilitate more accessible and personalized learning experience for individuals at all levels of education
[253]. For example, it can promote the curiosity-stimulating learning of children [4], explain the difficult
parts of learning materials, and provide information to college students on a particular research topic,
etc. For teachers, it can aid in lesson planning by generating candidate lesson activities and practice
problems, and semi-automate the assessment of students’ work, helping teachers provide timely and per-
sonalized feedback to students [253]. The potential applications of language models in other fields such
as law [107], finance [588], and software engineering [435] have also been actively explored by researchers
and practitioners. Due to the breadth of these applications, they are beyond the scope of this survey.
Readers interested in further exploration are encouraged to consult these referenced works.
In these scenarios, it is important for the model to provide truthful answers, and hallucination is a
big issue. Because large models excel at simulating the form of human language, a wrong response from
the model can be very misleading to humans. Due to the opaque nature of large models, it is not clear
whether the model has been equipped with the adequate domain knowledge, or whether it can correctly
conduct the reasoning steps over the knowledge, to generate a truthful response to the user’s question.
These problems, together with the privacy and fairness issues, affect the usefulness of language models
in high-stake application scenarios.
Recent works have evaluated the performance of large language models for various applications.
[113] evaluated the GPT-3.5 and GPT-4 models in healthcare with the help of different physicians,
based on the answers given by these models on several questions.
In conclusion, these models were
not defined as harmful in majority opinion, but sometimes generate inaccurate responses or hallucinate
references. [445] highlighted the utility of ChatGPT in healthcare education, research, and practice.
It gives an overview of how ChatGPT is affecting these domains of healthcare where it is being used
in improving the scientific writing. [467] discussed the merits and challenges of large language models
in clinical settings.
They highlighted the potential of these models to interact with patients, assist
physicians, and simplify the communication with insurance providers. However, their limitations include
generating unrealistic outputs (hallucinations), inability to access real-time data, and challenges with
specific tasks like summarizing patient history or image manipulation. The use of LLMs in professional
publications is also limited due to their authoritative style of writing. In the education domain, [253]
discussed the trustworthy challenges of large language models, deeming that these challenges are not
unique to education but inherent in the more generic technologies. They call for more intellectual efforts
in understanding these technologies and their unexpected limitations, as well as pedagogical approach
that emphasizes critical thinking abilities and strategies for fact checking.
Various methods have been proposed to address the trustworthy challenges of large language mod-
els, many of which are motivated by the notion of causality.
These works have focused on the ap-
plication of large models in various tasks such as sentiment analysis [51, 156, 254, 485, 562], natural
inference tasks [254], temporal relation prediction [156], question answering [156], and multi-label intent
classification [65].
Based on the concepts outlined in section 3, we can discuss the variety of meth-
ods around the second and third levels of the Causal Ladder. For example, the works [51, 281, 562]
harnessed the second level of causation to eradicate the confounding effects.
Specifically, [281] and
[51] used backdoor adjustment (Section 3.2.1) to achieve a causal-driven system. On the other hand,
[66, 140, 147, 156, 157, 254, 341, 485, 531, 589, 670] leveraged the third level of causation to solve
various problems regarding fairness [157, 531, 670], interpretability [140, 341] and robustness via data
augmentation [66, 147, 156, 254, 589]. For the task of online debates, [485] used treatment effect (Sec-
tion 3.3) to enhance the quality by estimating the causal effect of reply tones on linguistic and sentiment
changes. They mitigated the confounding effect of users’ ideologies acting as confounders. Similarly,
[531] alleviated gender bias in pre-trained language models via treatment effect. [156, 254] generated
the counterfactual samples via manual engineering. Meanwhile, [670] focused on generating counterfac-
tual data to mitigate gender biases via the replacement of keywords. However, manually generating the
data can be sup-optimal and time-consuming. Therefore, some research endeavors employed automatic
methods for counterfactual data generation. For instance, [589] fine-tuned GPT-2 on multiple datasets
of paired sentences to automatically generate counterfactual samples.
[66] generated counterfactuals
by infusing different domains for domain adaptation of a sentiment classifier and a multi-label intent
classifier. Regarding interpretability, [140, 341] used counterfactual intervention for identifying neuron
45


--- Page 46 ---
activations that are important to a model’s predictions.
5.3
Vision-Language
Vision-Language models are more flexible and versatile in the way they process information and interact
with humans, and therefore has immense potentials in medical applications. By integrating information
from different modalities, models could offer more holistic medical decision support [348]. They may also
support automatic generation of radiology reports by describing findings from imaging data [348], or
offer visual aid to patients for the chatbot to provide more easy-to-understand medical education. How-
ever, vision-language models inherit the trustworthy challenges from both vision and language models.
Additionally, we need to make sure that the model does not exploit heuristics from a single modality,
but integrates information from both modalities for prediction.
The performance of large vision-language models on medical tasks has been evaluated in recent
work. [77] studied prompt-based medical image generation and found that the Stable Diffusion model
[430] shows limited performance by both manual observation and the FID-score, before fine-tuning
on medical data. [347] explored the performance of pre-trained models on the medical domain task,
more specifically using BERT-based architecture to boost the multi-modal performance on radiology
images and unstructured reports. They modified the attention mechanism of the model in the medical
domain, where the attention map can attend to disease-discriminatory regions.
Furthermore, their
method is able to generate clinically appropriate reports using abbreviated medical terms. [407] studied
the transferability of vision-language models on the medical domain. They designed prompts by injecting
image-specific information to bridge the gap across medical domains and improve generalizability. They
developed prompts via automatic generation using three strategies: masked language model (MLM)
driven auto-prompt generation, image specific auto-prompt generation, and a hybrid of both. Apart
from numerical evaluation, [348] proposed to build generalist medical models by formally representing
medical knowledge. In this way, it can perform the reasoning to solve unseen tasks, raise self-explanatory
warnings, and give treatment recommendations.
Various methods have been proposed to alleviate the trustworthy issues of vision-language models on
tasks such as visual question answering (VQA), dataset preparation [30], sequential tasks [75, 151, 299],
and video-grounding [284, 360, 566, 643]. Video-grounding studies [284, 643] use counterfactual genera-
tion (Section 3.3). [284] utilized an object-action factorized Structural Causal Model (SCM) to explicitly
incorporate action and object information. [643] generated positive and negative counterfactuals for
the model to learn causal features.
In the same area, [360, 566] applied backdoor adjustment (Sec-
tion 3.2.1). [566] removed the spurious correlation between the background and causal features, while
[360] mitigated selection bias in the dataset, which causes the model to predict the moments based on
the presence of certain objects rather than the activities. For VQA tasks, several studies [11, 177, 247]
noted over-reliance on linguistic correlations rather than multimodal reasoning, which [368] addressed
through treatment effects (Section 3.3.2). Further, [85, 269, 302] generated counterfactual and factual
data using the third level of causation (Section 3.3). [85, 269] created counterfactual pairs of images
and questions to enforce robust predictions, while [302] used contrastive learning to adjust pair distance.
[3] increased model robustness by generating data, intervening on exogenous variables. For VQA ro-
bustness, [563] incorporated commonsense reasoning using backdoor adjustment (Section 3.2.1), while
[432] introduced augmented questions for resilience testing. In terms of dataset preparation, the CRAFT
dataset [30] introduced three types of questions: descriptive, causal, and counterfactual. The counter-
factual questions involve scenarios where an existing object is removed, and the causal questions involve
understanding the interactions between objects through notions like “cause”, “enable”, and “prevent”.
Regarding sequential tasks, various studies [75, 151, 299] tried to tackle different challenges. The study
[75] proposed iReason, which integrates a rationalizing causal module in vision and language captions,
emphasizing the perception of causal relationships in time-ordered events within videos. Similarly, [151]
focused on 3D navigation in visual surroundings using natural language instructions. The study applied
the concept of the third level of causation (Section 3.3) to sample challenging paths based on adversarial
learning, thereby making the model more robust for navigation tasks. In a similar vein, [299] worked to
remove long-tail bias in unraveling the dynamic interaction across visual concepts. The research argues
that non-uniformly distributed predicates can cause model biases, and therefore focused on incorporating
predicates uniformly via causal intervention for every subject and object.
46


--- Page 47 ---
6
Conclusion
In this survey, we review trustworthy and aligned machine learning. We focus on the four topics of
trustworthy properties, namely, robustness, adversarial robustness, fairness, and interpretability. We
start from the limitations of the i.i.d. assumption, and reviews recent methods to improve trustworthiness
of standalone machine learning models for different topics. Then, we revisit these methods to understand
them from Pearl’s causal hierarchy, and review trustworthy techniques that are directly based on or
inspired by causal inference methods. We move on to review the techniques commonly used with large
pretrained models, and show that the trustworthy techniques can be combined with them to address the
current limitations of large models.
Principled Solutions
Our survey found consistent solution patterns across various aspects of trust-
worthiness. These techniques were likely developed independently in their fields, but their convergence
suggests a universal applicability.
We believe these shared techniques will apply to large pretrained
models and may also be useful for future generations of machine learning.
Models with All Trustworthy Merits
Our survey covered topics like robustness, adversarial ro-
bustness, fairness, and interpretability. While there has been much research in each area, we found no
efforts to create models with all these properties at once. Our understanding of these aspects provides
an opportunity to build a model that combines all these merits.
Modern Trustworthy ML with Modern Causality Theory
We connected various aspects of
trustworthy machine learning to established causality theories.
As causality research advances, new
insights might inspire more effective trustworthy machine learning methods.
Additional Tiers of Trustworthiness
We focused mainly on the models’ statistical understanding
of data patterns, along with techniques such as augmentation and regularization. We predict that in the
future, this level of trustworthiness will be a basic requirement for AI, especially as AIs begin to work
together. More complex scenarios will likely demand higher levels of trustworthiness.
Acknowledgement
The collaboration of this survey was initiated by the reading group event from Trustworthy Machine
Learning Initiative (trustworthyml.org). One can find more information of the reading group from the
initiative or the gate website of this survey (trustAI.one).
References
[1] Aas, K., Jullum, M., and Løland, A. Explaining individual predictions when features are
dependent: More accurate approximations to shapley values. Artificial Intelligence 298 (2021),
103502.
[2] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., and
Zhang, L. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security (oct 2016), ACM.
[3] Abbasnejad, E., Teney, D., Parvaneh, A., Shi, J., and van den Hengel, A. Counterfactual
vision and language learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) (2020), pp. 10041–10051.
[4] Abdelghani, R., Wang, Y.-H., Yuan, X., Wang, T., Lucas, P., Sauz´eon, H., and
Oudeyer, P.-Y.
Gpt-3-driven pedagogical agents to train children’s curious question-asking
skills. International Journal of Artificial Intelligence in Education (2023), 1–36.
[5] Abid, A., Farooqi, M., and Zou, J. Persistent anti-muslim bias in large language models. In
Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society (2021), pp. 298–306.
[6] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., and Kim, B. Sanity
checks for saliency maps.
In Proceedings of the 32nd International Conference on Neural In-
47


--- Page 48 ---
formation Processing Systems (Red Hook, NY, USA, 2018), NIPS’18, Curran Associates Inc.,
p. 9525–9536.
[7] Adel, T., Valera, I., Ghahramani, Z., and Weller, A. One-network adversarial fairness.
In Proceedings of the AAAI Conference on Artificial Intelligence (2019), vol. 33, pp. 2412–2420.
[8] Adeli, E., Zhao, Q., Pfefferbaum, A., Sullivan, E. V., Li, F.-F., Niebles, J. C., and
Pohl, K. M. Bias-resilient neural network, 2020.
[9] Aghaei, S., Azizi, M. J., and Vayanos, P. Learning optimal and fair decision trees for non-
discriminative decision-making. In Proceedings of the AAAI Conference on Artificial Intelligence
(2019), vol. 33, pp. 1418–1426.
[10] Aghajanyan, A., Zettlemoyer, L., and Gupta, S.
Intrinsic dimensionality explains the
effectiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255 (2020).
[11] Agrawal, A., Lu, J., Antol, S., Mitchell, M., Zitnick, C. L., Batra, D., and Parikh,
D. Vqa: Visual question answering, 2015.
[12] Ahuja, K., Caballero, E., Zhang, D., Gagnon-Audet, J.-C., Bengio, Y., Mitliagkas,
I., and Rish, I. Invariance principle meets information bottleneck for out-of-distribution gener-
alization. Advances in Neural Information Processing Systems 34 (2021), 3438–3450.
[13] Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., and Marchand, M. Domain-
adversarial neural networks. arXiv preprint arXiv:1412.4446 (2014).
[14] Akhtar, N., and Mian, A. Threat of adversarial attacks on deep learning in computer vision:
A survey. IEEE Access 6 (2018), 14410–14430.
[15] Akula, A. R., Wang, K., Liu, C., Saba-Sadiya, S., Lu, H., Todorovic, S., Chai, J., and
Zhu, S.-C. Cx-tom: Counterfactual explanations with theory-of-mind for enhancing human trust
in image recognition models, 2021.
[16] Akuzawa, K., Iwasawa, Y., and Matsuo, Y.
Adversarial invariant feature learning with
accuracy constraint for domain generalization. arXiv preprint arXiv:1904.12543 (2019).
[17] Aky¨urek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning al-
gorithm is in-context learning? investigations with linear models. In The Eleventh International
Conference on Learning Representations (2023).
[18] Aky¨urek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algo-
rithm is in-context learning? investigations with linear models, 2023.
[19] Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Men-
sch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T.,
Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Ne-
matzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,
A., and Simonyan, K. Flamingo: a visual language model for few-shot learning. In Advances in
Neural Information Processing Systems (2022), A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho,
Eds.
[20] Allen-Zhu, Z., and Li, Y. Towards understanding ensemble, knowledge distillation and self-
distillation in deep learning, 2023.
[21] Amemiya, T. Advanced econometrics. Harvard university press, 1985.
[22] Amini, A., Soleimany, A. P., Schwarting, W., Bhatia, S. N., and Rus, D. Uncover-
ing and mitigating algorithmic bias through learned latent structure. In Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society (2019), pp. 289–295.
[23] Anderson, T. W., and Rubin, H. Estimation of the parameters of a single equation in a complete
system of stochastic equations. The Annals of mathematical statistics 20, 1 (1949), 46–63.
[24] Andriushchenko, M., Croce, F., Flammarion, N., and Hein, M. Square attack: a query-
efficient black-box adversarial attack via random search.
In European conference on computer
vision (2020), Springer, pp. 484–501.
[25] Anil, C., Wu, Y., Andreassen, A. J., Lewkowycz, A., Misra, V., Ramasesh, V. V.,
Slone, A., Gur-Ari, G., Dyer, E., and Neyshabur, B. Exploring length generalization in
48


--- Page 49 ---
large language models. In Advances in Neural Information Processing Systems (2022), A. H. Oh,
A. Agarwal, D. Belgrave, and K. Cho, Eds.
[26] Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S.,
Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y.,
Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder,
S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H., Ahn, J., Austin, J., Barham,
P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry,
C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev,
S., Devlin, J., D´ıaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag,
M., Garcia, X., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou,
L., Howland, J., Hu, A., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski,
M., Jia, W., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li,
E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M.,
Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E.,
Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S.,
Reif, E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby,
R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan,
V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y.,
Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W.,
Zhou, D., Petrov, S., and Wu, Y. Palm 2 technical report, 2023.
[27] Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. Invariant risk minimization.
arXiv preprint arXiv:1907.02893 (2019).
[28] Asai, A., and Hajishirzi, H. Logic-guided data augmentation and regularization for consistent
question answering. In Proceedings of the 58th Annual Meeting of the Association for Computa-
tional Linguistics (Online, July 2020), Association for Computational Linguistics, pp. 5642–5650.
[29] Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A.,
Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez,
D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T., Clark, J., McCan-
dlish, S., Olah, C., and Kaplan, J. A general language assistant as a laboratory for alignment,
2021.
[30] Ates, T., Atesoglu, M., Yigit, C., Kesen, I., Kobas, M., Erdem, E., Erdem, A., Goksun,
T., and Yuret, D. Craft: A benchmark for causal reasoning about forces and interactions, 12
2020.
[31] Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420 (2018).
[32] Babyak, M. A. Understanding confounding and mediation. BMJ Ment Health 12, 3 (2009),
68–71.
[33] Bach, S., Binder, A., Montavon, G., Klauschen, F., M¨uller, K.-R., and Samek, W.
On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.
PloS one 10, 7 (2015), e0130140.
[34] Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A.,
and Bengio, Y. An actor-critic algorithm for sequence prediction. In International Conference
on Learning Representations (2017).
[35] Bahdanau, D., Hill, F., Leike, J., Hughes, E., Kohli, P., and Grefenstette, E. Learning
to understand goal specifications by modelling reward. In International Conference on Learning
Representations (2019).
[36] Bahng, H., Chun, S., Yun, S., Choo, J., and Oh, S. J. Learning de-biased representations
with biased representations. In International Conference on Machine Learning (2020), PMLR,
pp. 528–539.
[37] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort,
S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T.,
El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston,
S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark,
49


--- Page 50 ---
J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training a helpful and harmless
assistant with reinforcement learning from human feedback, 2022.
[38] Baiocchi, M., Cheng, J., and Small, D. S. Instrumental variable methods for causal inference.
Statistics in medicine 33, 13 (2014), 2297–2340.
[39] Baldini, I., Wei, D., Ramamurthy, K. N., Yurochkin, M., and Singh, M. Your fair-
ness may vary: Pretrained language model fairness in toxic text classification.
arXiv preprint
arXiv:2108.01250 (2021).
[40] Baluja, S., and Fischer, I. Adversarial transformation networks: Learning to generate adver-
sarial examples. arXiv preprint arXiv:1703.09387 (2017).
[41] Bareinboim, E., Correa, J. D., Ibeling, D., and Icard, T. On pearl’s hierarchy and the
foundations of causal inference. ACM Special Volume in Honor of Judea Pearl (provisional title)
2, 3 (2020), 4.
[42] Barredo Arrieta, A., D´ıaz-Rodr´ıguez, N., Del Ser, J., Bennetot, A., Tabik, S., Bar-
bado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R., and
Herrera, F. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and
challenges toward responsible ai. Inf. Fusion 58, C (jun 2020), 82–115.
[43] Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer,
2020.
[44] Ben-David, E., Oved, N., and Reichart, R. Pada: Example-based prompt learning for on-
the-fly adaptation to unseen domains, 2022.
[45] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan,
J. W. A theory of learning from different domains. Machine learning 79, 1 (2010), 151–175.
[46] Ben-David, S., et al. Analysis of representations for domain adaptation. In NeurIPS (2007),
pp. 137–144.
[47] Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers
of stochastic parrots: Can language models be too big?
In Proceedings of the 2021 ACM Con-
ference on Fairness, Accountability, and Transparency (New York, NY, USA, 2021), FAccT ’21,
Association for Computing Machinery, p. 610–623.
[48] Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
Curriculum learning.
In
Proceedings of the 26th annual international conference on machine learning (2009), pp. 41–48.
[49] Berk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., Neel, S.,
and Roth, A. A convex framework for fair regression. arXiv preprint arXiv:1706.02409 (2017).
[50] Beutel, A., Chen, J., Zhao, Z., and Chi, E. H. Data decisions and theoretical implications
when adversarially learning fair representations. arXiv preprint arXiv:1707.00075 (2017).
[51] Bi, Z., Zhang, N., Ye, G., Yu, H., Chen, X., and Chen, H. Interventional aspect-based
sentiment analysis, 2021.
[52] Biten, A. F., Gomez, L., and Karatzas, D. Let there be a clock on the beach: Reducing
object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision (2022), pp. 1381–1390.
[53] Blitzer, J., Dredze, M., and Pereira, F. Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (Prague, Czech Republic, June 2007), Association for
Computational Linguistics, pp. 440–447.
[54] Blodgett, S. L., Barocas, S., Daum´e III, H., and Wallach, H. Language (technology)
is power: A critical survey of “bias” in NLP. In Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics (Online, July 2020), Association for Computational
Linguistics, pp. 5454–5476.
[55] B¨ohm, F., Gao, Y., Meyer, C. M., Shapira, O., Dagan, I., and Gurevych, I. Better
rewards yield better summaries: Learning to summarise without references. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (Hong Kong, China, Nov.
50


--- Page 51 ---
2019), Association for Computational Linguistics, pp. 3110–3120.
[56] Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bern-
stein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card,
D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis, J. Q., Demszky, D.,
Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh,
K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N., Grossman,
S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K.,
Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G.,
Khani, F., Khattab, O., Koh, P. W., Krass, M., Krishna, R., Kuditipudi, R., Kumar,
A., Ladhak, F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X. L., Li, X., Ma, T.,
Malik, A., Manning, C. D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair, S.,
Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J. C., Nilforoshan, H.,
Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park, J. S., Piech, C., Portelance,
E., Potts, C., Raghunathan, A., Reich, R., Ren, H., Rong, F., Roohani, Y., Ruiz, C.,
Ryan, J., R´e, C., Sadigh, D., Sagawa, S., Santhanam, K., Shih, A., Srinivasan, K.,
Tamkin, A., Taori, R., Thomas, A. W., Tram`er, F., Wang, R. E., Wang, W., Wu, B.,
Wu, J., Wu, Y., Xie, S. M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang,
T., Zhang, X., Zhang, Y., Zheng, L., Zhou, K., and Liang, P. On the opportunities and
risks of foundation models, 2022.
[57] Boukhers, Z., Hartmann, T., and J¨urjens, J. Coin: Counterfactual image generation for
vqa interpretation. ArXiv abs/2201.03342 (2022).
[58] Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., and Krishnan, D. Unsupervised
pixel-level domain adaptation with generative adversarial networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition (2017), pp. 3722–3731.
[59] Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. Domain
separation networks. Advances in neural information processing systems 29 (2016).
[60] Brendel, W., and Bethge, M. Approximating cnns with bag-of-local-features models works
surprisingly well on imagenet, 2019.
[61] Broscheit, S., Do, Q., and Gaspers, J. Distributionally robust finetuning bert for covari-
ate drift in spoken language understanding. In Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) (2022), pp. 1970–1985.
[62] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakan-
tan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems 33 (2020), 1877–1901.
[63] Bui, M.-H., Tran, T., Tran, A., and Phung, D.
Exploiting domain-specific features to
enhance domain generalization. Advances in Neural Information Processing Systems 34 (2021),
21189–21201.
[64] Bursztyn, V., Demeter, D., Downey, D., and Birnbaum, L. Learning to perform com-
plex tasks through compositional fine-tuning of language models. In Findings of the Association
for Computational Linguistics: EMNLP 2022 (Abu Dhabi, United Arab Emirates, Dec. 2022),
Association for Computational Linguistics, pp. 1676–1686.
[65] Calderon, N., Ben-David, E., Feder, A., and Reichart, R. Docogen: Domain counterfac-
tual generation for low resource domain adaptation. arXiv preprint arXiv:2202.12350 (2022).
[66] Calderon, N., Ben-David, E., Feder, A., and Reichart, R. DoCoGen: Domain counter-
factual generation for low resource domain adaptation. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) (Dublin, Ireland, May
2022), Association for Computational Linguistics, pp. 7727–7746.
[67] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.
End-to-end object detection with transformers, 2020.
[68] Carlini, N., and Wagner, D. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP) (2017), IEEE, pp. 39–57.
[69] Carlucci, F. M., Russo, P., Tommasi, T., and Caputo, B. Agnostic domain generalization.
51


--- Page 52 ---
arXiv preprint arXiv:1808.01102 (2018).
[70] Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J. C., and Liang, P. S. Unlabeled
data improves adversarial robustness. Advances in neural information processing systems 32 (2019).
[71] Castro, D. C., Walker, I., and Glocker, B. Causality matters in medical imaging. Nature
Communications 11, 1 (Jul 2020).
[72] Caton,
S.,
and Haas,
C.
Fairness in machine learning:
A survey.
arXiv preprint
arXiv:2010.04053 (2020).
[73] Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K.
Classification with fairness
constraints: A meta-algorithm with provable guarantees.
In Proceedings of the conference on
fairness, accountability, and transparency (2019), pp. 319–328.
[74] Celis, L. E., and Keswani, V.
Improved adversarial learning for fair classification.
arXiv
preprint arXiv:1901.10443 (2019).
[75] Chadha, A., and Jain, V. ireason: Multimodal commonsense reasoning using videos and natural
language with interpretability, 06 2021.
[76] Chalkidis, I., Pasini, T., Zhang, S., Tomada, L., Schwemer, S. F., and Søgaard, A.
Fairlex: A multilingual benchmark for evaluating fairness in legal text processing. arXiv preprint
arXiv:2203.07228 (2022).
[77] Chambon, P. J. M., Bluethgen, C., Langlotz, C., and Chaudhari, A. Adapting pre-
trained vision-language foundational models to medical imaging domains. In NeurIPS 2022 Foun-
dation Models for Decision Making Workshop (2022).
[78] Chang, C.-H., Adam, G. A., and Goldenberg, A. Towards robust classification model by
counterfactual and invariant data generation, 2021.
[79] Chattopadhyay, A., Manupriya, P., Sarkar, A., and Balasubramanian, V. N. Neural
network attributions: A causal perspective.
In International Conference on Machine Learning
(2019), PMLR, pp. 981–990.
[80] Chen, G., Li, J., Lu, J., and Zhou, J. Human trajectory prediction via counterfactual analysis,
2021.
[81] Chen, H., He, J., Narasimhan, K., and Chen, D. Can rationalization improve robustness?
arXiv preprint arXiv:2204.11790 (2022).
[82] Chen, H., and Ji, Y. Adversarial training for improving model robustness? look at both predic-
tion and interpretation, 2022.
[83] Chen, H.-Y., and Chao, W.-L.
Gradual domain adaptation without indexed intermediate
domains. Advances in Neural Information Processing Systems 34 (2021), 8201–8214.
[84] Chen, J., and Gu, Q.
Rays: A ray searching method for hard-label adversarial attack.
In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (2020), pp. 1739–1747.
[85] Chen, L., Yan, X., Xiao, J., Zhang, H., Pu, S., and Zhuang, Y. Counterfactual samples
synthesizing for robust visual question answering, 2020.
[86] Chen, M., Du, J., Pasunuru, R., Mihaylov, T., Iyer, S., Stoyanov, V., and Kozareva,
Z.
Improving in-context few-shot learning via self-supervised training.
In Proceedings of the
2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies (Seattle, United States, July 2022), Association for Computational
Linguistics, pp. 3558–3573.
[87] Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. Zoo: Zeroth order optimization
based black-box attacks to deep neural networks without training substitute models. In Proceedings
of the 10th ACM Workshop on Artificial Intelligence and Security (2017), ACM, pp. 15–26.
[88] Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., and Yu, X. Recall and learn: Fine-tuning
deep pretrained language models with less forgetting. arXiv preprint arXiv:2004.12651 (2020).
[89] Chen, S., Yang, W., Bi, X., and Sun, X. Fine-tuning deteriorates general textual out-of-
distribution detection by distorting task-agnostic features.
In Findings of the Association for
52


--- Page 53 ---
Computational Linguistics: EACL 2023 (Dubrovnik, Croatia, May 2023), Association for Compu-
tational Linguistics, pp. 564–579.
[90] Chen, W. Large language models are few(1)-shot table reasoners, 2023.
[91] Chen, W., Tian, J., Fan, C., He, H., and Jin, Y. Dependent multi-task learning with causal
intervention for image captioning. In IJCAI (2021).
[92] Chen, X., Ye, J., Zu, C., Xu, N., Zheng, R., Peng, M., Zhou, J., Gui, T., Zhang, Q.,
and Huang, X.
How robust is gpt-3.5 to predecessors?
a comprehensive study on language
understanding tasks, 2023.
[93] Chen, Y., Chen, D., Wang, T., Wang, Y., and Liang, Y. Causal intervention for subject-
deconfounded facial action unit recognition, 2022.
[94] Chen, Y., Yang, X., Cham, T.-J., and Cai, J. Towards unbiased visual emotion recognition
via causal intervention, 2021.
[95] Chen, Y., Zhao, C., Yu, Z., McKeown, K., and He, H. On the relation between sensitivity
and accuracy in in-context learning, 2023.
[96] Cheng, M., Singh, S., Chen, P., Chen, P.-Y., Liu, S., and Hsieh, C.-J.
Sign-opt: A
query-efficient hard-label adversarial attack. arXiv preprint arXiv:1909.10773 (2019).
[97] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham,
P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S.,
Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E.,
Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,
Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X.,
Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,
B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M.,
Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-
Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language
modeling with pathways, 2022.
[98] Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep
reinforcement learning from human preferences. In Advances in Neural Information Processing
Systems (2017), I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, Eds., vol. 30, Curran Associates, Inc.
[99] Cisse, M., Adi, Y., Neverova, N., and Keshet, J. Houdini: Fooling deep structured predic-
tion models. arXiv preprint arXiv:1707.05373 (2017).
[100] Clark, K., Luong, M.-T., Le, Q. V., and Manning, C. D. Electra: Pre-training text encoders
as discriminators rather than generators. In International Conference on Learning Representations
(2020).
[101] Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M.,
Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to
solve math word problems, 2021.
[102] Cotter, A., Jiang, H., Gupta, M. R., Wang, S., Narayan, T., You, S., and Sridharan,
K. Optimization with non-differentiable constraints with applications to fairness, recall, churn,
and other goals. J. Mach. Learn. Res. 20, 172 (2019), 1–59.
[103] Covert, I., Lundberg, S. M., and Lee, S.-I. Understanding global feature contributions with
additive importance measures. Advances in Neural Information Processing Systems 33 (2020),
17212–17223.
[104] Covert, I., Lundberg, S. M., and Lee, S.-I. Explaining by removing: A unified framework
for model explanation. J. Mach. Learn. Res. 22 (2021), 209–1.
[105] Croce, F., and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of
diverse parameter-free attacks, 2020.
[106] Csurka, G. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint
arXiv:1702.05374 (2017).
53


--- Page 54 ---
[107] Cui, J., Li, Z., Yan, Y., Chen, B., and Yuan, L. Chatlaw: Open-source legal large language
model with integrated external knowledge bases, 2023.
[108] Dabkowski, P., and Gal, Y. Real time image saliency for black box classifiers. Advances in
neural information processing systems 30 (2017).
[109] Dagaev, N., Roads, B. D., Luo, X., Barry, D. N., Patil, K. R., and Love, B. C. A
too-good-to-be-true prior to reduce shortcut reliance.
Pattern Recognition Letters 166 (2023),
164–171.
[110] Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F. Why can gpt learn
in-context? language models implicitly perform gradient descent as meta-optimizers, 2023.
[111] Dai, Q., Shen, X., Zhang, L., Li, Q., and Wang, D. Adversarial training methods for network
embedding, 2019.
[112] Das, S. D., Basak, A., Mandal, S., and Das, D. Advcodemix: Adversarial attack on code-
mixed data. In 5th Joint International Conference on Data Science & Management of Data (9th
ACM IKDD CODS and 27th COMAD) (2022), pp. 125–129.
[113] Dash, D., Thapa, R., Banda, J. M., Swaminathan, A., Cheatham, M., Kashyap, M.,
Kotecha, N., Chen, J. H., Gombar, S., Downing, L., Pedreira, R., Goh, E., Arnaout,
A., Morris, G. K., Magon, H., Lungren, M. P., Horvitz, E., and Shah, N. H. Evaluation
of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery, 2023.
[114] Daum´e III, H. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics (Prague, Czech Republic, June 2007), Association
for Computational Linguistics, pp. 256–263.
[115] Davison, J., Feldman, J., and Rush, A. Commonsense knowledge mining from pretrained mod-
els. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)
(Hong Kong, China, Nov. 2019), Association for Computational Linguistics, pp. 1173–1178.
[116] De Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A., Slabaugh,
G., and Tuytelaars, T. A continual learning survey: Defying forgetting in classification tasks.
IEEE transactions on pattern analysis and machine intelligence 44, 7 (2021), 3366–3385.
[117] De Raedt, M., Godin, F., Develder, C., and Demeester, T.
Robustifying sentiment
classification by maximally exploiting few counterfactuals. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec.
2022), Association for Computational Linguistics, pp. 11386–11400.
[118] Delobelle, P., Tokpo, E. K., Calders, T., and Berendt, B.
Measuring fairness with
biased rulers: A comparative study on bias metrics for pre-trained language models. In NAACL
2022: the 2022 Conference of the North American chapter of the Association for Computational
Linguistics: human language technologies (2022), pp. 1693–1706.
[119] Deng, X., and Zhang, Z. Comprehensive knowledge distillation with causal intervention. Ad-
vances in Neural Information Processing Systems 34 (2021).
[120] Dess`ı, R., Bevilacqua, M., Gualdoni, E., Rakotonirina, N. C., Franzon, F., and Ba-
roni, M. Cross-domain image captioning with discriminative finetuning, 2023.
[121] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirec-
tional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[122] Dhouib, S., Redko, I., and Lartizien, C. Margin-aware adversarial domain adaptation with
optimal transport. In Thirty-seventh International Conference on Machine Learning (2020).
[123] Dhurandhar, A., Chen, P.-Y., Luss, R., Tu, C.-C., Ting, P., Shanmugam, K., and Das,
P. Explanations based on the missing: Towards contrastive explanations with pertinent negatives.
Advances in neural information processing systems 31 (2018).
[124] Di Stefano, P. G., Hickey, J. M., and Vasileiou, V. Counterfactual fairness: removing
direct effects through regularization. arXiv preprint arXiv:2002.10774 (2020).
[125] Ding, G. W., Sharma, Y., Lui, K. Y. C., and Huang, R. Mma training: Direct input space
margin maximization through adversarial training. arXiv preprint arXiv:1812.02637 (2018).
54


--- Page 55 ---
[126] Dombrowski, A.-K., Alber, M., Anders, C., Ackermann, M., M¨uller, K.-R., and
Kessel, P. Explanations can be manipulated and geometry is to blame. Advances in Neural
Information Processing Systems 32 (2019).
[127] Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell,
T. Decaf: A deep convolutional activation feature for generic visual recognition. In International
conference on machine learning (2014), PMLR, pp. 647–655.
[128] Dong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang,
T. Raft: Reward ranked finetuning for generative foundation model alignment, 2023.
[129] Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., and
Sui, Z. A survey on in-context learning, 2023.
[130] Dong, Y., Wang, S., Gan, Z., Cheng, Y., Cheung, J. C. K., and Liu, J. Multi-fact cor-
rection in abstractive text summarization. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) (Online, Nov. 2020), Association for Compu-
tational Linguistics, pp. 9320–9331.
[131] Doshi-Velez, F., and Kim, B. Towards a rigorous science of interpretable machine learning.
arXiv: Machine Learning (2017).
[132] Du, M., Yang, F., Zou, N., and Hu, X. Fairness in deep learning: A computational perspective.
IEEE Intelligent Systems 36, 4 (2020), 25–34.
[133] Dwivedi, Y. K., Kshetri, N., Hughes, L., Slade, E. L., Jeyaraj, A., Kar, A. K., Baab-
dullah, A. M., Koohang, A., Raghavan, V., Ahuja, M., Albanna, H., Albashrawi,
M. A., Al-Busaidi, A. S., Balakrishnan, J., Barlette, Y., Basu, S., Bose, I., Brooks,
L., Buhalis, D., Carter, L., Chowdhury, S., Crick, T., Cunningham, S. W., Davies,
G. H., Davison, R. M., D´e, R., Dennehy, D., Duan, Y., Dubey, R., Dwivedi, R., Ed-
wards, J. S., Flavi´an, C., Gauld, R., Grover, V., Hu, M.-C., Janssen, M., Jones, P.,
Junglas, I., Khorana, S., Kraus, S., Larsen, K. R., Latreille, P., Laumer, S., Ma-
lik, F. T., Mardani, A., Mariani, M., Mithas, S., Mogaji, E., Nord, J. H., O’Connor,
S., Okumus, F., Pagani, M., Pandey, N., Papagiannidis, S., Pappas, I. O., Pathak, N.,
Pries-Heje, J., Raman, R., Rana, N. P., Rehm, S.-V., Ribeiro-Navarrete, S., Richter,
A., Rowe, F., Sarker, S., Stahl, B. C., Tiwari, M. K., van der Aalst, W., Venkatesh,
V., Viglia, G., Wade, M., Walton, P., Wirtz, J., and Wright, R. Opinion paper: “so
what if chatgpt wrote it?” multidisciplinary perspectives on opportunities, challenges and impli-
cations of generative conversational ai for research, practice and policy. International Journal of
Information Management 71 (2023), 102642.
[134] Dwork, C., Immorlica, N., Kalai, A. T., and Leiserson, M. Decoupled classifiers for group-
fair and efficient machine learning. In Conference on fairness, accountability and transparency
(2018), PMLR, pp. 119–133.
[135] D’Innocente, A., and Caputo, B. Domain generalization with domain-specific aggregation
modules. In German Conference on Pattern Recognition (2018), Springer, pp. 187–198.
[136] Eastwood, C., Nanbo, L., and Williams, C. K. I. Align-deform-subtract: An interventional
framework for explaining object differences, 2022.
[137] Eckstein, N., Bates, A. S., Jefferis, G. S. X. E., and Funke, J. Discriminative attribution
from counterfactuals, 2021.
[138] Edalati, A., Tahaei, M., Kobyzev, I., Nia, V. P., Clark, J. J., and Rezagholizadeh,
M. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650
(2022).
[139] Edwards, H., and Storkey, A. Censoring representations with an adversary. arXiv preprint
arXiv:1511.05897 (2015).
[140] Elazar, Y., Ravfogel, S., Jacovi, A., and Goldberg, Y. Amnesic probing: Behavioral
explanation with amnesic counterfactuals, 2021.
[141] Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A. Adver-
sarial robustness as a prior for learned representations, 2019.
55


--- Page 56 ---
[142] Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A. Learning
perceptually-aligned representations via adversarial robustness. arXiv preprint arXiv:1906.00945
(2019).
[143] Erhan, D., Bengio, Y., Courville, A., and Vincent, P. Visualizing higher-layer features of
a deep network. University of Montreal 1341, 3 (2009), 1.
[144] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M., Blau, H. M., and Thrun,
S. Dermatologist-level classification of skin cancer with deep neural networks. nature 542, 7639
(2017), 115–118.
[145] Feder, A., Keith, K. A., Manzoor, E., Pryzant, R., Sridhar, D., Wood-Doughty, Z.,
Eisenstein, J., Grimmer, J., Reichart, R., Roberts, M. E., Stewart, B. M., Veitch,
V., and Yang, D. Causal inference in natural language processing: Estimation, prediction, inter-
pretation and beyond. Transactions of the Association for Computational Linguistics 10 (2022),
1138–1158.
[146] Feder, A., Oved, N., Shalit, U., and Reichart, R. CausaLM: Causal model explanation
through counterfactual language models. Computational Linguistics 47, 2 (June 2021), 333–386.
[147] Feder, A., Oved, N., Shalit, U., and Reichart, R. CausaLM: Causal model explanation
through counterfactual language models. Computational Linguistics (may 2021), 1–54.
[148] Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2022.
[149] Feng, R., Yang, Y., Lyu, Y., Tan, C., Sun, Y., and Wang, C. Learning fair representations
via an adversarial framework. arXiv preprint arXiv:1904.13341 (2019).
[150] Field, A., Blodgett, S. L., Waseem, Z., and Tsvetkov, Y.
A survey of race, racism,
and anti-racism in NLP. In Proceedings of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint Conference on Natural Language Process-
ing (Volume 1: Long Papers) (Online, Aug. 2021), Association for Computational Linguistics,
pp. 1905–1925.
[151] Fu, T.-J., Wang, X. E., Peterson, M., Grafton, S., Eckstein, M., and Wang, W. Y.
Counterfactual vision-and-language navigation via adversarial path sampling, 2020.
[152] Fu, X., Peng, Y., Liu, Y., Lin, Y., Gui, G., Gacanin, H., and Adachi, F. Semi-supervised
specific emitter identification method using metric-adversarial training. IEEE Internet of Things
Journal (2023).
[153] Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B.,
Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly,
T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds,
Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S.,
Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., Mc-
Candlish, S., Olah, C., Kaplan, J., and Clark, J. Red teaming language models to reduce
harms: Methods, scaling behaviors, and lessons learned, 2022.
[154] Gao, P., Zheng, M., Wang, X., Dai, J., and Li, H. Fast convergence of detr with spatially
modulated co-attention. arXiv preprint arXiv:2101.07448 (2021).
[155] Gao, T., Fisch, A., and Chen, D. Making pre-trained language models better few-shot learners.
In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)
(Online, Aug. 2021), Association for Computational Linguistics, pp. 3816–3830.
[156] Gardner, M., Artzi, Y., Basmov, V., Berant, J., Bogin, B., Chen, S., Dasigi, P., Dua,
D., Elazar, Y., Gottumukkala, A., Gupta, N., Hajishirzi, H., Ilharco, G., Khashabi,
D., Lin, K., Liu, J., Liu, N. F., Mulcaire, P., Ning, Q., Singh, S., Smith, N. A., Subra-
manian, S., Tsarfaty, R., Wallace, E., Zhang, A., and Zhou, B. Evaluating models’ local
decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics:
EMNLP 2020 (Online, Nov. 2020), Association for Computational Linguistics, pp. 1307–1323.
[157] Garg, S., Perot, V., Limtiaco, N., Taly, A., Chi, E. H., and Beutel, A. Counterfactual
fairness in text classification through robustness, 2019.
56


--- Page 57 ---
[158] Ge, S., Wang, H., Alavi, A., Xing, E., and Bar-Joseph, Z. Supervised adversarial alignment
of single-cell rna-seq data. Journal of Computational Biology 28, 5 (2021), 501–513.
[159] Ge, Y., Ren, J., Gallagher, A., Wang, Y., Yang, M.-H., Adam, H., Itti, L., Lakshmi-
narayanan, B., and Zhao, J. Improving zero-shot generalization and robustness of multi-modal
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(2023), pp. 11093–11101.
[160] Geiger, A., Lu, H., Icard, T., and Potts, C.
Causal abstractions of neural networks.
Advances in Neural Information Processing Systems 34 (2021), 9574–9586.
[161] Geiger, A., Wu, Z., Lu, H., Rozner, J., Kreiss, E., Icard, T., Goodman, N., and Potts,
C. Inducing causal structure for interpretable neural networks. In International Conference on
Machine Learning (2022), PMLR, pp. 7324–7338.
[162] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel,
W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy
and robustness. arXiv preprint arXiv:1811.12231 (2018).
[163] Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel,
W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy
and robustness, 2022.
[164] Gentner, T., Neitzel, T., Schulze, J., and Buettner, R. A systematic literature review
of medical chatbot research from a behavior change perspective.
In 2020 IEEE 44th Annual
Computers, Software, and Applications Conference (COMPSAC) (2020), pp. 735–740.
[165] Germain, P., Habrard, A., Laviolette, F., and Morvant, E. A new pac-bayesian per-
spective on domain adaptation. In Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 (2016), M. Balcan and K. Q.
Weinberger, Eds., vol. 48 of JMLR Workshop and Conference Proceedings, JMLR.org, pp. 859–868.
[166] Girshick, R. Fast r-cnn. In Proceedings of the IEEE International Conference on Computer
Vision (ICCV) (December 2015).
[167] Girshick, R., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate
object detection and semantic segmentation, 2014.
[168] Glymour, M., Pearl, J., and Jewell, N. P. Causal inference in statistics: A primer. John
Wiley & Sons, 2016.
[169] Goel, N., Yaghini, M., and Faltings, B. Non-discriminatory machine learning through convex
fairness criteria. In Proceedings of the AAAI Conference on Artificial Intelligence (2018), vol. 32.
[170] Goh, G., Cotter, A., Gupta, M., and Friedlander, M. P. Satisfying real-world goals with
dataset constraints. Advances in Neural Information Processing Systems 29 (2016).
[171] Gong, R., Li, W., Chen, Y., and Gool, L. V. Dlow: Domain flow for adaptation and gener-
alization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
(2019), pp. 2477–2486.
[172] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., and Bengio, Y. Generative adversarial nets. In Advances in Neural In-
formation Processing Systems (2014), Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and
K. Weinberger, Eds., vol. 27, Curran Associates, Inc.
[173] Goodfellow, I. J., et al. Explaining and harnessing adversarial examples (2014). In ICLR
(2015).
[174] Gordaliza, P. M., Vaquero, J. J., and Mu˜noz-Barrutia, A. Translational lung imaging
analysis through disentangled representations, 2022.
[175] Gowal, S., Qin, C., Uesato, J., Mann, T., and Kohli, P. Uncovering the limits of adversarial
training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593 (2020).
[176] Goyal, S., Kumar, A., Garg, S., Kolter, Z., and Raghunathan, A. Finetune like you pre-
train: Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (2023), pp. 19338–19347.
57


--- Page 58 ---
[177] Goyal, Y., Khot, T., Summers Stay, D., Batra, D., and Parikh, D. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In abcd (07 2017),
pp. 6325–6334.
[178] Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D., and Lee, S. Counterfactual visual
explanations. In International Conference on Machine Learning (2019), PMLR, pp. 2376–2384.
[179] Greenland, S., Pearl, J., and Robins, J. M. Confounding and collapsibility in causal infer-
ence. Statistical science 14, 1 (1999), 29–46.
[180] Gretton, A., Smola, A. J., Huang, J., Schmittfull, M., Borgwardt, K. M., and
Sch¨olkopf, B. Covariate shift by kernel mean matching. Journal of Machine Learning Research
(2009).
[181] Gu, S., and Rigazio, L. Towards deep neural network architectures robust to adversarial exam-
ples, 2014.
[182] Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S.,
and Song, D. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717
(2023).
[183] GUION, R. Employment tests and discriminatory hiring. Industrial Relations: A Journal of
Economy and Society 5 (05 2008), 20 – 37.
[184] Guo, C., Frank, J. S., and Weinberger, K. Q. Low frequency adversarial perturbation.
arXiv preprint arXiv:1809.08758 (2018).
[185] Guo, D., Rush, A. M., and Kim, Y. Parameter-efficient transfer learning with diff pruning.
arXiv preprint arXiv:2012.07463 (2020).
[186] Guo, H., Zheng, K., Fan, X., Yu, H., and Wang, S. Visual attention consistency under
image transforms for multi-label image classification. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition (2019), pp. 729–739.
[187] Guo, Y., Yang, Y., and Abbasi, A. Auto-debias: Debiasing masked language models with
automated biased prompts.
In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) (2022), pp. 1012–1023.
[188] Gupta, U., Dhamala, J., Kumar, V., Verma, A., Pruksachatkun, Y., Krishna, S.,
Gupta, R., Chang, K.-W., Steeg, G. V., and Galstyan, A.
Mitigating gender bias in
distilled language models via counterfactual role reversal. arXiv preprint arXiv:2203.12574 (2022).
[189] Gurrapu, S., Kulkarni, A., Huang, L., Lourentzou, I., Freeman, L., and Batarseh,
F. A. Rationalization for explainable nlp: A survey. arXiv preprint arXiv:2301.08912 (2023).
[190] Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M. Retrieval augmented language
model pre-training. In Proceedings of the 37th International Conference on Machine Learning (13–
18 Jul 2020), H. D. III and A. Singh, Eds., vol. 119 of Proceedings of Machine Learning Research,
PMLR, pp. 3929–3938.
[191] Ha, T., Dang, T. K., Dang, T. T., Truong, T. A., and Nguyen, M. T. Differential privacy
in deep learning: An overview. In 2019 International Conference on Advanced Computing and
Applications (ACOMP) (2019), pp. 97–102.
[192] Hambardzumyan, K., Khachatrian, H., and May, J. WARP: Word-level Adversarial Re-
Programming. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) (Online, Aug. 2021), Association for Computational Linguistics, pp. 4921–4933.
[193] Han, B., Zheng, C., Chan, H., Paster, K., Zhang, M. R., and Ba, J. Learning domain
invariant representations in goal-conditioned block mdps. arXiv preprint arXiv:2110.14248 (2021).
[194] Han, X., Zhao, W., Ding, N., Liu, Z., and Sun, M. Ptr: Prompt tuning with rules for text
classification, 2021.
[195] Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J. Learning from dialogue after
deployment: Feed yourself, chatbot!
In Proceedings of the 57th Annual Meeting of the Associ-
ation for Computational Linguistics (Florence, Italy, July 2019), Association for Computational
Linguistics, pp. 3667–3684.
58


--- Page 59 ---
[196] Hao, Y., Dong, L., Wei, F., and Xu, K. Investigating learning dynamics of BERT fine-tuning.
In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Compu-
tational Linguistics and the 10th International Joint Conference on Natural Language Processing
(Suzhou, China, Dec. 2020), Association for Computational Linguistics, pp. 87–92.
[197] Harrison,
S.,
and
Pan,
B.
Mitigating
bias
in
facial
recognition
with
fairgan.
https://hci.stanford.edu/courses/cs335/2020/sp/harrisonsasha˙36812˙6193573˙CS335˙Project.pdf
(2020).
[198] Hasani, N., Morris, M., Rhamim, A., Summers, R., Jones, E., Siegel, E., and Saboury,
B. Trustworthy artificial intelligence in medical imaging. PET Clinics 17 (01 2022), 1–12.
[199] Haviv, A., Berant, J., and Globerson, A. BERTese: Learning to speak to BERT. In Proceed-
ings of the 16th Conference of the European Chapter of the Association for Computational Linguis-
tics: Main Volume (Online, Apr. 2021), Association for Computational Linguistics, pp. 3618–3623.
[200] Hayes, J., and Danezis, G. Learning universal adversarial perturbations with generative models.
In 2018 IEEE Security and Privacy Workshops (SPW) (2018), IEEE, pp. 43–49.
[201] He, G., Chen, J., and Zhu, J. Preserving pre-trained features helps calibrate fine-tuned language
models. In The Eleventh International Conference on Learning Representations (2023).
[202] He, J., Beurer-Kellner, L., and Vechev, M. On distribution shift in learning-based bug
detectors. In Proceedings of the 39th International Conference on Machine Learning (Jun 2022),
PMLR, p. 8559–8580.
[203] He, J., Xia, M., Fellbaum, C., and Chen, D. MABEL: Attenuating gender bias using textual
entailment data. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-
guage Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Computational
Linguistics, pp. 9681–9702.
[204] He, P., Liu, X., Gao, J., and Chen, W. {DEBERTA}: {DECODING}-{enhanced} {bert}
{with} {disentangled} {attention}.
In International Conference on Learning Representations
(2021).
[205] He, X., Yang, D., Feng, W., Fu, T.-J., Akula, A., Jampani, V., Narayana, P., Basu,
S., Wang, W. Y., and Wang, X. CPL: Counterfactual prompt learning for vision and language
models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Process-
ing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Computational Linguistics,
pp. 3407–3418.
[206] Heo, J., Joo, S., and Moon, T. Fooling neural network interpretations via adversarial model
manipulation. Advances in Neural Information Processing Systems 32 (2019).
[207] Hermann, K., Chen, T., and Kornblith, S. The origins and prevalence of texture bias in
convolutional neural networks. Advances in Neural Information Processing Systems 33 (2020),
19000–19015.
[208] Ho, J., Lee, B.-G., and Kang, D.-K. Attack-less adversarial training for a robust adversarial
defense. Applied Intelligence (2022), 1–18.
[209] Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., and
Darrell, T. Cycada: Cycle-consistent adversarial domain adaptation. In International conference
on machine learning (2018), Pmlr, pp. 1989–1998.
[210] Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B. A Benchmark for Interpretability
Methods in Deep Neural Networks. Curran Associates Inc., Red Hook, NY, USA, 2019, ch. 873,
p. 12.
[211] Hou, X., Liu, J., Xu, B., Wang, X., Liu, B., and Qiu, G. Class-aware domain adaptation
for improving adversarial robustness. Image and Vision Computing 99 (2020), 103926.
[212] Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-
mundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In
International Conference on Machine Learning (2019), PMLR, pp. 2790–2799.
[213] Hsiung, L., Tsai, Y.-Y., Chen, P.-Y., and Ho, T.-Y. Towards compositional adversarial
robustness: Generalizing adversarial training to composite semantic perturbations. In Proceedings
59


--- Page 60 ---
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023), pp. 24658–
24667.
[214] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen,
W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).
[215] Hu, W., Niu, G., Sato, I., and Sugiyama, M. Does distributionally robust supervised learning
give robust classifiers? In International Conference on Machine Learning (2018), PMLR, pp. 2029–
2037.
[216] Hu, X., Tang, K., Miao, C., Hua, X., and Zhang, H. Distilling causal effect of data in class-
incremental learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR) (2021), 3956–3965.
[217] Hu, Z., Lan, Y., Wang, L., Xu, W., Lim, E.-P., Lee, R. K.-W., Bing, L., and Poria,
S. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models.
arXiv preprint arXiv:2304.01933 (2023).
[218] Hua, H., Yan, J., Fang, X., Huang, W., Yin, H., and Ge, W. Causal information bottleneck
boosts adversarial robustness of deep neural network. arXiv preprint arXiv:2210.14229 (2022).
[219] Huang, J., and Chang, K. C.-C. Towards reasoning in large language models: A survey, 2023.
[220] Huang, J., Guan, D., Xiao, A., and Lu, S. Fsdr: Frequency space domain randomization
for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2021), pp. 6891–6902.
[221] Huang, L., and Vishnoi, N.
Stable and fair classification.
In International Conference on
Machine Learning (2019), PMLR, pp. 2879–2890.
[222] Huang, W., Jiang, M., Li, M., Meng, B., Ren, J., Zhao, S., Bai, R., and Yang, Y.
Causal intervention for object detection. In 2021 IEEE 33rd International Conference on Tools
with Artificial Intelligence (ICTAI) (2021), pp. 770–774.
[223] Huang, Z., Wang, H., Huang, D., Lee, Y. J., and Xing, E. P. The two dimensions of
worst-case training and their integrated effect for out-of-domain generalization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2022), pp. 9631–9641.
[224] Huang, Z., Wang, H., Xing, E. P., and Huang, D. Self-challenging improves cross-domain
generalization. In European Conference on Computer Vision (2020), Springer, Cham, pp. 124–140.
[225] Huff, D. How to lie with statistics. Penguin UK, 2023.
[226] Hvilshøj, F., Iosifidis, A., and Assent, I. Ecinn: Efficient counterfactuals from invertible
neural networks, 2021.
[227] H¨oltgen, B., Schut, L., Brauner, J. M., and Gal, Y. Deduce: Generating counterfactual
explanations efficiently, 2021.
[228] Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. Reward learning
from human preferences and demonstrations in atari. In Advances in Neural Information Pro-
cessing Systems (2018), S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, Eds., vol. 31, Curran Associates, Inc.
[229] Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. Adver-
sarial examples are not bugs, they are features. Advances in neural information processing systems
32 (2019).
[230] Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu,
J., Joulin, A., Riedel, S., and Grave, E. Atlas: Few-shot learning with retrieval augmented
language models, 2022.
[231] J. Thiagarajan, J., Thopalli, K., Rajan, D., and Turaga, P. Training calibration-based
counterfactual explainers for deep learning models in medical image analysis. Scientific Reports 12
(01 2022).
[232] Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N.,
Gu, S., and Picard, R. Way off-policy batch deep reinforcement learning of implicit human
preferences in dialog, 2019.
60


--- Page 61 ---
[233] Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A.,
and Fung, P. Survey of hallucination in natural language generation. ACM Computing Surveys
55, 12 (2023), 1–38.
[234] Jia, C., and Zhang, Y. Prompt-based distribution alignment for domain generalization in text
classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Computational Lin-
guistics, pp. 10147–10157.
[235] Jian, Y., Gao, C., and Vosoughi, S. Embedding hallucination for few-shot language fine-
tuning. arXiv preprint arXiv:2205.01307 (2022).
[236] Jiang, H., and Nachum, O.
Identifying and correcting label bias in machine learning.
In
International Conference on Artificial Intelligence and Statistics (2020), PMLR, pp. 702–712.
[237] Jiang, L., Zhou, H., Lin, Y., Li, P., Zhou, J., and Jiang, R.
ROSE: Robust selective
fine-tuning for pre-trained language models. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Associ-
ation for Computational Linguistics, pp. 2886–2897.
[238] Jiang, M., Zhang, X., Kamp, M., Li, X., and Dou, Q. Tsmobn: Generalization for unseen
clients in federated learning via causal intervention, 2021.
[239] Jiang, R., Pacchiano, A., Stepleton, T., Jiang, H., and Chiappa, S. Wasserstein fair
classification. In Uncertainty in Artificial Intelligence (2020), PMLR, pp. 862–872.
[240] Jiang, Z., Xu, F. F., Araki, J., and Neubig, G. How can we know what language models
know? Transactions of the Association for Computational Linguistics 8 (2020), 423–438.
[241] Jin, G., Yi, X., Wu, D., Mu, R., and Huang, X.
Randomized adversarial training via
taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2023), pp. 16447–16457.
[242] Jin, J., Dundar, A., and Culurciello, E. Robust convolutional neural networks under ad-
versarial noise. arXiv preprint arXiv:1511.06306 (2015).
[243] Jo, J., and Bengio, Y. Measuring the tendency of cnns to learn surface statistical regularities.
arXiv preprint arXiv:1711.11561 (2017).
[244] Joshi, N., Pan, X., and He, H. Are all spurious features in natural language alike? an analysis
through a causal lens. arXiv preprint arXiv:2210.14011 (2022).
[245] Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tun-
yasuvunakool, K., Bates, R., ˇZ´ıdek, A., Potapenko, A., et al. Highly accurate protein
structure prediction with alphafold. Nature 596, 7873 (2021), 583–589.
[246] Jung, H.-G., Kang, S.-H., Kim, H.-D., Won, D.-O., and Lee, S.-W. Counterfactual expla-
nation based on gradual construction for deep networks, 2021.
[247] Kafle, K., and Kanan, C. An analysis of visual question answering algorithms. abcd (03 2017).
[248] Kamath, P., Tangella, A., Sutherland, D., and Srebro, N. Does invariant risk mini-
mization capture invariance? In International Conference on Artificial Intelligence and Statistics
(2021), PMLR, pp. 4069–4077.
[249] Kandge, V. V., Kandge, S. V., Kumbharkar, K., Pattanshetti, P., et al. De-biasing
facial detection system using vae. arXiv preprint arXiv:2204.09556 (2022).
[250] Kaneko, M., and Bollegala, D. Unmasking the mask – evaluating social biases in masked
language models. Proceedings of the AAAI Conference on Artificial Intelligence 36, 11 (Jun. 2022),
11954–11962.
[251] Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial logit pairing. arXiv preprint
arXiv:1803.06373 (2018).
[252] Kardan, N., and Stanley, K. O. Mitigating fooling with competitive overcomplete output layer
neural networks. In Neural Networks (IJCNN), 2017 International Joint Conference on (2017),
IEEE, pp. 518–525.
[253] Kasneci, E., Sessler, K., K¨uchemann, S., Bannert, M., Dementieva, D., Fischer, F.,
61


--- Page 62 ---
Gasser, U., Groh, G., G¨unnemann, S., H¨ullermeier, E., Krusche, S., Kutyniok, G.,
Michaeli, T., Nerdel, C., Pfeffer, J., Poquet, O., Sailer, M., Schmidt, A., Seidel, T.,
Stadler, M., Weller, J., Kuhn, J., and Kasneci, G. Chatgpt for good? on opportunities
and challenges of large language models for education. Learning and Individual Differences 103
(2023), 102274.
[254] Kaushik, D., Hovy, E., and Lipton, Z. C. Learning the difference that makes a difference
with counterfactually-augmented data, 2020.
[255] Kawakami, Y., Kuroki, M., and Tian, J. Instrumental variable estimation of average partial
causal effects. In International Conference on Machine Learning (2023), PMLR, pp. 16097–16130.
[256] Khosla, A., Zhou, T., Malisiewicz, T., Efros, A. A., and Torralba, A. Undoing the
damage of dataset bias. In Computer Vision–ECCV 2012: 12th European Conference on Computer
Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12 (2012), Springer, pp. 158–171.
[257] Khrulkov, V., and Oseledets, I. Art of singular vectors and universal adversarial perturba-
tions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018),
pp. 8562–8570.
[258] Kıcıman, E., Ness, R., Sharma, A., and Tan, C. Causal reasoning and large language models:
Opening a new frontier for causality. arXiv preprint arXiv:2305.00050 (2023).
[259] Kim, B., Kim, H., Kim, K., Kim, S., and Kim, J. Learning not to learn: Training deep neural
networks with biased data. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2019), pp. 9012–9020.
[260] Kim, J., Lee, B.-K., and Ro, Y. M. Demystifying causal features on adversarial examples and
causal inoculation for robust network by adversarial instrumental variable regression, 2023.
[261] Kim, M., and Byun, H. Learning texture invariant representation for domain adaptation of
semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (2020), pp. 12975–12984.
[262] Kim, T., Cha, M., Kim, H., Lee, J. K., and Kim, J.
Learning to discover cross-domain
relations with generative adversarial networks. In International conference on machine learning
(2017), PMLR, pp. 1857–1865.
[263] Kingma, D. P., and Welling, M. Auto-encoding variational bayes, 2022.
[264] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
T., Whitehead, S., Berg, A. C., Lo, W.-Y., et al.
Segment anything.
arXiv preprint
arXiv:2304.02643 (2023).
[265] Kiritchenko, S., and Mohammad, S. M. Examining gender and race bias in two hundred
sentiment analysis systems, 2018.
[266] Ko, D., Choi, J., Choi, H. K., On, K.-W., Roh, B., and Kim, H. J. Meltr: Meta loss
transformer for learning to fine-tune video foundation models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (2023), pp. 20105–20115.
[267] Kocaoglu, M., Snyder, C., Dimakis, A. G., and Vishwanath, S.
CausalGAN: Learn-
ing causal implicit generative models with adversarial training. In International Conference on
Learning Representations (2018).
[268] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models
are zero-shot reasoners. In Advances in Neural Information Processing Systems (2022), A. H. Oh,
A. Agarwal, D. Belgrave, and K. Cho, Eds.
[269] Kolling, C., More, M., Gavenski, N., Pooch, E., Parraga, O., and Barros, R. C.
Efficient counterfactual debiasing for visual question answering. In 2022 IEEE/CVF Winter Con-
ference on Applications of Computer Vision (WACV) (2022), pp. 2572–2581.
[270] Kong, L., Ganesh, P., Wang, T., Liu, J., Zhang, L., and Chen, Y. Free lunch for co-saliency
detection: Context adjustment, 2021.
[271] Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning.
In International Conference on Learning Representations (2022).
62


--- Page 63 ---
[272] Krasanakis, E., Spyromitros-Xioufis, E., Papadopoulos, S., and Kompatsiaris, Y.
Adaptive sensitive reweighting to mitigate bias in fairness-aware classification.
In Proceedings
of the 2018 world wide web conference (2018), pp. 853–862.
[273] Kreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. Can neural machine translation
be improved with user feedback?
In Proceedings of the 2018 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-
ume 3 (Industry Papers) (New Orleans - Louisiana, June 2018), Association for Computational
Linguistics, pp. 92–105.
[274] Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convo-
lutional neural networks. Communications of the ACM 60, 6 (2017), 84–90.
[275] Krotov, D., and Hopfield, J. J. Dense associative memory is robust to adversarial inputs.
arXiv preprint arXiv:1701.00939 (2017).
[276] Kumar, A., Ma, T., and Liang, P. Understanding self-training for gradual domain adaptation.
In International Conference on Machine Learning (2020), PMLR, pp. 5468–5479.
[277] Kumar, A., Raghunathan, A., Jones, R., Ma, T., and Liang, P. Fine-tuning can distort
pretrained features and underperform out-of-distribution. arXiv preprint arXiv:2202.10054 (2022).
[278] Kumar, S., Balachandran, V., Njoo, L., Anastasopoulos, A., and Tsvetkov, Y. Lan-
guage generation models can cause harm: So what can we do about it? an actionable survey. arXiv
preprint arXiv:2210.07700 (2022).
[279] Kurakin, A., Goodfellow, I., and Bengio, S. Adversarial examples in the physical world.
In Workshop of International Conference on Learning Representations (2017).
[280] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. Albert: A
lite bert for self-supervised learning of language representations. In International Conference on
Learning Representations (2020).
[281] Landeiro, V., and Culotta, A. Robust text classification in the presence of confounding bias.
In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (2016), AAAI’16, AAAI
Press, p. 186–193.
[282] Lauscher, A., L¨uken, T., and Glavaˇs, G. Sustainable modular debiasing of language models,
2021.
[283] Lawrence, C., and Riezler, S. Improving a neural semantic parser by counterfactual learning
from human bandit feedback. In Proceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) (Melbourne, Australia, July 2018), Associa-
tion for Computational Linguistics, pp. 1820–1830.
[284] Le, H., Chen, N. F., and Hoi, S. C. H. c3: Compositional counterfactual constrastive learning
for video-grounded dialogues, 2021.
[285] Lee, K., Firat, O., Agarwal, A., Fannjiang, C., and Sussillo, D. Hallucinations in neural
machine translation, 2019.
[286] Lee, T., Choi, M., and Yoon, S. Manifold regularized deep neural networks using adversarial
examples. arXiv preprint arXiv:1511.06381 (2015).
[287] Lee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C. Surgical
fine-tuning improves adaptation to distribution shifts. In The Eleventh International Conference
on Learning Representations (2023).
[288] Lei, T., Barzilay, R., and Jaakkola, T. Rationalizing neural predictions. In Proceedings of
the 2016 Conference on Empirical Methods in Natural Language Processing (Austin, Texas, Nov.
2016), Association for Computational Linguistics, pp. 107–117.
[289] Lemeire, J., and Dirkx, E. Causal models as minimal descriptions of multivariate systems,
2006.
[290] Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt
tuning, 2021.
[291] Li, C., Chan, S. H., and Chen, Y.-T. Driver-centric risk object identification. arXiv preprint
63


--- Page 64 ---
arXiv:2106.13201 (2021).
[292] Li, H., Pan, S. J., Wang, S., and Kot, A. C. Domain generalization with adversarial feature
learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(2018), pp. 5400–5409.
[293] Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for
unified vision-language understanding and generation. In ICML (2022).
[294] Li, S., Song, Z., Xia, Y., Yu, T., and Zhou, T. The closeness of in-context learning and
weight shifting for softmax regression, 2023.
[295] Li, X., Zhang, Z., Wei, G., Lan, C., Zeng, W., Jin, X., and Chen, Z.
Confounder
identification-free causal visual feature learning. arXiv preprint arXiv:2111.13420 (2021).
[296] Li, X. L., and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation, 2021.
[297] Li, Y., Che, T., Wang, Y., Jiang, Z., Xiong, C., and Chaturvedi, S. SPE: Symmetrical
prompt enhancement for fact probing. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for
Computational Linguistics, pp. 11689–11698.
[298] Li, Y., Tian, X., Gong, M., Liu, Y., Liu, T., Zhang, K., and Tao, D. Deep domain gener-
alization via conditional invariant adversarial networks. In Proceedings of the European Conference
on Computer Vision (ECCV) (2018), pp. 624–639.
[299] Li, Y., Yang, X., Shang, X., and Chua, T.-S.
Interventional Video Relation Detection.
Association for Computing Machinery, New York, NY, USA, 2021, p. 4091–4099.
[300] Li, Z., Yin, B., Yao, T., Guo, J., Ding, S., Chen, S., and Liu, C. Sibling-attack: Rethink-
ing transferable adversarial attacks against face recognition.
In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (2023), pp. 24626–24637.
[301] Liang, D., Huang, Z., and Lipton, Z. C. Learning noise-invariant representations for robust
speech recognition, 2018.
[302] Liang, Z., Jiang, W., Hu, H., and Zhu, J. Learning to contrast the counterfactual samples
for robust visual question answering. In Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) (Online, Nov. 2020), Association for Computational
Linguistics, pp. 3285–3292.
[303] Liao, F., Liang, M., Dong, Y., Pang, T., Zhu, J., and Hu, X. Defense against adversarial
attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (2018), pp. 1778–1787.
[304] Lin, I., Njoo, L., Field, A., Sharma, A., Reinecke, K., Althoff, T., and Tsvetkov, Y.
Gendered mental health stigma in masked language models. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec.
2022), Association for Computational Linguistics, pp. 2152–2170.
[305] Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Dollar, P. Focal loss for dense object
detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) (Oct
2017).
[306] Ling, C., Zhao, X., Lu, J., Deng, C., Zheng, C., Wang, J., Chowdhury, T., Li, Y.,
Cui, H., Zhao, T., et al. Beyond one-model-fits-all: A survey of domain specialization for large
language models. arXiv preprint arXiv:2305.18703 (2023).
[307] Lipton, Z. C. The mythos of model interpretability: In machine learning, the concept of inter-
pretability is both important and slippery. Queue 16, 3 (jun 2018), 31–57.
[308] Lipton, Z. C., Wang, Y.-X., and Smola, A. Detecting and correcting for label shift with black
box predictors. In International Conference on Machine Learning (ICML) (2018).
[309] Liu, H., Tam, D., Muqeeth, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. A.
Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances
in Neural Information Processing Systems 35 (2022), 1950–1965.
[310] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen, W. What makes good
64


--- Page 65 ---
in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The
3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures (Dublin,
Ireland and Online, May 2022), Association for Computational Linguistics, pp. 100–114.
[311] Liu, N., Du, M., Guo, R., Liu, H., and Hu, X. Adversarial attacks and defenses: An inter-
pretation perspective, 2020.
[312] Liu, X., Lai, H., Yu, H., Xu, Y., Zeng, A., Du, Z., Zhang, P., Dong, Y., and Tang, J.
Webglm: Towards an efficient web-enhanced question answering system with human preferences,
2023.
[313] Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. Gpt understands,
too, 2021.
[314] Liu, Y., Chen, J., Chen, Z., Deng, B., Huang, J., and Zhang, H. The blessings of unlabeled
background in untrimmed videos, 2021.
[315] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettle-
moyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining approach, 2019.
[316] Liu, Y., Zhang, W., Li, S., and Yu, N. Enhanced attacks on defensively distilled deep neural
networks, 2017.
[317] Liu, Z., Xu, Y., Ji, X., and Chan, A. B.
Twins: A fine-tuning framework for improved
transferability of adversarial robustness and generalization, 2023.
[318] Long, Q., Luo, T., Wang, W., and Pan, S. J. Domain confused contrastive learning for
unsupervised domain adaptation. arXiv preprint arXiv:2207.04564 (2022).
[319] Lucy, L., and Bamman, D. Gender and representation bias in GPT-3 generated stories. In
Proceedings of the Third Workshop on Narrative Understanding (Virtual, June 2021), Association
for Computational Linguistics, pp. 48–55.
[320] Lundberg, S. M., and Lee, S.-I. A unified approach to interpreting model predictions. Advances
in neural information processing systems 30 (2017).
[321] Luo, Y., Boix, X., Roig, G., Poggio, T., and Zhao, Q. Foveation-based mechanisms alleviate
adversarial examples. Computer Science (2015).
[322] Lyu, C., Huang, K., and Liang, H.-N. A unified gradient regularization family for adversarial
examples. 2015 IEEE International Conference on Data Mining (Nov 2015).
[323] Ma, L., and Liang, L. Increasing-margin adversarial ({ima}) training to improve adversarial
robustness of neural networks, 2021.
[324] Madras, D., Creager, E., Pitassi, T., and Zemel, R.
Learning adversarially fair and
transferable representations. In International Conference on Machine Learning (2018), PMLR,
pp. 3384–3393.
[325] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning
models resistant to adversarial attacks. In International Conference on Learning Representations
(2018).
[326] Maekawa, S., Zhang, D., Kim, H., Rahman, S., and Hruschka, E. Low-resource interactive
active labeling for fine-tuning language models. In Findings of the Association for Computational
Linguistics: EMNLP 2022 (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Com-
putational Linguistics, pp. 3230–3242.
[327] Mahajan, D., Tople, S., and Sharma, A. Domain generalization using causal matching, 2021.
[328] Mahendran, A., and Vedaldi, A.
Understanding deep image representations by inverting
them. In Proceedings of the IEEE conference on computer vision and pattern recognition (2015),
pp. 5188–5196.
[329] Malaviya, C., Bhatia, S., and Yatskar, M. Cascading biases: Investigating the effect of
heuristic annotation strategies on data and models. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022),
Association for Computational Linguistics, pp. 6525–6540.
[330] Mansour, Y., Mohri, M., and Rostamizadeh, A. Domain adaptation: Learning bounds and
65


--- Page 66 ---
algorithms. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada,
June 18-21, 2009 (2009).
[331] Mao, C., Teotia, R., Sundar, A., Menon, S., Yang, J., Wang, X., and Vondrick,
C. Doubly right object recognition: A why prompt for visual rationales. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023), pp. 2722–2732.
[332] Marasovi´c, A., Beltagy, I., Downey, D., and Peters, M. E. Few-shot self-rationalization
with natural language prompts. arXiv preprint arXiv:2111.08284 (2021).
[333] Matsui, T., Taki, M., Pham, T. Q., Chikazoe, J., and Jimura, K. Counterfactual explana-
tion of brain activity classifiers using image-to-image transfer by generative adversarial network,
2021.
[334] Mattern, J., Jin, Z., Sachan, M., Mihalcea, R., and Sch¨olkopf, B.
Understanding
stereotypes in language models: Towards robust measurement and zero-shot debiasing.
arXiv
preprint arXiv:2212.10678 (2022).
[335] Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. On faithfulness and factual-
ity in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics (Online, July 2020), Association for Computational Linguistics,
pp. 1906–1919.
[336] Mazurowski, M. A., Dong, H., Gu, H., Yang, J., Konz, N., and Zhang, Y. Segment
anything model for medical image analysis: an experimental study, 2023.
[337] McDonald, J. Confounding variables. Handbook of biological statistics, 3rd edn. Sparky House
Publishing, Baltimore (2014), 24–28.
[338] McDuff, D., Song, Y., Lee, J., Vineet, V., Vemprala, S., Gyde, N., Salman, H., Ma,
S., Sohn, K., and Kapoor, A. Causalcity: Complex simulations with agency for causal discovery
and reasoning, 06 2021.
[339] Meade, N., Poole-Dayan, E., and Reddy, S. An empirical survey of the effectiveness of
debiasing techniques for pre-trained language models. arXiv preprint arXiv:2110.08527 (2021).
[340] Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. A survey on
bias and fairness in machine learning. ACM Computing Surveys (CSUR) 54, 6 (2021), 1–35.
[341] Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associa-
tions in gpt, 2023.
[342] Merchant, A., Rahimtoroghi, E., Pavlick, E., and Tenney, I. What happens to BERT
embeddings during fine-tuning? In Proceedings of the Third BlackboxNLP Workshop on Analyzing
and Interpreting Neural Networks for NLP (Online, Nov. 2020), Association for Computational
Linguistics, pp. 33–44.
[343] Mertes,
S.,
Huber,
T.,
Weitz,
K.,
Heimerl,
A.,
and Andr’e,
E.
Ganterfac-
tual—counterfactual explanations for medical non-experts using generative adversarial learning.
In Frontiers in Artificial Intelligence (2022).
[344] Mikolov, T., Chen, K., Corrado, G., and Dean, J. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781 (2013).
[345] Mogensen, S. W. Instrumental processes using integrated covariances, 2023.
[346] Montavon, G., Samek, W., and M¨uller, K.-R. Methods for interpreting and understanding
deep neural networks. Digital Signal Processing 73 (feb 2018), 1–15.
[347] Moon, J. H., Lee, H., Shin, W., Kim, Y.-H., and Choi, E. Multi-modal understanding
and generation for medical images and text via vision-language pre-training. IEEE Journal of
Biomedical and Health Informatics 26, 12 (dec 2022), 6070–6080.
[348] Moor, M., Banerjee, O., Shakeri, Z., Krumholz, H., Leskovec, J., Topol, E., and
Rajpurkar, P. Foundation models for generalist medical artificial intelligence. Nature 616 (04
2023), 259–265.
[349] Moosavi-Dezfooli, S.-M., et al. Deepfool: a simple and accurate method to fool deep neural
networks. In CVPR (2016), pp. 2574–2582.
66


--- Page 67 ---
[350] Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P. Universal adversarial
perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition
(2017), pp. 1765–1773.
[351] Mosbach, M., Khokhlova, A., Hedderich, M. A., and Klakow, D.
On the interplay
between fine-tuning and sentence-level probing for linguistic knowledge in pre-trained transformers.
In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks
for NLP (Online, Nov. 2020), Association for Computational Linguistics, pp. 68–82.
[352] Motiian, S., Piccirilli, M., Adjeroh, D. A., and Doretto, G. Unified deep supervised
domain adaptation and generalization. In The IEEE International Conference on Computer Vision
(ICCV) (2017), vol. 2, p. 3.
[353] Muandet, K., Balduzzi, D., and Sch¨olkopf, B. Domain generalization via invariant feature
representation. In International Conference on Machine Learning (2013), pp. 10–18.
[354] M¨utze, A., Rottmann, M., and Gottschalk, H. Semi-supervised domain adaptation with
cyclegan guided by a downstream task loss. arXiv preprint arXiv:2208.08815 (2022).
[355] Na, T., Ko, J. H., and Mukhopadhyay, S. Cascade adversarial machine learning regularized
with a unified embedding. arXiv preprint arXiv:1708.02582 (2017).
[356] Nadeem, M., Bethke, A., and Reddy, S. Stereoset: Measuring stereotypical bias in pretrained
language models. arXiv preprint arXiv:2004.09456 (2020).
[357] Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain,
S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G.,
Button, K., Knight, M., Chess, B., and Schulman, J. Webgpt: Browser-assisted question-
answering with human feedback, 2022.
[358] Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J. Learning from failure: Training debiased
classifier from biased classifier. In Advances in Neural Information Processing Systems (2020).
[359] Namkoong, H., and Duchi, J. C.
Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Advances in Neural Information Processing Systems (2016),
D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29, Curran Associates,
Inc.
[360] Nan, G., Qiao, R., Xiao, Y., Liu, J., Leng, S., Zhang, H., and Lu, W. Interventional video
grounding with dual contrastive learning, 2021.
[361] Nangia, N., Vania, C., Bhalerao, R., and Bowman, S. R. CrowS-pairs: A challenge dataset
for measuring social biases in masked language models. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP) (Online, Nov. 2020), Association for
Computational Linguistics, pp. 1953–1967.
[362] Narasimhan, H. Learning with complex loss functions and constraints. In International Confer-
ence on Artificial Intelligence and Statistics (2018), PMLR, pp. 1646–1654.
[363] Narodytska, N., and Kasiviswanathan, S. P. Simple black-box adversarial perturbations for
deep networks, 2016.
[364] Nayebi, A., and Ganguli, S. Biologically inspired protection of deep networks from adversarial
attacks. arXiv preprint arXiv:1703.09202 (2017).
[365] Nguyen, A., Dosovitskiy, A., Yosinski, J., Brox, T., and Clune, J. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. Advances in neural
information processing systems 29 (2016).
[366] Nguyen, A. T., Tran, T., Gal, Y., and Baydin, A. G. Domain invariant representation
learning with domain density transformations. arXiv preprint arXiv:2102.05082 (2021).
[367] Nguyen, L., and Sinha, A. ‘a learning and masking approach to secure learning, 2017.
[368] Niu, Y., Tang, K., Zhang, H., Lu, Z., Hua, X.-S., and Wen, J.-R. Counterfactual vqa: A
cause-effect look at language bias, 2021.
[369] Nozza, D., Bianchi, F., and Hovy, D. HONEST: Measuring hurtful sentence completion in
language models. In Proceedings of the 2021 Conference of the North American Chapter of the
67


--- Page 68 ---
Association for Computational Linguistics: Human Language Technologies (Online, June 2021),
Association for Computational Linguistics, pp. 2398–2406.
[370] Oh, G., Jeong, E., and Lim, S.
Causal affect prediction model using a past facial image
sequence. In 2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)
(Los Alamitos, CA, USA, oct 2021), IEEE Computer Society, pp. 3543–3549.
[371] Oh, K., Yoon, J. S., and Suk, H.-I. Born identity network: Multi-way counterfactual map
generation to explain a classifier’s decision, 2021.
[372] Oh, S. J., Fritz, M., and Schiele, B. Adversarial image perturbation for privacy protection a
game theory perspective. 2017 IEEE International Conference on Computer Vision (ICCV) (Oct
2017).
[373] Oneto, L., Doninini, M., Elders, A., and Pontil, M. Taking advantage of multitask learning
for fair classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society
(2019), pp. 227–237.
[374] OpenAI. Gpt-4 technical report, 2023.
[375] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fer-
nandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W.,
Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G.,
Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2:
Learning robust visual features without supervision, 2023.
[376] Ou, J., Zhang, J., Feng, Y., and Zhou, J. Counterfactual data augmentation via perspective
transition for open-domain dialogues. In Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for
Computational Linguistics, pp. 1635–1648.
[377] Ouyang, C., Chen, C., Li, S., Li, Z., Qin, C., Bai, W., and Rueckert, D. Causality-
inspired single-source domain generalization for medical image segmentation, 2021.
[378] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C.,
Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,
Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training
language models to follow instructions with human feedback. In Advances in Neural Information
Processing Systems (2022), A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds.
[379] Pan, S. J., and Yang, Q. A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering 22, 10 (2010), 1345–1359.
[380] Pang, T., Yang, X., Dong, Y., Su, H., and Zhu, J. Bag of tricks for adversarial training.
arXiv preprint arXiv:2010.00467 (2020).
[381] Pang, T., Yang, X., Dong, Y., Xu, K., Zhu, J., and Su, H. Boosting adversarial training
with hypersphere embedding.
Advances in Neural Information Processing Systems 33 (2020),
7779–7792.
[382] Papernot, N., McDaniel, P., and Goodfellow, I.
Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277
(2016).
[383] Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z. B., and Swami, A.
Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia
conference on computer and communications security (2017), ACM, pp. 506–519.
[384] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., and Swami, A.
The limitations of deep learning in adversarial settings. In Security and Privacy (EuroS&P), 2016
IEEE European Symposium on (2016), IEEE, pp. 372–387.
[385] Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A. Distillation as a defense to
adversarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and
Privacy (SP) (2016), IEEE, pp. 582–597.
[386] Pastaltzidis, I., Dimitriou, N., Quezada-Tavarez, K., Aidinlis, S., Marquenie, T.,
Gurzawska, A., and Tzovaras, D. Data augmentation for fairness-aware machine learning:
68


--- Page 69 ---
Preventing algorithmic bias in law enforcement systems. In Proceedings of the 2022 ACM Con-
ference on Fairness, Accountability, and Transparency (New York, NY, USA, 2022), FAccT ’22,
Association for Computing Machinery, p. 2302–2314.
[387] Pawlowski, N., Coelho de Castro, D., and Glocker, B. Deep structural causal models for
tractable counterfactual inference. Advances in Neural Information Processing Systems 33 (2020),
857–869.
[388] Pearl, J. Causal diagrams for empirical research. Biometrika 82, 4 (1995), 669–688.
[389] Pearl, J. Causality: Models, Reasoning, and Inference, 2nd ed. Cambridge University Press,
2009.
[390] Pearl, J., et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress
19 (2000).
[391] Pearl, J., and Mackenzie, D. The book of why: the new science of cause and effect. Basic
books, 2018.
[392] Peng, K.-C., Wu, Z., and Ernst, J. Zero-shot deep domain adaptation. In Proceedings of the
European Conference on Computer Vision (ECCV) (2018), pp. 764–781.
[393] Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word represen-
tation. In Proceedings of the 2014 conference on empirical methods in natural language processing
(EMNLP) (2014), pp. 1532–1543.
[394] Peters, J., Janzing, D., and Sch¨olkopf, B. Elements of causal inference: foundations and
learning algorithms. The MIT Press, 2017.
[395] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettle-
moyer, L. Deep contextualized word representations, 2018.
[396] Petroni, F., Rockt¨aschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller,
A. Language models as knowledge bases?
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) (Hong Kong, China, Nov. 2019), Association for Compu-
tational Linguistics, pp. 2463–2473.
[397] Petsiuk, V., Das, A., and Saenko, K. Rise: Randomized input sampling for explanation of
black-box models. arXiv preprint arXiv:1806.07421 (2018).
[398] Pfeiffer, J., Kamath, A., R¨uckl´e, A., Cho, K., and Gurevych, I. Adapterfusion: Non-
destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247 (2020).
[399] Pfeiffer, J., Vuli´c, I., Gurevych, I., and Ruder, S. Mad-x: An adapter-based framework
for multi-task cross-lingual transfer. arXiv preprint arXiv:2005.00052 (2020).
[400] Pfister, N., and Peters, J. Identifiability of sparse causal effects using instrumental variables,
2022.
[401] Plumb, G., Ribeiro, M. T., and Talwalkar, A. Finding and fixing spurious patterns with
explanations, 2021.
[402] Pourhoseingholi, M. A., Baghestani, A. R., and Vahedi, M. How to control confounding
effects by statistical analysis. Gastroenterology and hepatology from bed to bench 5, 2 (2012), 79.
[403] Prabhushankar, M., and Alregib, G.
Extracting causal visual features for limited label
classification, 03 2021.
[404] Qi, J., Niu, Y., Huang, J., and Zhang, H. Two causal principles for improving visual dialog,
2020.
[405] Qin, G., and Eisner, J. Learning how to ask: Querying LMs with mixtures of soft prompts. In
Proceedings of the 2021 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies (Online, June 2021), Association for Compu-
tational Linguistics, pp. 5203–5212.
[406] Qin, W., Zhang, H., Hong, R., Lim, E.-P., and Sun, Q. Causal interventional training for
image recognition. IEEE Transactions on Multimedia (2021), 1–1.
69


--- Page 70 ---
[407] Qin, Z., Yi, H., Lao, Q., and Li, K. Medical image understanding with pretrained vision
language models: A comprehensive study, 2023.
[408] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from
natural language supervision. In International conference on machine learning (2021), PMLR,
pp. 8748–8763.
[409] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language
models are unsupervised multitask learners, 2019.
[410] Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides,
J., Henderson, S., Ring, R., Young, S., et al. Scaling language models: Methods, analysis
& insights from training gopher. arXiv preprint arXiv:2112.11446 (2021).
[411] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y.,
Li, W., and Liu, P. J.
Exploring the limits of transfer learning with a unified text-to-text
transformer, 2020.
[412] Raghuram, J., Chandrasekaran, V., Jha, S., and Banerjee, S.
Detecting anomalous
inputs to dnn classifiers by joint statistical testing at the layers, 07 2020.
[413] Rahimian, H., and Mehrotra, S. Frameworks and results in distributionally robust optimiza-
tion. Open Journal of Mathematical Optimization 3 (jul 2022), 1–85.
[414] Rahman, M. M., Fookes, C., and Sridharan, S. Discriminative domain-invariant adversarial
network for deep domain generalization. arXiv preprint arXiv:2108.08995 (2021).
[415] Rajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging lan-
guage models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics (Florence, Italy, July 2019), Association for Computational
Linguistics, pp. 4932–4942.
[416] Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K.,
and Shoham, Y. In-context retrieval-augmented language models, 2023.
[417] Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage,
C., Hajishirzi, H., and Choi, Y. Is reinforcement learning (not) for natural language processing:
Benchmarks, baselines, and building blocks for natural language policy optimization.
In The
Eleventh International Conference on Learning Representations (2023).
[418] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional
image generation with clip latents, 2022.
[419] Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
Sutskever, I.
Zero-shot text-to-image generation.
In International Conference on Machine
Learning (2021), PMLR, pp. 8821–8831.
[420] Rao, Y., Chen, G., Lu, J., and Zhou, J. Counterfactual attention learning for fine-grained
visual categorization and re-identification, 2021.
[421] Rasheed, B., Khan, A., Ahmad, M., Mazzara, M., Kazmi, S., et al. Multiple adversarial
domains adaptation approach for mitigating adversarial attacks effects. International Transactions
on Electrical Energy Systems 2022 (2022).
[422] Raunak, V., Menezes, A., and Junczys-Dowmunt, M. The curious case of hallucinations in
neural machine translation. arXiv preprint arXiv:2104.06683 (2021).
[423] Reddy, A. G., L, B. G., and Balasubramanian, V. N. On causally disentangled representa-
tions, 2021.
[424] Ren, S., He, K., Girshick, R., and Sun, J. Faster r-cnn: Towards real-time object detection
with region proposal networks. In Advances in Neural Information Processing Systems (2015),
C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, Eds., vol. 28, Curran Associates,
Inc.
[425] Ren, Y., Mao, Z., Fang, S., Lu, Y., He, T., Du, H., Zhang, Y., and Ouyang, W. Crossing
the gap: Domain generalization for image captioning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) (June 2023), pp. 2871–2880.
70


--- Page 71 ---
[426] Resnick, C., Litany, O., Kar, A., Kreis, K., Lucas, J., Cho, K., and Fidler, S. Causal
scene bert: Improving object detection by searching for challenging groups of data, 02 2022.
[427] Ribeiro, M. T., Singh, S., and Guestrin, C.
” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining (2016), pp. 1135–1144.
[428] Rodriguez, P., Caccia, M., Lacoste, A., Zamparo, L., Laradji, I., Charlin, L., and
Vazquez, D. Beyond trivial counterfactual explanations with diverse valuable explanations, 2021.
[429] Rohrbach, A., Hendricks, L. A., Burns, K., Darrell, T., and Saenko, K.
Object
hallucination in image captioning. arXiv preprint arXiv:1809.02156 (2018).
[430] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (June 2022), pp. 10684–10695.
[431] Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical
image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI
2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III
18 (2015), Springer, pp. 234–241.
[432] Rosenberg, D., Gat, I., Feder, A., and Reichart, R. Are vqa systems rad? measuring
robustness to augmented data with focused interventions, 2021.
[433] Rosenfeld, E., Ravikumar, P., and Risteski, A. The risks of invariant risk minimization.
arXiv preprint arXiv:2010.05761 (2020).
[434] Ross, A. S., and Doshi-Velez, F. Improving the adversarial robustness and interpretability of
deep neural networks by regularizing their input gradients, 2017.
[435] Ross, S. I., Martinez, F., Houde, S., Muller, M., and Weisz, J. D. The programmer’s
assistant: Conversational interaction with a large language model for software development. In
Proceedings of the 28th International Conference on Intelligent User Interfaces (2023), pp. 491–514.
[436] Rozsa, A., Rudd, E. M., and Boult, T. E. Adversarial diversity and hard positive generation.
2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) (Jun
2016).
[437] Rubin, O., Herzig, J., and Berant, J. Learning to retrieve prompts for in-context learn-
ing. In Proceedings of the 2022 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies (Seattle, United States, July 2022),
Association for Computational Linguistics, pp. 2655–2671.
[438] R¨uckl´e, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., and
Gurevych, I.
Adapterdrop:
On the efficiency of adapters in transformers.
arXiv preprint
arXiv:2010.11918 (2020).
[439] Sadria, M., Layton, A., and Bader, G. Adversarial training improves model interpretability
in single-cell rna-seq analysis. bioRxiv (2023), 2023–05.
[440] Saengkyongam, S., Henckel, L., Pfister, N., and Peters, J. Exploiting independent in-
struments: Identification and distribution generalization. In International Conference on Machine
Learning (2022), PMLR, pp. 18935–18958.
[441] Saenko, K., Kulis, B., Fritz, M., and Darrell, T. Adapting visual category models to new
domains. In Proceedings of the European conference on computer vision (2010).
[442] Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural
networks for group shifts: On the importance of regularization for worst-case generalization. arXiv
preprint arXiv:1911.08731 (2019).
[443] Sagawa*, S., Koh*, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural
networks. In International Conference on Learning Representations (2020).
[444] Sajjadi, M., Javanmardi, M., and Tasdizen, T. Regularization with stochastic transforma-
tions and perturbations for deep semi-supervised learning, 2016.
[445] Sallam, M. Chatgpt utility in healthcare education, research, and practice: systematic review on
71


--- Page 72 ---
the promising perspectives and valid concerns. In Healthcare (2023), vol. 11, MDPI, p. 887.
[446] Samek, W., Binder, A., Montavon, G., Lapuschkin, S., and M¨uller, K.-R. Evaluating the
visualization of what a deep neural network has learned. IEEE Transactions on Neural Networks
and Learning Systems 28 (11 2017), 2660–2673.
[447] Sani, N., Malinsky, D., and Shpitser, I. Explaining the behavior of black-box prediction
algorithms with causal learning, 2021.
[448] Sarkar, S., Bansal, A., Mahbub, U., and Chellappa, R. Upset and angri: Breaking high
performance image classifiers. arXiv preprint arXiv:1707.01159 (2017).
[449] Sauer, A., and Geiger, A. Counterfactual generative networks, 2021.
[450] Schick, T., and Sch¨utze, H. Exploiting cloze questions for few shot text classification and
natural language inference, 2021.
[451] Schick, T., and Sch¨utze, H. Few-shot text generation with pattern-exploiting training, 2021.
[452] Schick, T., Udupa, S., and Sch¨utze, H. Self-diagnosis and self-debiasing: A proposal for
reducing corpus-based bias in nlp, 2021.
[453] Sch¨olkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. On
causal and anticausal learning. arXiv preprint arXiv:1206.6471 (2012).
[454] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy
optimization algorithms, 2017.
[455] Schwab, P., and Karlen, W. Cxplain: Causal explanations for model interpretation under
uncertainty. Advances in Neural Information Processing Systems 32 (2019).
[456] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D.
Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings
of the IEEE international conference on computer vision (2017), pp. 618–626.
[457] Seo, S., Suh, Y., Kim, D., Kim, G., Han, J., and Han, B. Learning to optimize domain
specific normalization for domain generalization. In European Conference on Computer Vision
(2020), Springer, pp. 68–83.
[458] Shafieezadeh-Abadeh, S., Esfahani, P. M., and Kuhn, D. Distributionally robust logistic
regression, 2015.
[459] Shaghaghian, S., Feng, L. Y., Jafarpour, B., and Pogrebnyakov, N. Customizing con-
textualized language models for legal document reviews. In 2020 IEEE International Conference
on Big Data (Big Data) (2020), IEEE, pp. 2139–2148.
[460] Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. The pitfalls of
simplicity bias in neural networks. Advances in Neural Information Processing Systems 33 (2020),
9573–9585.
[461] Shah, M., Chen, X., Rohrbach, M., and Parikh, D. Cycle-consistency for robust visual
question answering, 2019.
[462] Shankar, S., Piratla, V., Chakrabarti, S., Chaudhuri, S., Jyothi, P., and Sarawagi,
S. Generalizing across domains via cross-gradient training. In International Conference on Learning
Representations (2018).
[463] Shao, F., Luo, Y., Zhang, L., Ye, L., Tang, S., Yang, Y., and Xiao, J.
Improving
weakly-supervised object localization via causal intervention, 2021.
[464] Sharma, S., Dey, M., and Sinha, K. How sensitive are translation systems to extra contexts?
mitigating gender bias in neural machine translation models through relevant contexts. In Find-
ings of the Association for Computational Linguistics: EMNLP 2022 (Abu Dhabi, United Arab
Emirates, Dec. 2022), Association for Computational Linguistics, pp. 1968–1984.
[465] Sharma, S., Zhang, Y., R´ıos Aliaga, J. M., Bouneffouf, D., Muthusamy, V., and
Varshney, K. R. Data augmentation for discrimination prevention and bias disambiguation.
In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (New York, NY, USA,
2020), AIES ’20, Association for Computing Machinery, p. 358–364.
72


--- Page 73 ---
[466] Shen, F., Liu, J., and Hu, P. Conterfactual generative zero-shot semantic segmentation, 2021.
[467] Shen, Y., Heacock, L., Elias, J., Hentel, K. D., Reig, B., Shih, G., and Moy, L.
ChatGPT and other large language models are double-edged swords. Radiology 307, 2 (Apr. 2023),
e230163.
[468] Shen, Z., Liu, J., He, Y., Zhang, X., Xu, R., Yu, H., and Cui, P. Towards out-of-distribution
generalization: A survey. arXiv preprint arXiv:2108.13624 (2021).
[469] Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N. The woman worked as a babysitter:
On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) (Hong Kong, China, Nov. 2019), Association for Computational
Linguistics, pp. 3407–3412.
[470] Shi, P., Qiu, J., Abaxi, S. M. D., Wei, H., Lo, F. P.-W., and Yuan, W. Generalist vision
foundation models for medical imaging: A case study of segment anything model on zero-shot
medical segmentation. Diagnostics 13, 11 (2023), 1947.
[471] Shi, P., Qiu, J., Abaxi, S. M. D., Wei, H., Lo, F. P.-W., and Yuan, W. Generalist vision
foundation models for medical imaging: A case study of segment anything model on zero-shot
medical segmentation. Diagnostics 13, 11 (jun 2023), 1947.
[472] Shi, W., Shea, R., Chen, S., Zhang, C., Jia, R., and Yu, Z. Just fine-tune twice: Selective
differential privacy for large language models. In Proceedings of the 2022 Conference on Empir-
ical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022),
Association for Computational Linguistics, pp. 6327–6340.
[473] Shin, R., Lin, C., Thomson, S., Chen, C., Roy, S., Platanios, E. A., Pauls, A., Klein, D.,
Eisner, J., and Van Durme, B. Constrained language models yield few-shot semantic parsers. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (Online
and Punta Cana, Dominican Republic, Nov. 2021), Association for Computational Linguistics,
pp. 7699–7715.
[474] Shin, T., Razeghi, Y., au2, R. L. L. I., Wallace, E., and Singh, S. Autoprompt: Eliciting
knowledge from language models with automatically generated prompts, 2020.
[475] Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through prop-
agating activation differences. In International conference on machine learning (2017), PMLR,
pp. 3145–3153.
[476] Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.
Retrieval augmentation
reduces hallucination in conversation. In Findings of the Association for Computational Linguistics:
EMNLP 2021 (Punta Cana, Dominican Republic, Nov. 2021), Association for Computational
Linguistics, pp. 3784–3803.
[477] Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visual-
ising image classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).
[478] Singhal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tan-
wani, A., Cole-Lewis, H., Pfohl, S., Payne, P., Seneviratne, M., Gamble, P., Kelly,
C., Babiker, A., Sch¨arli, N., Chowdhery, A., Mansfield, P., Demner-Fushman, D.,
Ag¨uera Y Arcas, B., Webster, D., Corrado, G. S., Matias, Y., Chou, K., Gottweis,
J., Tomasev, N., Liu, Y., Rajkomar, A., Barral, J., Semturs, C., Karthikesalingam,
A., and Natarajan, V. Large language models encode clinical knowledge. Nature (July 2023).
[479] Singla, S., and Feizi, S. Salient imagenet: How to discover spurious features in deep learning?,
2022.
[480] Singla, S., Pollack, B., Wallace, S., and Batmanghelich, K. Explaining the black-box
smoothly- a counterfactual approach, 2021.
[481] Singla, S., Wallace, S., Triantafillou, S., and Batmanghelich, K. Using causal analysis
for conceptual deep learning explanation. Medical image computing and computer-assisted interven-
tion : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted
Intervention 12903 (2021), 519–528.
73


--- Page 74 ---
[482] Smith, S. C., and Ramamoorthy, S. Counterfactual explanation and causal inference in service
of robustness in robot control, 2020.
[483] Snell, C. V., Kostrikov, I., Su, Y., Yang, S., and Levine, S.
Offline RL for natural
language generation with implicit language q learning. In The Eleventh International Conference
on Learning Representations (2023).
[484] Sorensen, T., Robinson, J., Rytting, C., Shaw, A., Rogers, K., Delorey, A., Khalil,
M., Fulda, N., and Wingate, D. An information-theoretic approach to prompt engineering
without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) (Dublin, Ireland, May 2022), Association for
Computational Linguistics, pp. 819–862.
[485] Sridhar, D., and Getoor, L. Estimating causal effects of tone in online debates, 2019.
[486] Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A.,
Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. In Ad-
vances in Neural Information Processing Systems (2020), H. Larochelle, M. Ranzato, R. Hadsell,
M. Balcan, and H. Lin, Eds., vol. 33, Curran Associates, Inc., pp. 3008–3021.
[487] Storkey, A. When training and test sets are different: characterizing learning transfer. Dataset
shift in machine learning (2009).
[488] Strauss, T., Hanselmann, M., Junginger, A., and Ulmer, H. Ensemble methods as a
defense to adversarial perturbations against deep neural networks. arXiv preprint arXiv:1709.03423
(2017).
[489] Strumbelj, E., and Kononenko, I. An efficient explanation of individual classifications using
game theory. The Journal of Machine Learning Research 11 (2010), 1–18.
[490] ˇStrumbelj, E., Kononenko, I., and ˇSikonja, M. R. Explaining instance classifications with
interactions of subsets of feature values. Data & Knowledge Engineering 68, 10 (2009), 886–904.
[491] Su, J., Vargas, D. V., and Sakurai, K. One pixel attack for fooling deep neural networks.
IEEE Transactions on Evolutionary Computation (2019).
[492] Su, X., Zhao, Y., and Bethard, S. A comparison of strategies for source-free domain adapta-
tion. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) (2022), pp. 8352–8367.
[493] Sun, P., Wu, B., Li, X., Li, W., Duan, L., and Gan, C. Counterfactual Debiasing Inference
for Compositional Action Recognition. Association for Computing Machinery, New York, NY, USA,
2021, p. 3220–3228.
[494] Sun, T., He, J., Qiu, X., and Huang, X. BERTScore is unfair: On social bias in language
model-based metrics for text generation. In Proceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association
for Computational Linguistics, pp. 3726–3739.
[495] Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., and Wang, H.
Ernie 2.0: A
continual pre-training framework for language understanding. Proceedings of the AAAI Conference
on Artificial Intelligence 34, 05 (Apr. 2020), 8968–8975.
[496] Sun, Z., Ozay, M., and Okatani, T. Hypernetworks with statistical filtering for defending
adversarial examples. arXiv preprint arXiv:1711.01791 (2017).
[497] Sundararajan, M., Taly, A., and Yan, Q.
Axiomatic attribution for deep networks.
In
International conference on machine learning (2017), PMLR, pp. 3319–3328.
[498] Sung, Y.-L., Cho, J., and Bansal, M. Lst: Ladder side-tuning for parameter and memory
efficient transfer learning. arXiv preprint arXiv:2206.06522 (2022).
[499] Sung, Y.-L., Nair, V., and Raffel, C. A. Training neural networks with fixed sparse masks.
Advances in Neural Information Processing Systems 34 (2021), 24193–24205.
[500] Suter, R., Miladinovic, D., Sch¨olkopf, B., and Bauer, S. Robustly disentangled causal
mechanisms: Validating deep representations for interventional robustness. In International Con-
ference on Machine Learning (2019), PMLR, pp. 6056–6065.
74


--- Page 75 ---
[501] Szegedy, C., et al. Intriguing properties of neural networks. arXiv:1312.6199 (2013).
[502] Tack, J., Yu, S., Jeong, J., Kim, M., Hwang, S. J., and Shin, J. Consistency regularization
for adversarial robustness, 2021.
[503] Taghanaki, S. A., Khani, A., Khani, F., Gholami, A., Tran, L., Mahdavi-Amiri, A.,
and Hamarneh, G. Masktune: Mitigating spurious correlations by forcing to explore, 2022.
[504] Tan, B., Song, Y., Zhong, E., and Yang, Q. Transitive transfer learning. In Proceedings
of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(2015), pp. 1155–1164.
[505] Tan, B., Zhang, Y., Pan, S., and Yang, Q. Distant domain transfer learning. In Proceedings
of the AAAI conference on artificial intelligence (2017), vol. 31.
[506] Tan, Y. C., and Celis, L. E. Assessing social and intersectional biases in contextualized word
representations, 2019.
[507] Tang, K., Niu, Y., Huang, J., Shi, J., and Zhang, H. Unbiased scene graph generation from
biased training, 2020.
[508] Tang, K., Tao, M., and Zhang, H. Adversarial visual robustness by causal intervention, 2021.
[509] Terzi, M., Achille, A., Maggipinto, M., and Susto, G. A. Adversarial training reduces
information and improves transferability, 2020.
[510] Thams, N., Søndergaard, R., Weichwald, S., and Peters, J. Identifying causal effects
using instrumental time series: Nuisance iv and correcting for the past, 2022.
[511] Thapa, S., and Adhikari, S. ChatGPT, bard, and large language models for biomedical research:
Opportunities and pitfalls. Ann Biomed Eng (June 2023).
[512] Theil, H. Estimation and simultaneous correlation in complete equation systems. In Henri Theil’s
Contributions to Economics and Econometrics: Econometric Theory and Methodology. Springer,
1992, pp. 65–107.
[513] Thomson, C., and Reiter, E. A gold standard methodology for evaluating accuracy in data-to-
text systems. In Proceedings of the 13th International Conference on Natural Language Generation
(Dublin, Ireland, Dec. 2020), Association for Computational Linguistics, pp. 158–168.
[514] Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T.,
Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications.
arXiv preprint arXiv:2201.08239 (2022).
[515] Tian, C. X., Li, H., Xie, X., Liu, Y., and Wang, S. Neuron coverage-guided domain gener-
alization. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).
[516] Tian, J., Dai, X., Ma, C.-Y., He, Z., Liu, Y.-C., and Kira, Z. Trainable projected gradient
method for robust fine-tuning, 2023.
[517] Tommasi, T., Patricia, N., Caputo, B., and Tuytelaars, T. A deeper look at dataset bias,
2015.
[518] Torralba, A., and Efros, A. A.
Unbiased look at dataset bias.
In CVPR 2011 (2011),
pp. 1521–1528.
[519] Torralba, A., and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011 (2011), IEEE,
pp. 1521–1528.
[520] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,
Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E.,
and Lample, G. Llama: Open and efficient foundation language models, 2023.
[521] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer,
C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller,
B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan,
H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X.,
Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta,
75


--- Page 76 ---
R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E.,
Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang,
Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.
[522] Tsimpoukelli, M., Menick, J., Cabi, S., Eslami, S. M. A., Vinyals, O., and Hill, F.
Multimodal few-shot learning with frozen language models, 2021.
[523] Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. Simultaneous deep transfer across
domains and tasks. In Proceedings of the IEEE international conference on computer vision (2015),
pp. 4068–4076.
[524] Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition
(2017), pp. 7167–7176.
[525] Udomcharoenchaikit,
C.,
Ponwitayarat,
W.,
Payoungkhamdee,
P.,
Masuk,
K.,
Buaphet, W., Chuangsuwanich, E., and Nutanong, S.
Mitigating spurious correlation
in natural language understanding with counterfactual inference. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates,
Dec. 2022), Association for Computational Linguistics, pp. 11308–11321.
[526] Vadillo, J., Santana, R., and Lozano, J. A. When and how to fool explainable models (and
humans) with adversarial examples, 2023.
[527] Vahdat, A. Toward robustness against label noise in training deep discriminative neural networks,
2017.
[528] VanderWeele, T. J., and Shpitser, I. On the definition of a confounder. Annals of statistics
41, 1 (2013), 196.
[529] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,
 L., and Polosukhin, I. Attention is all you need. Advances in neural information processing
systems 30 (2017).
[530] Vermeire, T., and Martens, D. Explainable image classification with evidence counterfactual,
2020.
[531] Vig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and Shieber, S.
Investigating gender bias in language models using causal mediation analysis. Advances in neural
information processing systems 33 (2020), 12388–12401.
[532] Vigen, T. Spurious correlations. Hachette books, 2015.
[533] von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zh-
moginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent, 2023.
[534] Vucetic, D., Tayaranian, M., Ziaeefard, M., Clark, J. J., Meyer, B. H., and Gross,
W. J. Efficient fine-tuning of bert models on the edge. In 2022 IEEE International Symposium
on Circuits and Systems (ISCAS) (2022), IEEE, pp. 1838–1842.
[535] Wadsworth, C., Vera, F., and Piech, C. Achieving fairness through adversarial learning: an
application to recidivism prediction. arXiv preprint arXiv:1807.00199 (2018).
[536] Wallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S. Universal adversarial
triggers for attacking and analyzing NLP. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP) (Hong Kong, China, Nov. 2019), Association for Compu-
tational Linguistics, pp. 2153–2162.
[537] Wang, B., Deng, X., and Sun, H. Iteratively prompt pre-trained language models for chain of
thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Pro-
cessing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Computational Linguistics,
pp. 2714–2730.
[538] Wang, D., Yang, Y., Tao, C., Gan, Z., Chen, L., Kong, F., Henao, R., and Carin,
L. Proactive pseudo-intervention: Causally informed contrastive learning for interpretable vision
models, 2021.
76


--- Page 77 ---
[539] Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by
penalizing local predictive power. Advances in Neural Information Processing Systems 32 (2019).
[540] Wang, H., He, Z., Lipton, Z. C., and Xing, E. P.
Learning robust representations by
projecting superficial statistics out. arXiv preprint arXiv:1903.06256 (2019).
[541] Wang, H., Huang, Z., Wu, X., and Xing, E. Toward learning robust and invariant repre-
sentations with alignment regularization and data augmentation. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (New York, NY, USA, 2022),
KDD ’22, Association for Computing Machinery, p. 1846–1856.
[542] Wang, H., Huang, Z., Zhang, H., Lee, Y. J., and Xing, E. P. Toward learning human-
aligned cross-domain robust models by countering misaligned features. In Uncertainty in Artificial
Intelligence (2022), PMLR, pp. 2075–2084.
[543] Wang, H., Huang, Z., Zhang, H., and Xing, E. Toward learning human-aligned cross-domain
robust models by countering misaligned features. UAI 2022 (2021).
[544] Wang, H., Li, B., and Zhao, H. Understanding gradual domain adaptation: Improved analysis,
optimal path and beyond. arXiv preprint arXiv:2204.08200 (2022).
[545] Wang, H., Meghawat, A., Morency, L.-P., and Xing, E. P.
Select-additive learning:
Improving generalization in multimodal sentiment analysis. 2017 IEEE International Conference
on Multimedia and Expo (ICME) (2017), 949–954.
[546] Wang, H., Ustun, B., and Calmon, F.
Repairing without retraining: Avoiding disparate
impact with counterfactual distributions. In International Conference on Machine Learning (2019),
PMLR, pp. 6618–6627.
[547] Wang, H., Wu, X., Huang, Z., and Xing, E. P. High-frequency component helps explain the
generalization of convolutional neural networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2020), pp. 8684–8694.
[548] Wang, J., Hu, X., Hou, W., Chen, H., Zheng, R., Wang, Y., Yang, L., Huang, H., Ye,
W., Geng, X., et al. On the robustness of chatgpt: An adversarial and out-of-distribution
perspective. arXiv preprint arXiv:2302.12095 (2023).
[549] Wang, J., HU, X., Hou, W., Chen, H., Zheng, R., Wang, Y., Yang, L., Ye, W., Huang,
H., Geng, X., Jiao, B., Zhang, Y., and Xie, X. On the robustness of chatGPT: An adversarial
and out-of-distribution perspective. In ICLR 2023 Workshop on Trustworthy and Reliable Large-
Scale Machine Learning Models (2023).
[550] Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., and Yu,
P. Generalizing to unseen domains: A survey on domain generalization. IEEE Transactions on
Knowledge and Data Engineering (2022).
[551] Wang, J., Liu, Y., and Wang, X. E. Assessing multilingual fairness in pre-trained multimodal
representations. arXiv preprint arXiv:2106.06683 (2021).
[552] Wang, L., Yan, Y., He, K., Wu, Y., and Xu, W. Dynamically disentangling social bias from
task-oriented representations with adversarial attack. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies (Online, June 2021), Association for Computational Linguistics, pp. 3740–3750.
[553] Wang, L., Zhang, H., Yi, J., Hsieh, C.-J., and Jiang, Y.
Spanning attack: Reinforce
black-box attacks with unlabeled data. Machine Learning 109 (2020), 2349–2368.
[554] Wang, M., and Deng, W. Deep visual domain adaptation: A survey. Neurocomputing 312 (Oct
2018), 135–153.
[555] Wang, M., and Deng, W. Mitigating bias in face recognition using skewness-aware reinforcement
learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition
(2020), pp. 9322–9331.
[556] Wang, P., Song, Y., Liu, T., Lin, B., Cao, Y., Li, S., and Sui, Z.
Learning robust
representations for continual relation extraction via adversarial class augmentation. In Proceedings
of the 2022 Conference on Empirical Methods in Natural Language Processing (Abu Dhabi, United
Arab Emirates, Dec. 2022), Association for Computational Linguistics, pp. 6264–6278.
77


--- Page 78 ---
[557] Wang, P., and Vasconcelos, N. Scout: Self-aware discriminant counterfactual explanations,
2020.
[558] Wang, Q., Guo, W., Alexander, G., Ororbia, I., Xing, X., Lin, L., Giles, C. L., Liu, X.,
Liu, P., and Xiong, G. Using non-invertible data transformations to build adversary-resistant
deep neural networks. CoRR, abs/1610.01934 (2016).
[559] Wang, Q., Guo, W., Zhang, K., Ororbia, I., Alexander, G., Xing, X., Liu, X., and
Giles, C. L. Learning adversary-resistant deep neural networks. arXiv preprint arXiv:1612.01401
(2016).
[560] Wang, Q., Guo, W., Zhang, K., Ororbia II, A. G., Xing, X., Liu, X., and Giles, C. L.
Adversary resistant deep neural networks with an application to malware detection. In Proceedings
of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
(2017), ACM, pp. 1145–1153.
[561] Wang, S., Veldhuis, R., Brune, C., and Strisciuglio, N. What do neural networks learn
in image classification? a frequency shortcut perspective, 2023.
[562] Wang, S., Zhou, J., Sun, C., Ye, J., Gui, T., Zhang, Q., and Huang, X. Causal intervention
improves implicit sentiment analysis. arXiv preprint arXiv:2208.09329 (2022).
[563] Wang, T., Huang, J., Zhang, H., and Sun, Q. Visual commonsense r-cnn. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020), pp. 10760–10770.
[564] Wang, T., Sridhar, R., Yang, D., and Wang, X. Identifying and mitigating spurious corre-
lations for improving robustness in nlp models. arXiv preprint arXiv:2110.07736 (2021).
[565] Wang, T., Zhou, C., Sun, Q., and Zhang, H. Causal attention for unbiased visual recognition.
2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021), 3071–3080.
[566] Wang, W., Gao, J., and Xu, C. Weakly-supervised video object grounding via causal inter-
vention, 2021.
[567] Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery,
A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. In
The Eleventh International Conference on Learning Representations (2023).
[568] Wang, Y., Ma, X., Bailey, J., Yi, J., Zhou, B., and Gu, Q.
On the convergence and
robustness of adversarial training, 2022.
[569] Wang, Y., Mukherjee, S., Liu, X., Gao, J., Awadallah, A. H., and Gao, J. Adamix:
Mixture-of-adapter for parameter-efficient tuning of large language models.
arXiv preprint
arXiv:2205.12410 (2022).
[570] Wang, Y., Qin, Y., Han, Y., Yin, M., Zhou, J., Yang, H., and Zhang, M.
Ad-aug:
Adversarial data augmentation for counterfactual recommendation.
In Machine Learning and
Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France,
September 19–23, 2022, Proceedings, Part I (2023), Springer, pp. 474–490.
[571] Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q. Improving adversarial robustness
requires revisiting misclassified examples. In International conference on learning representations
(2019).
[572] Wang, Y., Zou, D., Yi, J., Bailey, J., Ma, X., and Gu, Q. Improving adversarial robustness
requires revisiting misclassified examples. In International Conference on Learning Representations
(2020).
[573] Wang, Z., Dong, X., Xue, H., Zhang, Z., Chiu, W., Wei, T., and Ren, K. Fairness-
aware adversarial perturbation towards bias mitigation for deployed deep models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2022),
pp. 10379–10388.
[574] Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M.,
and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on
Learning Representations (2022).
[575] Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le,
Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In
78


--- Page 79 ---
Advances in Neural Information Processing Systems (2022), A. H. Oh, A. Agarwal, D. Belgrave,
and K. Cho, Eds.
[576] Wei, Z., Wang, Y., Guo, Y., and Wang, Y. Cfa: Class-wise calibrated fair adversarial training.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2023),
pp. 8193–8201.
[577] Weiss, K., Khoshgoftaar, T. M., and Wang, D. A survey of transfer learning. Journal of
Big Data 3, 1 (2016), 9.
[578] White, A., Ngan, K. H., Phelan, J., Afgeh, S. S., Ryan, K., Reyes-Aldasoro, C. C.,
and d’Avila Garcez, A. Contrastive counterfactual visual explanations with overdetermination,
2021.
[579] Wiegreffe, S., Marasovi´c, A., and Smith, N. A.
Measuring association between labels
and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing (Online and Punta Cana, Dominican Republic, Nov. 2021), Association for
Computational Linguistics, pp. 10266–10284.
[580] Williams, P. N., and Li, K. Black-box sparse adversarial attack via multi-objective optimisa-
tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(2023), pp. 12291–12301.
[581] Wong, E., Schmidt, F. R., and Kolter, J. Z. Wasserstein adversarial examples via projected
sinkhorn iterations, 2019.
[582] Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes,
R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al. Robust fine-tuning of zero-shot
models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(2022), pp. 7959–7971.
[583] Wright, P. G. Tariff on animal and vegetable oils. Macmillan Company, New York, 1928.
[584] Wu, A., Kuang, K., Xiong, R., Zhu, M., Liu, Y., Li, B., Liu, F., Wang, Z., and Wu, F.
Learning instrumental variable from data fusion for treatment effect estimation, 2022.
[585] Wu, D., Xia, S.-T., and Wang, Y. Adversarial weight perturbation helps robust generalization.
Advances in Neural Information Processing Systems 33 (2020), 2958–2969.
[586] Wu, H., and Shi, X. Adversarial soft prompt tuning for cross-domain sentiment analysis. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers) (2022), pp. 2438–2447.
[587] Wu, J., Ouyang, L., Ziegler, D. M., Stiennon, N., Lowe, R., Leike, J., and Christiano,
P. Recursively summarizing books with human feedback, 2021.
[588] Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,
P., Rosenberg, D., and Mann, G. Bloomberggpt: A large language model for finance. arXiv
preprint arXiv:2303.17564 (2023).
[589] Wu, T., Ribeiro, M. T., Heer, J., and Weld, D. S. Polyjuice: Generating counterfactuals
for explaining, evaluating, and improving models, 2021.
[590] Wu, X., Mao, Y., Wang, H., Zeng, X., Gao, X., Xing, E. P., and Xu, M. Regularized
adversarial training (rat) for robust cellular electron cryo tomograms classification. In 2019 IEEE
International Conference on Bioinformatics and Biomedicine (BIBM) (2019), pp. 1–6.
[591] Wu, Y., Gardner, M., Stenetorp, P., and Dasigi, P. Generating data to mitigate spurious
correlations in natural language inference datasets. arXiv preprint arXiv:2203.12942 (2022).
[592] Wu, Y., Winston, E., Kaushik, D., and Lipton, Z. Domain adaptation with asymmetrically-
relaxed distribution alignment. In International conference on machine learning (2019), PMLR,
pp. 6872–6881.
[593] Wu,
Z.,
Xu,
H.,
Fang,
J.,
and Gao,
K.
Continual machine reading comprehen-
sion via uncertainty-aware fixed memory and adversarial domain adaptation.
arXiv preprint
arXiv:2208.05217 (2022).
[594] Xiao, Y., Tang, Z., Wei, P., Liu, C., and Lin, L. Masked images are counterfactual samples
79


--- Page 80 ---
for robust fine-tuning, 2023.
[595] Xie, S., Yang, T., Wang, X., and Lin, Y. Hyper-class augmented and regularized deep learning
for fine-grained image classification. In 2015 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2015), pp. 2645–2654.
[596] Xu, D., Wu, Y., Yuan, S., Zhang, L., and Wu, X. Achieving causal fairness through genera-
tive adversarial networks. In Proceedings of the Twenty-Eighth International Joint Conference on
Artificial Intelligence (2019).
[597] Xu, H., Caramanis, C., and Mannor, S. Robust regression and lasso, 2008.
[598] Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward:
Learning and evaluating human preferences for text-to-image generation, 2023.
[599] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., and
Bengio, Y. Show, attend and tell: Neural image caption generation with visual attention. In
International conference on machine learning (2015), PMLR, pp. 2048–2057.
[600] Xu, N., Wang, F., Li, B., Dong, M., and Chen, M.
Does your model classify entities
reasonably? diagnosing and mitigating spurious correlations in entity typing. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab
Emirates, Dec. 2022), Association for Computational Linguistics, pp. 8642–8658.
[601] Yang, C., and Ma, X.
Improving stability of fine-tuning pretrained language models via
component-wise gradient norm clipping. In Proceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec. 2022), Association
for Computational Linguistics, pp. 4854–4859.
[602] Yang, C.-H. H., Liu, Y.-C., Chen, P.-Y., Ma, X., and Tsai, Y.-C. J. When causal in-
tervention meets adversarial examples and image masking for deep neural networks. 2019 IEEE
International Conference on Image Processing (ICIP) (Sep 2019).
[603] Yang, F., Liu, N., Du, M., and Hu, X. Generative counterfactuals for neural networks via
attribute-informed perturbation, 2021.
[604] Yang, J., Soltan, A. A., Eyre, D. W., Yang, Y., and Clifton, D. A. An adversarial
training framework for mitigating algorithmic biases in clinical machine learning. NPJ Digital
Medicine 6, 1 (2023), 55.
[605] Yang, K., Ji, S., Zhang, T., Xie, Q., Kuang, Z., and Ananiadou, S. Towards interpretable
mental health analysis with chatgpt, 2023.
[606] Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang, J. Causalvae: Disentangled
representation learning via neural structural causal models.
In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition (2021), pp. 9593–9602.
[607] Yang, P., Chen, J., Hsieh, C.-J., Wang, J.-L., and Jordan, M. Ml-loo: Detecting ad-
versarial examples with feature attribution. In Proceedings of the AAAI Conference on Artificial
Intelligence (2020), vol. 34, pp. 6639–6647.
[608] Yang, X., Zhang, H., Qi, G., and Cai, J. Causal attention for vision-language tasks. 2021
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021), 9842–9852.
[609] Yang, Y., Kim, K. S., Kim, M., and Park, J. Gram: Fast fine-tuning of pre-trained language
models for content-based collaborative filtering. arXiv preprint arXiv:2204.04179 (2022).
[610] Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q. V.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural
Information Processing Systems (2019), H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc,
E. Fox, and R. Garnett, Eds., vol. 32, Curran Associates, Inc.
[611] Yi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., and Tenenbaum, J. B. Clevrer:
Collision events for video representation and reasoning, 2020.
[612] Yi, M., Hou, L., Sun, J., Shang, L., Jiang, X., Liu, Q., and Ma, Z.-M. Improved ood
generalization via adversarial training and pre-training, 2021.
[613] Yi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh,
80


--- Page 81 ---
A., Gabriel, R., and Hakkani-Tur, D. Towards coherent and engaging spoken dialog re-
sponse generation using automatic conversation evaluators. In Proceedings of the 12th Interna-
tional Conference on Natural Language Generation (Tokyo, Japan, Oct.–Nov. 2019), Association
for Computational Linguistics, pp. 65–75.
[614] Yosinski, J., Clune, J., Nguyen, A., Fuchs, T., and Lipson, H. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579 (2015).
[615] Yu, S., Jiang, J., Zhang, H., Niu, Y., Sun, Q., and Bing, L.
Interventional training
for out-of-distribution natural language understanding.
In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab Emirates, Dec.
2022), Association for Computational Linguistics, pp. 11627–11638.
[616] Yu, Y., Kong, L., Zhang, J., Zhang, R., and Zhang, C. Actune: Uncertainty-based active
self-training for active fine-tuning of pretrained language models. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (2022), pp. 1422–1436.
[617] Yu, Y., Xiong, C., Sun, S., Zhang, C., and Overwijk, A.
COCO-DR: Combating the
distribution shift in zero-shot dense retrieval with contrastive and distributionally robust learning.
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (Abu
Dhabi, United Arab Emirates, Dec. 2022), Association for Computational Linguistics, pp. 1462–
1479.
[618] Yuan, W., Neubig, G., and Liu, P. Bartscore: Evaluating generated text as text generation,
2021.
[619] Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F. Rrhf: Rank responses
to align language models with human feedback without tears, 2023.
[620] Yue, X., Zhang, Y., Zhao, S., Sangiovanni-Vincentelli, A., Keutzer, K., and Gong, B.
Domain randomization and pyramid consistency: Simulation-to-real generalization without access-
ing target domain data. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (2019), pp. 2100–2110.
[621] Yue, Z., Wang, T., Zhang, H., Sun, Q., and Hua, X.-S.
Counterfactual zero-shot and
open-set visual recognition, 2021.
[622] Yue, Z., Zeng, H., Kratzwald, B., Feuerriegel, S., and Wang, D. QA domain adaptation
using hidden space augmentation and self-supervised contrastive adaptation. In Proceedings of the
2022 Conference on Empirical Methods in Natural Language Processing (Abu Dhabi, United Arab
Emirates, Dec. 2022), Association for Computational Linguistics, pp. 2308–2321.
[623] Yue, Z., Zhang, H., Sun, Q., and Hua, X.-S. Interventional few-shot learning. In NeurIPS
(2020).
[624] Zaharia, G.-E., Sm˘adu, R.-A., Cercel, D.-C., and Dascalu, M. Domain adaptation in
multilingual and multi-domain monolingual settings for complex word identification. arXiv preprint
arXiv:2205.07283 (2022).
[625] Zaken, E. B., Ravfogel, S., and Goldberg, Y. Bitfit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. arXiv preprint arXiv:2106.10199 (2021).
[626] Zantedeschi, V., Nicolae, M.-I., and Rawat, A. Efficient defenses against adversarial at-
tacks. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security (2017),
ACM, pp. 39–49.
[627] Zeˇcevi´c, M., Willig, M., Dhami, D. S., and Kersting, K. Pearl causal hierarchy on image
data: Intricacies & challenges. arXiv preprint arXiv:2212.12570 (2022).
[628] Zeiler, M. D., and Fergus, R.
Visualizing and understanding convolutional networks.
In
European conference on computer vision (2014), Springer, pp. 818–833.
[629] Zemel, R., Wu, Y., Swersky, K., Pitassi, T., and Dwork, C. Learning fair representations.
In International conference on machine learning (2013), PMLR, pp. 325–333.
[630] Zhang, B. H., Lemoine, B., and Mitchell, M. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society (2018),
81


--- Page 82 ---
pp. 335–340.
[631] Zhang, D., Zhang, H., Tang, J., Hua, X., and Sun, Q. Causal intervention for weakly-
supervised semantic segmentation. ArXiv abs/2009.12547 (2020).
[632] Zhang, H., Liang, H., Zhang, Y., Zhan, L., Wu, X.-M., Lu, X., and Lam, A.
Fine-
tuning pre-trained language models for few-shot intent detection: Supervised pre-training and
isotropization. arXiv preprint arXiv:2205.07208 (2022).
[633] Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. Theoretically
principled trade-off between robustness and accuracy. arXiv preprint arXiv:1901.08573 (2019).
[634] Zhang, J., Huang, Y., Wu, W., and Lyu, M. R. Transferable adversarial attacks on vision
transformers with token gradient regularization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (2023), pp. 16415–16424.
[635] Zhang, K., Sch¨olkopf, B., Muandet, K., and Wang, Z. Domain adaptation under target
and conditional shift. In International Conference on Machine Learning (2013).
[636] Zhang, T., Min, W., Yang, J., Liu, T., Jiang, S., and Rui, Y.
What if we could not
see? counterfactual analysis for egocentric action anticipation. In Proceedings of the Thirtieth
International Joint Conference on Artificial Intelligence, IJCAI-21 (8 2021), Z.-H. Zhou, Ed.,
International Joint Conferences on Artificial Intelligence Organization, pp. 1316–1322. Main Track.
[637] Zhang, T., and Zhu, Z. Interpreting adversarially trained convolutional neural networks, 2019.
[638] Zhang, Y., Liu, T., Long, M., and Jordan, M. I. Bridging theory and algorithm for domain
adaptation. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
9-15 June 2019, Long Beach, California, USA (2019), K. Chaudhuri and R. Salakhutdinov, Eds.,
vol. 97 of Proceedings of Machine Learning Research, PMLR, pp. 7404–7413.
[639] Zhang, Y., Tio, P., Leonardis, A., and Tang, K. A survey on neural network interpretability.
IEEE Transactions on Emerging Topics in Computational Intelligence (2021).
[640] Zhang, Z., Li, J., Shi, N., Yuan, B., Liu, X., Zhang, R., Xue, H., Sun, D., and Zhang,
C. RoChBert: Towards robust BERT fine-tuning for Chinese. In Findings of the Association
for Computational Linguistics: EMNLP 2022 (Abu Dhabi, United Arab Emirates, Dec. 2022),
Association for Computational Linguistics, pp. 3502–3516.
[641] Zhang, Z., Lyu, L., Ma, X., Wang, C., and Sun, X. Fine-mixing: Mitigating backdoors in
fine-tuned language models. In Findings of the Association for Computational Linguistics: EMNLP
2022 (Abu Dhabi, United Arab Emirates, Dec. 2022), Association for Computational Linguistics,
pp. 355–372.
[642] Zhang, Z., Wu, S., Liu, S., Li, M., Zhou, M., and Xu, T. Regularizing neural machine
translation by target-bidirectional agreement, 2018.
[643] Zhang, Z., Zhao, Z., Lin, Z., He, X., et al. Counterfactual contrastive learning for weakly-
supervised vision-language grounding.
Advances in Neural Information Processing Systems 33
(2020), 18123–18134.
[644] Zhao, H., Combes, R. T. D., Zhang, K., and Gordon, G. On learning invariant represen-
tations for domain adaptation. In Proceedings of the 36th International Conference on Machine
Learning (09–15 Jun 2019), K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97 of Proceedings of
Machine Learning Research, PMLR, pp. 7523–7532.
[645] Zhao, H., et al. On learning invariant representations for domain adaptation. In ICML 2019
(2019), vol. 97 of Proceedings of Machine Learning Research, PMLR, pp. 7523–7532.
[646] Zhao, H., Ma, C., Dong, X., Luu, A. T., Deng, Z.-H., and Zhang, H. Certified robustness
against natural language attacks by causal intervention. In Proceedings of the 39th International
Conference on Machine Learning (17–23 Jul 2022), K. Chaudhuri, S. Jegelka, L. Song, C. Szepes-
vari, G. Niu, and S. Sabato, Eds., vol. 162 of Proceedings of Machine Learning Research, PMLR,
pp. 26958–26970.
[647] Zhao, J., Wang, T., Yatskar, M., Cotterell, R., Ordonez, V., and Chang, K.-W.
Gender bias in contextualized word embeddings, 2019.
[648] Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. Gender bias in coref-
82


--- Page 83 ---
erence resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 2 (Short Papers) (New Orleans, Louisiana, June 2018), Association for Com-
putational Linguistics, pp. 15–20.
[649] Zhao, Q., Adeli, E., and Pohl, K. M. Training confounder-free deep learning models for
medical applications. Nature communications 11, 1 (2020), 6010.
[650] Zhao, S., Gong, M., Liu, T., Fu, H., and Tao, D.
Domain generalization via entropy
regularization. Advances in Neural Information Processing Systems 33 (2020), 16096–16107.
[651] Zhao, W., Oyama, S., and Kurihara, M. Generating natural counterfactual visual explana-
tions. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence,
IJCAI-20 (7 2020), C. Bessiere, Ed., International Joint Conferences on Artificial Intelligence Or-
ganization, pp. 5204–5205. Doctoral Consortium.
[652] Zhao, Y. Fast real-time counterfactual explanations, 2020.
[653] Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P. J. Slic-hf: Sequence
likelihood calibration with human feedback, 2023.
[654] Zhao, Z., Dua, D., and Singh, S. Generating natural adversarial examples. arXiv preprint
arXiv:1710.11342 (2017).
[655] Zheng, S., Song, Y., Leung, T., and Goodfellow, I. Improving the robustness of deep
neural networks via stability training, 2016.
[656] Zhong, Z., Friedman, D., and Chen, D. Factual probing is [mask]: Learning vs. learning to
recall, 2021.
[657] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A. Learning deep features
for discriminative localization.
In Proceedings of the IEEE conference on computer vision and
pattern recognition (2016), pp. 2921–2929.
[658] Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu,
L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more
for alignment, 2023.
[659] Zhou, C., Ma, X., Michel, P., and Neubig, G. Examining and combating spurious features
under distribution shift, 2021.
[660] Zhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C.,
Bousquet, O., Le, Q. V., and Chi, E. H. Least-to-most prompting enables complex reasoning
in large language models. In The Eleventh International Conference on Learning Representations
(2023).
[661] Zhou, K., Yang, Y., Hospedales, T., and Xiang, T. Deep domain-adversarial image gener-
ation for domain generalisation. In Proceedings of the AAAI Conference on Artificial Intelligence
(2020), vol. 34, pp. 13025–13032.
[662] Zhou, W., and Xu, K. Learning to compare for better training and evaluation of open domain
natural language generation models. Proceedings of the AAAI Conference on Artificial Intelligence
34, 05 (Apr. 2020), 9717–9724.
[663] Zhou, Y., and Srikumar, V. A closer look at how fine-tuning changes bert. arXiv preprint
arXiv:2106.14282 (2021).
[664] Zhu, H., Zhu, Y., Zheng, H., Ren, Y., and Jiang, W. Ligaa: Generative adversarial attack
method based on low-frequency information. Computers & Security 125 (2023), 103057.
[665] Zhu, X., Su, W., Lu, L., Li, B., Wang, X., and Dai, J. Deformable {detr}: Deformable trans-
formers for end-to-end object detection. In International Conference on Learning Representations
(2021).
[666] Zhuang, F., Cheng, X., Luo, P., Pan, S. J., and He, Q. Supervised representation learning:
Transfer learning with deep autoencoders. In Twenty-Fourth International Joint Conference on
Artificial Intelligence (2015).
[667] Zhuang, H., Zhang, Y., and Liu, S. A pilot study of query-free adversarial attack against
83


--- Page 84 ---
stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (2023), pp. 2384–2391.
[668] Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Chris-
tiano, P., and Irving, G. Fine-tuning language models from human preferences, 2020.
[669] Zintgraf, L. M., Cohen, T. S., Adel, T., and Welling, M. Visualizing deep neural network
decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595 (2017).
[670] Zmigrod, R., Mielke, S. J., Wallach, H., and Cotterell, R. Counterfactual data aug-
mentation for mitigating gender stereotypes in languages with rich morphology. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics (Florence, Italy, July
2019), Association for Computational Linguistics, pp. 1651–1661.
[671] Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Gao, J., and Lee, Y. J. Segment everything
everywhere all at once, 2023.
[672] Zou, X., Yin, D., Zhong, Q., Ding, M., Yang, H., Yang, Z., and Tang, J. Controllable
generation from pre-trained language models via inverse prompting. In Proceedings of the 27th
ACM SIGKDD Conference on Knowledge Discovery & Data Mining (2021), p. 2450–2460.
[673] ´Alvaro Parafita, and Vitri`a, J. Explaining visual models by causal attribution, 2019.
84
