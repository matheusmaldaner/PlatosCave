{
  "nodes": [
    {
      "id": 0,
      "text": "Trustworthiness of machine learning can be improved from a data-centric perspective by identifying and controlling spurious or non-causal features in training data, using methods unified by causal reasoning",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "Many distinct trustworthy-ML methods (robustness, adversarial robustness, interpretability, fairness) converge to two master formulations: (A) domain-invariance adversarial objective and (B) worst-case/augmented empirical risk minimization; plus optional sample reweighting",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        7
      ]
    },
    {
      "id": 2,
      "text": "Pearl's causal hierarchy (associational L1, interventional L2, counterfactual L3) provides a unifying framework to categorize and explain these data-centric trustworthy methods",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 3,
      "text": "Data-centric view: dataset structure (spurious features, confounders, dataset bias) causes vanilla ERM-trained models to learn undesired signals, degrading real-world performance under distribution shifts",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        1,
        2,
        6
      ]
    },
    {
      "id": 4,
      "text": "Large pretrained models (fine-tuning, PEFT, prompting, RLHF) can be expressed as ERM variants and therefore are amenable to the same data-centric and causal interventions developed for standalone models",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 5,
      "text": "Stakeholder specifications (what shifts to be robust to, what counts as fair or interpretable) are necessary inputs to design and evaluate trustworthy ML methods",
      "role": "Assumption",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Master formulation A: adversarial domain-invariance (DANN style) trains representations invariant to confounder/domain IDs using a minimax with a domain discriminator",
      "role": "Claim",
      "parents": [
        1,
        3
      ],
      "children": [
        13
      ]
    },
    {
      "id": 7,
      "text": "Master formulation B: worst-case data augmentation and adversarial training minimizes loss under the worst perturbations or generated counterfactual samples (adversarial training, TRADES, consistency regularization)",
      "role": "Claim",
      "parents": [
        1,
        3
      ],
      "children": [
        13
      ]
    },
    {
      "id": 8,
      "text": "At causal level L2 (intervention): randomized controlled trials and data augmentation that randomize confounders emulate do-operations, removing confounding in P(y|x) and improving robustness",
      "role": "Claim",
      "parents": [
        2,
        3
      ],
      "children": [
        9
      ]
    },
    {
      "id": 9,
      "text": "Adjustment techniques from causality map to ML tools: backdoor adjustment corresponds to conditioning or marginalizing over confounder strata using learned covariates; inverse probability weighting corresponds to sample reweighting; front-door adjustment and instrumental variables provide alternatives when confounders are unobserved",
      "role": "Claim",
      "parents": [
        2,
        8
      ],
      "children": [
        13
      ]
    },
    {
      "id": 10,
      "text": "At causal level L3 (counterfactuals): generative and latent-variable models (GANs, VAEs, SCM-based generators) are used to produce counterfactual samples for augmentation, explanation, and estimating treatment effects",
      "role": "Claim",
      "parents": [
        2,
        3
      ],
      "children": [
        7,
        13
      ]
    },
    {
      "id": 11,
      "text": "Fine-tuning, prompting, and parameter-efficient fine-tuning can be formalized as ERM with constrained parameterization or input transforms, so causal/data-centric regularizations and augmentations can be integrated into these workflows",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Reinforcement learning from human feedback (RLHF) and reward-based alignment address human-preference alignment orthogonally to causal/statistical robustness and can be combined with causal interventions to improve overall trustworthiness",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Empirical and theoretical evidence: domain adversarial learning, adversarial training, representation alignment, counterfactual augmentation, weighting/group-DRO and causal adjustment methods have improved robustness, fairness, or interpretability across tasks and modalities",
      "role": "Evidence",
      "parents": [
        6,
        7,
        9,
        10
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Conclusion: a principled, causality-informed, data-centric paradigm—identify spurious/confounding structure, apply appropriate intervention/adjustment/augmentation or reweighting, and incorporate stakeholder constraints—offers a unifying path to more trustworthy ML including large pretrained models",
      "role": "Conclusion",
      "parents": [
        1,
        2,
        4,
        13,
        5
      ],
      "children": null
    }
  ]
}