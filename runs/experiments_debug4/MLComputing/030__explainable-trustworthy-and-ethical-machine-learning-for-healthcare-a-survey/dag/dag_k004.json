{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable, trustworthy, and ethical machine learning (ML/DL) is necessary for safe clinical deployment and adoption in healthcare",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "Deep learning (DL) methods achieve state-of-the-art performance across many healthcare tasks but are black-box and lack theoretical interpretability, limiting clinician and patient trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 2,
      "text": "Explainable ML (XML) techniques can provide human-understandable explanations of model decisions and increase transparency and trustworthiness",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "There are major methodological and practical challenges for effective XML in healthcare: lack of formal definitions, lack of standardized representations and requirements, inadequate validation metrics, lack of causality and theoretical understanding, and the accuracy-explainability trade-off",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12
      ]
    },
    {
      "id": 4,
      "text": "Security, privacy, and robustness vulnerabilities (including adversarial and privacy attacks, distribution shifts, and data imperfections) undermine trust and require specific defenses",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        13
      ]
    },
    {
      "id": 5,
      "text": "Ethical issues (data bias, privacy breaches, informed consent limits, and impacts on care relationships and accountability) arise from ML use in healthcare and must be addressed alongside explainability",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        14
      ]
    },
    {
      "id": 6,
      "text": "A development-to-deployment pipeline for explainable ML should include: explaining data, explaining model structure, explaining results, and measuring explanation effectiveness with human- and function-based evaluations",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 7,
      "text": "Principles and guidelines for trustworthy AI (OECD, EU HLEG) emphasize human-centered values, transparency, robustness, privacy, fairness, and accountability as necessary for healthcare AI",
      "role": "Context",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Empirical evidence shows DL models are vulnerable: adversarial attacks can add/remove medical evidence in CT scans and succeed against expert radiologists; adversarial examples exist for ECG, dermatology, and chest X-ray classifiers",
      "role": "Evidence",
      "parents": [
        1,
        4
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "DL complexity causes clinicians to distrust outputs and prevents meaningful informed consent when model rationale is not interpretable",
      "role": "Result",
      "parents": [
        1,
        5
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "XML taxonomies and methods include intrinsic (white-box) models, post-hoc explanations, model-specific and model-agnostic methods, surrogate, visualization, local vs global explanations",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Common XML techniques applied in healthcare include CAM/Grad-CAM, LRP, LIME, SHAP, DeepLIFT, integrated gradients, guided backprop, PDP, and interpretable architectures (e.g., HSCNN, Patient2Vec); these have been used to localize lesions, explain predictions, and rank features",
      "role": "Evidence",
      "parents": [
        2,
        6,
        10
      ],
      "children": [
        3,
        12
      ]
    },
    {
      "id": 12,
      "text": "Limitations of existing XML approaches: instability to input perturbations, dependence on model/data and reference points, lack of standardized evaluation metrics and ground truth for heatmaps, and suboptimal alignment with clinician expectations",
      "role": "Limitation",
      "parents": [
        3,
        11
      ],
      "children": [
        13,
        14
      ]
    },
    {
      "id": 13,
      "text": "Defenses and technical approaches to improve trustworthiness include: adversarial robustness (data/model/auxiliary defenses), privacy-preserving techniques (federated learning, differential privacy, homomorphic encryption), and methods to handle distribution shifts",
      "role": "Method",
      "parents": [
        4,
        12
      ],
      "children": [
        15
      ]
    },
    {
      "id": 14,
      "text": "Ethical safeguards required: dataset auditing and bias mitigation, privacy protection and clear consent procedures, accountability frameworks for errors, and human-centered design to preserve care and avoid harmful automation",
      "role": "Conclusion",
      "parents": [
        5,
        12
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Future research directions: explain medical data and remove bias, develop standardized and human-centered explanation representations, create generalized and robust explainable models, build adversarially-robust ML, and foster interdisciplinary development teams",
      "role": "Conclusion",
      "parents": [
        6,
        13,
        14
      ],
      "children": null
    }
  ]
}