{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable, trustworthy, and ethical machine learning is required for safe clinical deployment of ML/DL in healthcare to gain clinicians' and patients' trust and avoid life-threatening errors",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6
      ]
    },
    {
      "id": 1,
      "text": "There are major challenges to developing clinically effective explainable and trustworthy ML for healthcare including lack of formal definitions, standardized representations and requirements, validation metrics, theoretical understanding, causality, and ethical and security concerns",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 2,
      "text": "A pipeline for explainable ML in healthcare should include: data explanation, explaining black-box structure, explaining results (local/global), and measuring explanation effectiveness during development, testing, and deployment",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 3,
      "text": "Existing explainability techniques include intrinsic (white-box), post-hoc model-specific and model-agnostic methods and visualizations (e.g., decision trees, LIME, SHAP, CAM, LRP, Grad-CAM, DeepLIFT, PDP, IG), each with strengths and limitations for healthcare tasks",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 4,
      "text": "Ethical issues for ML in healthcare include biased and imbalanced data, privacy breaches, challenges for informed consent due to black-box models, accountability attribution, and impacts on care relationships",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "Robustness, security, and privacy are critical: adversarial attacks, model extraction, distribution shifts, and data imperfections undermine safety and trust and require defenses like adversarial training, differential privacy, federated learning, and cryptographic techniques",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "Conclusions: Explainability, trustworthiness, safety, robustness, and ethics must be integrated; current methods are promising but have limitations and open research problems remain",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Validation of explanations lacks standard metrics and ground truth; evaluation approaches include application-based, human-based, and function-based evaluations but no consensus exists for healthcare",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Lack of causality and theoretical understanding in DL limits meaningful clinical explanations and can produce spurious or incomplete explanations for cause-effect medical decisions",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Explainability should begin at data level to detect bias, leakage, and misrepresentation (example: pneumonia model misinterpreting asthma effect due to treatment confounder) then proceed to model- and result-level explanations",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Applications of XML in healthcare show utility: intrinsic models (DT, RF, GAM) enable transparency for some tasks, model-agnostic methods (LIME, SHAP, PDP, CAM, LRP, GradCAM) provide local/global explanations and have been applied to imaging, EHR, genomics, and time-series",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Data-related ethical evidence: imbalanced or non-diverse datasets and measurement proxies cause biased predictions (e.g., racial bias in health-cost proxy), threatening fairness and justice",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Empirical security evidence: demonstrated adversarial attacks on medical imaging, ECG, and CT reconstruction (e.g., CT-GAN tampering, adversarial examples fooling radiologists), showing real risk to clinical safety",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "There is a trade-off: adversarial robustness and interpretability often reduce standard accuracy, and vice versa; robust models learn different feature representations that may be more human-aligned but less accurate on standard metrics",
      "role": "Claim",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Limitations and pitfalls: explanations can be fragile to input changes, suboptimal or misaligned with clinician expectations, model- and data-dependent, and create accountability ambiguity",
      "role": "Limitation",
      "parents": [
        1
      ],
      "children": [
        6
      ]
    },
    {
      "id": 15,
      "text": "Future directions: explain and de-bias medical data, develop standardized representation and evaluation of explanations, build generalized and adversarially robust explainable models, and form interdisciplinary development teams for clinical translation",
      "role": "Claim",
      "parents": [
        2,
        3,
        4,
        5
      ],
      "children": [
        6
      ]
    }
  ]
}