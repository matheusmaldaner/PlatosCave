{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable, trustworthy, and ethical machine learning (ML) is necessary to enable safe clinical deployment of ML and deep learning (DL) in healthcare",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4
      ]
    },
    {
      "id": 1,
      "text": "DL models achieve state-of-the-art performance on many healthcare tasks but are black-box and lack theoretical foundations, reducing clinician and patient trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 2,
      "text": "Explainable ML (XML) methods can increase trust, transparency, and clinical acceptability of ML/DL decisions by providing human-understandable explanations",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 3,
      "text": "Security, safety, robustness, and ethical issues (e.g., adversarial attacks, privacy breaches, bias, lack of causality) hinder ML trustworthiness in healthcare",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 4,
      "text": "There exist gaps: lack of formal definitions, standardized representations, validation metrics for explanations, and guidelines specific to medical XML",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "Because DL models are complex and non-linear, there is an accuracy versus explainability trade-off; inherently interpretable models may be less accurate on complex medical data",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "ML explanations should address data issues first (data bias, contamination, leakage) because models learn latent patterns from data that can mislead decisions",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 7,
      "text": "XML methods taxonomy: intrinsic (interpretable models), post-hoc, model-specific vs model-agnostic, surrogate, and visualization methods; explanations can be local or global",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        13
      ]
    },
    {
      "id": 8,
      "text": "Effective clinical XML pipeline includes: explain data, explain model structure, explain results, and measure explanation effectiveness via application-, human-, or function-based evaluation",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        11,
        13
      ]
    },
    {
      "id": 9,
      "text": "Adversarial attacks and privacy attacks have been demonstrated on medical imaging and signals, showing real clinical risk and need for adversarially robust and privacy-preserving ML",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 10,
      "text": "Lack of causality in DL produces inadequate explanations for cause-effect clinical questions; XML should support causal reasoning for fair decisions",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Evaluation and standardization gaps: no widely accepted metrics to compare explanations, no medical-domain requirement guidelines, and explanations may be unintelligible to clinicians or patients",
      "role": "Limitation",
      "parents": [
        4,
        6,
        8
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Robust, generalized, and clinically useful explanations require (a) data interrogation to expose bias, (b) representation techniques tailored to clinicians, and (c) interdisciplinary workforce involvement",
      "role": "Conclusion",
      "parents": [
        5,
        6
      ],
      "children": [
        14
      ]
    },
    {
      "id": 13,
      "text": "Applied XML techniques for healthcare include white-box models (decision trees, GAMs), model-agnostic methods (LIME, SHAP, PDP, CAM, LRP, IG), visualization and surrogate methods, and hybrid interpretable architectures",
      "role": "Result",
      "parents": [
        7,
        8
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Clinical validation and case studies demonstrate that combining robust data practices, explainable methods, and security/privacy measures improves clinical acceptability and safety of ML systems",
      "role": "Evidence",
      "parents": [
        9,
        12,
        13
      ],
      "children": null
    }
  ]
}