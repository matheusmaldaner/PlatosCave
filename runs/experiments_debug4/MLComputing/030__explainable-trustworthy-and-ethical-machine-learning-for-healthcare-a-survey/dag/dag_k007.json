{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable, trustworthy, and ethical machine learning (XML) is necessary to enable safe clinical adoption of ML/DL in healthcare by increasing transparency, clinician and patient trust, and addressing safety, robustness, fairness, privacy, and ethical concerns",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "Deep learning models achieve state-of-the-art performance in many healthcare tasks but are black-box, complex, and lack theoretical foundations, which hinders clinician trust and clinical deployment",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 2,
      "text": "The paper proposes a pipeline for explainable ML in healthcare covering data explanation, model-structure explanation, result explanation, and measures for evaluating explanation effectiveness during development, testing, and deployment",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "Taxonomy of XML methods: intrinsic (inherently interpretable), post-hoc, model-specific vs model-agnostic, surrogate, visualization; and local vs global explainability distinctions",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 4,
      "text": "Explainability methods applied in healthcare include visualization/saliency (e.g., CAM, Grad-CAM, saliency maps), attribution (LRP, DeepLIFT, Integrated Gradients), model-agnostic tools (LIME, SHAP, PDP), and interpretable architectures (decision trees, GAM, HSCNN, Patient2Vec)",
      "role": "Evidence",
      "parents": [
        0,
        3
      ],
      "children": [
        12
      ]
    },
    {
      "id": 5,
      "text": "XML can help identify data issues (bias, leakage), select better models, facilitate clinician engagement, and make individualized and global explanations usable for clinical decision making",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "The literature documents real security and robustness threats to medical ML: adversarial attacks can alter scans or signals to add/remove disease, model extraction and privacy attacks can leak patient data, and distributional shifts reduce deployed performance",
      "role": "Evidence",
      "parents": [
        0,
        1
      ],
      "children": [
        13
      ]
    },
    {
      "id": 7,
      "text": "Ethical challenges of ML in healthcare include data bias and imbalance, privacy and data governance, informed consent difficulties due to black-box models, accountability attribution, and potential harms to patient dignity and equity",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        13,
        14
      ]
    },
    {
      "id": 8,
      "text": "DL black-box nature limits causal reasoning: deep models learn associations but typically cannot reliably infer cause-effect needed for many clinical decisions",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        7
      ]
    },
    {
      "id": 9,
      "text": "Clinically-relevant evaluation of explanations is underdeveloped: human-, application-, and function-based evaluation approaches exist but no standardized metrics or ground truth for comparing explanation methods in healthcare",
      "role": "Limitation",
      "parents": [
        1,
        2
      ],
      "children": [
        12,
        14
      ]
    },
    {
      "id": 10,
      "text": "Data-level explanation is a prerequisite: interrogating data biases, contamination, representational issues and provenance is essential before model development to avoid learning misleading patterns (example: pneumonia model and asthma confounder)",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        5
      ]
    },
    {
      "id": 11,
      "text": "Explainability should be tailored in scope and representation: choose local vs global explanations and domain-adapted representations (textual rules, visual heatmaps, patient-specific summaries) appropriate for clinicians and patients",
      "role": "Claim",
      "parents": [
        2,
        3
      ],
      "children": [
        4,
        9
      ]
    },
    {
      "id": 12,
      "text": "Applied examples show XML utility: CAM/Grad-CAM, LRP, LIME, SHAP, DeepLIFT and interpretable architectures have been used to explain fracture detection, tumor grading, ECG classification, Alzheimer classification, diabetic nephropathy and genomic findings",
      "role": "Evidence",
      "parents": [
        4,
        5,
        9
      ],
      "children": [
        14
      ]
    },
    {
      "id": 13,
      "text": "Building trustworthy ML requires addressing adversarial robustness, privacy-preserving techniques (federated learning, differential privacy, homomorphic encryption), distribution shift robustness, and security across the ML pipeline",
      "role": "Claim",
      "parents": [
        6,
        7
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "There are trade-offs: adversarial robustness and interpretability often come at a cost to standard accuracy; designers must balance accuracy, explainability, and robustness depending on clinical priorities",
      "role": "Conclusion",
      "parents": [
        9,
        12,
        13
      ],
      "children": null
    }
  ]
}