{
  "nodes": [
    {
      "id": 0,
      "text": "Explainable, trustworthy, and ethical machine learning is necessary for safe clinical adoption of ML/DL in healthcare",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4
      ]
    },
    {
      "id": 1,
      "text": "Deep learning models achieve state-of-the-art performance in many healthcare tasks but are black-box and lack interpretability, hindering clinician and patient trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 2,
      "text": "Explainable ML (XML) techniques, combined with safety, robustness, privacy and ethical practices, can build trustworthiness and address clinical, legal, and ethical barriers",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        8,
        9
      ]
    },
    {
      "id": 3,
      "text": "There exist practical and conceptual challenges to XML in healthcare including lack of formal definitions, standard representations, validation metrics, causality, and clinician-centered formats",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 4,
      "text": "A pipeline for explainable ML in healthcare should include data explanation, model-structure explanation, result explanation, and effectiveness measurement during development and deployment",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        5,
        7,
        11
      ]
    },
    {
      "id": 5,
      "text": "Data-level issues (bias, leakage, imbalance, distribution shifts, heterogeneity) cause misleading model behavior and must be interrogated first via data explanation methods",
      "role": "Claim",
      "parents": [
        1,
        4
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "Existing post-hoc explanation methods include saliency maps, gradient-based attributions (IG, LRP, Deep Taylor), LIME, SHAP, CAM/Grad-CAM, DeepLIFT and surrogate models; each has modality and reliability trade-offs",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        11
      ]
    },
    {
      "id": 7,
      "text": "Trustworthy ML requires addressing security (adversarial attacks, model extraction), privacy (differential privacy, federated learning, cryptography), and robustness to distribution shifts",
      "role": "Claim",
      "parents": [
        2,
        4
      ],
      "children": [
        13
      ]
    },
    {
      "id": 8,
      "text": "OECD and EU principles emphasize human-centered values, transparency, robustness, fairness, privacy, and accountability as foundations for trustworthy AI in healthcare",
      "role": "Context",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Explainability supports ethical requirements such as informed consent, auditability, fairness, non-maleficence and can mitigate harms from biased datasets and opaque decision making",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 10,
      "text": "Validation of explanations lacks standardized metrics and human-grounded, application-based, and function-based evaluation approaches are proposed but remain incomplete",
      "role": "Limitation",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Model-agnostic and intrinsic strategies (interpretable models, surrogate models, visualization, hybrid grey-box) can be chosen based on trade-offs between accuracy, interpretability, and robustness",
      "role": "Claim",
      "parents": [
        4,
        6
      ],
      "children": [
        5,
        12
      ]
    },
    {
      "id": 12,
      "text": "Empirical evidence shows adversarial attacks and data issues can alter medical images, ECGs, and other modalities causing clinically significant misdiagnoses and misleading explanations",
      "role": "Evidence",
      "parents": [
        5,
        7,
        11
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "There is a trade-off: models trained for adversarial robustness often lose some standard accuracy but can yield representations more aligned with human perception and interpretability",
      "role": "Result",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Ethical deployment requires interdisciplinary development, patient-centered explanation formats, privacy safeguards, clear accountability, and rigorous clinical evaluation",
      "role": "Conclusion",
      "parents": [
        9,
        8,
        10
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Open research directions include robust generalized explanations, explanation representation design for clinicians, adversarially robust ML, data explanation techniques, and interdisciplinary teams",
      "role": "Claim",
      "parents": [
        14
      ],
      "children": null
    }
  ]
}