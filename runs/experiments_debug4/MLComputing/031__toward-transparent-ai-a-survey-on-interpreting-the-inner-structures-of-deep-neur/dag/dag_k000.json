{
  "nodes": [
    {
      "id": 0,
      "text": "Inner interpretability methods that analyze internal components of deep neural networks can improve transparency, diagnostics, debugging, and trustworthiness of deployed AI systems",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "A taxonomy organizing inner interpretability methods by the network component they explain (weights, neurons, subnetworks, latent representations) and by intrinsic versus post hoc implementation clarifies the field and guides method selection",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        7
      ]
    },
    {
      "id": 2,
      "text": "Inner interpretability methods provide practical benefits: open-ended evaluation, failure analysis, bug fixing, accountability, basic scientific understanding, and enabling 'microscope' AI and reverse engineering",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 3,
      "text": "Interpretability research connects productively with other ML areas—adversarial robustness, continual learning, modularity, network compression, and human-vision modeling—and progress depends on exploiting these connections",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 4,
      "text": "Current interpretability research often produces hypotheses rather than validated conclusions; many methods lack rigorous evaluation, quantification of uncertainty, and scalable benchmarks",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 5,
      "text": "Future work should emphasize diagnostics, debugging, adversaries, benchmarking, scalable human oversight, and combining intrinsic and post hoc methods to make interpretability tools practically useful to engineers",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Weights can be interpreted via intrinsic continual learning (specializing weights per task) or post hoc weight-masking to identify task-relevant subnetworks, but many weights are frivolous and pruning shows many weights are unnecessary",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        13
      ]
    },
    {
      "id": 7,
      "text": "Individual neuron analysis uses dataset-based dissection, feature synthesis, perturbation/ablation, and gradient-based attribution, but faces hazards like polysemantic and frivolous neurons and dataset/label limitations",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        13
      ]
    },
    {
      "id": 8,
      "text": "Interpretability enables open-ended evaluation that can reveal dataset biases, harmful behaviors, deceptive solutions, and failure modes not captured by test-set performance",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Interpretability can directly support fixing models by identifying and editing weights, neurons, or subnetworks tied to harmful biases or failure modes and by guiding adversarial training or fine-tuning",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Empirical and theoretical results show bidirectional relationships: more interpretable DNNs tend to be more robust to adversaries and adversarially robust models often yield more interpretable representations and better transfer",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Many interpretability methods scale poorly: successes are often limited to small models or toy tasks and require extensive expert labor, limiting practical applicability to large deployed systems",
      "role": "Limitation",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Common evaluation shortcomings include treating plausible visualizations as conclusions, cherry-picking best examples, insufficient controls in probing, and lack of benchmarks that measure usefulness to engineers",
      "role": "Limitation",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Subnetworks and representations can be analyzed via sparsity, modularity, modular partitioning, circuits analysis, disentanglement, tokens/attention, concept vectors, probing, and representation comparison, each with specific strengths and hazards",
      "role": "Claim",
      "parents": [
        6,
        7
      ],
      "children": [
        10,
        11
      ]
    }
  ]
}