{
  "nodes": [
    {
      "id": 0,
      "text": "A systematic empirical comparison of commonly used feature selection methods will provide guidance to select appropriate methods for text classification and improve classification accuracy in research and applications",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3
      ]
    },
    {
      "id": 1,
      "text": "There is a gap in recent literature: few studies provide comprehensive, systematic comparisons of typical feature selection methods for text classification",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        2
      ]
    },
    {
      "id": 2,
      "text": "We summarize 20 typical feature selection methods (supervised and unsupervised) and perform comparative experiments on four benchmark datasets using a decision tree classifier with 10-fold cross validation and classification accuracy as metric",
      "role": "Method",
      "parents": [
        0,
        1
      ],
      "children": [
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        15
      ]
    },
    {
      "id": 3,
      "text": "Feature selection reduces noisy terms in high-dimensional text data by selecting informative terms, thereby improving text classification performance",
      "role": "Context",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "The study groups methods into supervised (use class information, e.g., IG, ECE, MI, CHI, OR, GI, IGI, MOR, MC-OR) and unsupervised (use frequency statistics, e.g., TF, DF, TV, TVQ, MM, AMGM, MAD)",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Datasets: four UCI-derived benchmark datasets covering diverse domains â€” CARR (car reviews), COMD (company descriptions), IMDB (movie reviews), and KDCN (Turkish news)",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Evaluation protocol: decision tree classifier, classification accuracy as performance measure, varying numbers of selected terms, 10-fold cross validation to obtain robust results",
      "role": "Method",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Experimental result: MOR and MC-OR achieve the best classification accuracy among the evaluated feature selection methods",
      "role": "Result",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 8,
      "text": "Experimental result: Improved Gini Index (IGI) performs closely after MOR and MC-OR, offering similar accuracy with a simpler formula",
      "role": "Result",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 9,
      "text": "Experimental result: Unsupervised frequency-based methods TV, TVQ, TF and DF achieve relatively good performance overall and provide higher efficiency; TVQ is best on the IMDB dataset",
      "role": "Result",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 10,
      "text": "Experimental result: Three versions of Chi-square (CHI) yield relatively good and similar classification performance",
      "role": "Result",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Experimental result: Expected cross entropy (ECE) performs well in many cases but underperforms when only a small number of terms (e.g., 20 or 50) are selected",
      "role": "Result",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Experimental result: AMGM, MM and MAD show unstable performance across the four datasets, with particularly poor results for AMGM on KDCN and poor MM and MAD on CARR",
      "role": "Result",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Experimental result: Information Gain (IG), Gini Index (GI) and Mutual Information (MI) achieve the worst accuracies overall; the maximum-version of MI outperforms the average-version",
      "role": "Result",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Conclusion and recommendation: For maximum accuracy use MOR or MC-OR (or IGI as a simpler alternative); when computational efficiency matters, prefer TV, TVQ, TF or DF; selection should balance accuracy and computational cost",
      "role": "Conclusion",
      "parents": [
        7,
        8,
        9
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Limitation: Experiments are limited to four UCI-based datasets and a single classifier (decision tree), so generalizability across other datasets, languages, or classifiers may be constrained",
      "role": "Limitation",
      "parents": [
        2
      ],
      "children": null
    }
  ]
}