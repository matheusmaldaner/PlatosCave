{
  "nodes": [
    {
      "id": 0,
      "text": "Modifying random forest can improve prediction of the minority (rare) class in extremely imbalanced classification problems",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "Two specific modifications are proposed to make random forest effective on imbalanced data: Balanced Random Forest (BRF) and Weighted Random Forest (WRF)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        2,
        3,
        11,
        12,
        13
      ]
    },
    {
      "id": 2,
      "text": "Balanced Random Forest (BRF): for each tree draw a bootstrap sample of the minority class and an equal-size bootstrap sample from the majority class, grow an unpruned CART tree with random feature selection, and aggregate by majority vote",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        6,
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "Weighted Random Forest (WRF): assign larger class weight to minority class, incorporate class weights into Gini split criterion and terminal node weighted majority vote, and select weights using out-of-bag accuracy",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        6,
        10,
        11,
        13
      ]
    },
    {
      "id": 4,
      "text": "Random forest is an ensemble of unpruned classification trees built from bootstrap samples with random feature selection; standard RF optimizes overall error and is biased toward majority class on imbalanced data",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        2,
        3
      ]
    },
    {
      "id": 5,
      "text": "Experimental evaluation used six previously studied highly imbalanced datasets, multiple performance metrics (true positive rate, true negative rate, precision, recall, F-measure, G-mean, weighted accuracy), ROC analysis and ten-fold cross validation",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8,
        9,
        10,
        15
      ]
    },
    {
      "id": 6,
      "text": "On the oil spill dataset BRF and WRF (with tuned cutoff or weight) outperform SHRINK and other published methods in G-mean, weighted accuracy and F-measure; WRF weight equal to class proportion achieves best reported numbers",
      "role": "Evidence",
      "parents": [
        2,
        3,
        5
      ],
      "children": [
        11
      ]
    },
    {
      "id": 7,
      "text": "On the mammography dataset WRF improves over SMOTE and SMOTEBoost on F-measure, G-mean and weighted accuracy; BRF improves over SMOTE and has mixed comparison versus SMOTEBoost",
      "role": "Evidence",
      "parents": [
        2,
        3,
        5
      ],
      "children": [
        11
      ]
    },
    {
      "id": 8,
      "text": "On the satimage dataset BRF and WRF are superior to SMOTE and standard RIPPER in F-measure and G-mean, and outperform SMOTEBoost in G-mean but not in F-measure",
      "role": "Evidence",
      "parents": [
        2,
        3,
        5
      ],
      "children": [
        11
      ]
    },
    {
      "id": 9,
      "text": "On hypothyroid and euthyroid datasets BRF and WRF outperform SHRINK, C4.5 and 1-NN in G-mean and weighted accuracy",
      "role": "Evidence",
      "parents": [
        2,
        3,
        5
      ],
      "children": [
        11
      ]
    },
    {
      "id": 10,
      "text": "ROC analyses across datasets show BRF and WRF have very similar ROC curves; sometimes WRF slightly better, sometimes BRF slightly better, with no consistent winner",
      "role": "Result",
      "parents": [
        2,
        3,
        5
      ],
      "children": [
        11,
        14
      ]
    },
    {
      "id": 11,
      "text": "Both Balanced RF and Weighted RF improve minority-class prediction and have favorable performance compared to many existing imbalance methods across multiple datasets and metrics",
      "role": "Conclusion",
      "parents": [
        1,
        6,
        7,
        8,
        9,
        10
      ],
      "children": [
        12,
        13,
        14
      ]
    },
    {
      "id": 12,
      "text": "BRF is computationally more efficient on large imbalanced datasets because each tree uses only a small subsample of the majority class",
      "role": "Claim",
      "parents": [
        1,
        11
      ],
      "children": [
        14
      ]
    },
    {
      "id": 13,
      "text": "WRF requires using the entire training set and, by weighting the minority class, may be more vulnerable to label noise or mislabeled majority cases than BRF",
      "role": "Claim",
      "parents": [
        1,
        3,
        11
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Limitations: there is no clear universal winner between BRF and WRF; further study is needed to compare robustness under label noise and to tune class weights or cutoffs for application-specific tradeoffs",
      "role": "Limitation",
      "parents": [
        10,
        12,
        13,
        11
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Performance metrics were computed from confusion matrices and evaluated with ten-fold cross validation; ROC curves were generated by varying BRF vote cutoff or WRF class weight",
      "role": "Method",
      "parents": [
        5
      ],
      "children": [
        6,
        7,
        8,
        9,
        10
      ]
    }
  ]
}