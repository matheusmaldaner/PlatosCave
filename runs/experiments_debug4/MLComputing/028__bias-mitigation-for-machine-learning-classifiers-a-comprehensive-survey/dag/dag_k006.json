{
  "nodes": [
    {
      "id": 0,
      "text": "A comprehensive survey can characterize bias mitigation methods for machine learning classifiers and their empirical evaluation practices to support researchers and practitioners",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ]
    },
    {
      "id": 1,
      "text": "We performed a systematic literature search and snowballing, selecting 341 publications on bias mitigation for tabular classification; data sources included preliminary surveys, six repositories, snowballing and author feedback",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 2,
      "text": "Bias mitigation methods cluster into three intervention stages: pre-processing (modify training data), in-processing (modify learning), and post-processing (modify model or outputs)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 3,
      "text": "The survey identified the dataset landscape: 83 unique datasets used, but usage is concentrated (Adult used by 77% of empirical papers, COMPAS 51%); on average papers evaluate on 2.7 datasets and 59% of datasets were used only once",
      "role": "Result",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "The survey identified a diverse set of fairness metrics: 109 unique metrics grouped into six categories (based on labels, predicted outcome, predicted+actual outcome, predicted probabilities, similarity, causal reasoning); the most used are statistical parity/disparate impact and equality of opportunity/equalized odds",
      "role": "Result",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Benchmarking practices vary: common baselines are the original (unmitigated) model (used by 82% of experiments), suppression of sensitive attribute, and random baselines; 137 surveyed methods have been used as benchmarks and 56% of papers shared source code",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 6,
      "text": "We extracted practical challenges from the literature: (1) lack of consensus on fairness definitions and many metrics, (2) need for formal fairness guarantees, (3) dataset limitations and sensitive attribute availability, (4) uncertain transferability to real-world settings, (5) limited experimental breadth in many studies",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "From the aggregated evidence we recommend best practices for evaluating bias mitigation: check existing work, test on at least three diverse datasets, state protected attributes, use at least two fairness metrics plus a performance metric, benchmark against original model and similar methods, evaluate across multiple classifiers, repeat experiments (e.g., 10 runs), and share code and results",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Overall conclusion: in-processing methods are the most prevalent, evaluation practices are fragmented, and consolidating datasets, metrics, benchmarks and sharing code would improve reproducibility and applicability",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Search details and selection: preliminary collection from four prior surveys yielded 100 papers; two repository searches (Oct 2021, Jul 2022), snowballing, and author feedback produced the final 341 publications",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Counts and taxonomy evidence: among 341 publications 123 used pre-processing methods, 212 used in-processing methods, 56 used post-processing methods; methods were further categorized into 13 approach categories (e.g., relabelling, sampling, representation, regularization, constraints, adversarial)",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Benchmarking practices detail: 308 of 324 empirical papers applied benchmarking; many papers benchmark only against original model (254/308); pre-processing methods less frequently benchmark against other mitigation methods and 51 papers benchmarked against fairness-unaware methods; code availability increased over time with 192/341 papers sharing implementations",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    }
  ]
}