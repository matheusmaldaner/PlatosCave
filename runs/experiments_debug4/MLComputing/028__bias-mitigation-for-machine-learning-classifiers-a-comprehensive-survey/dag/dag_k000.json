{
  "nodes": [
    {
      "id": 0,
      "text": "A comprehensive survey of bias mitigation methods for machine learning classifiers can identify how methods are evaluated (datasets, metrics, benchmarking) and provide actionable guidance to practitioners for developing and evaluating new bias mitigation methods",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        11,
        12
      ]
    },
    {
      "id": 1,
      "text": "We performed a systematic literature search combining a preliminary collection from existing surveys, repository searches (IEEE, ACM, ScienceDirect, Scopus, arXiv, Google Scholar), backward snowballing, and author feedback to assemble candidate publications",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        13
      ]
    },
    {
      "id": 2,
      "text": "We collected and analyzed 341 publications about bias mitigation for ML classifiers (focused on classification with tabular data)",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        14
      ]
    },
    {
      "id": 3,
      "text": "Bias mitigation methods are categorized by intervention stage (pre-processing, in-processing, post-processing) and by technique into 13 approach categories",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        15
      ]
    },
    {
      "id": 4,
      "text": "Surveyed papers use a limited set of datasets for evaluation: 83 unique datasets identified; most publications use few datasets (average 2.7 per study)",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 5,
      "text": "Fairness is measured with many, diverse metrics: 109 unique fairness metrics were found and organized into six categories (labels, predicted outcomes, predicted+actual outcomes, predicted probabilities, similarity, causal reasoning)",
      "role": "Result",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 6,
      "text": "Benchmarking practices vary: most studies compare to the original (fairness-agnostic) model and many benchmark against prior bias-mitigation methods; some use fairness-unaware baselines and a portion perform no benchmarking",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 7,
      "text": "Source code availability is partial: 192 of 341 publications (56%) provided code or implementations, enabling some reproducibility and reuse",
      "role": "Result",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Dataset specifics as evidence: Adult dataset used most often (249 of 324 empirical papers, 77%), COMPAS and German follow; 59% of datasets used only once and 90% of publications use four or fewer datasets",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Metric usage evidence: most papers use one or two fairness metrics on average; Statistical Parity Difference, Disparate Impact (p-rule), Equality of Opportunity and Equalized Odds are among the most frequently applied metrics",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Benchmarking evidence: of 308 benchmarked experiments, 254 compared to the original model baseline (about 82%); pre-/in-/post-processing methods tend to benchmark most often against methods of the same type",
      "role": "Evidence",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Key challenges identified: (1) proliferation and selection of fairness metrics complicates evaluation; (2) limited and criticized benchmark datasets and lack of protected-attribute availability; (3) lack of fairness guarantees and limited transfer to real-world deployments; (4) evaluation practices often use few datasets/models and inconsistent data-splits",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Practical recommendations distilled from the survey: check prior work; evaluate on at least three diverse datasets; state protected attributes; report at least two fairness metrics plus a performance metric; benchmark against the original model and relevant prior methods; test on multiple classifiers when applicable; repeat experiments (suggested 10 runs); and share code and numeric results",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Selection and inclusion criteria: included works that describe human biases, address classification problems, and use tabular data; three-stage manual screening (title, abstract, full body) and repeated snowballing were applied",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Distribution evidence: in-processing methods are most common (212 papers), followed by pre-processing (123) and post-processing (56); publications increased steeply from 2018 onward and are concentrated in AI and data venues",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Combined approaches evidence: 70 publications applied multiple mitigation techniques (e.g., pre+in, in+post), showing techniques are often combined and that in-processing methods frequently co-occur with other types",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    }
  ]
}