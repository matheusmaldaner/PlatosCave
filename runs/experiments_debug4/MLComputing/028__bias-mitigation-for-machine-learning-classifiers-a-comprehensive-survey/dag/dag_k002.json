{
  "nodes": [
    {
      "id": 0,
      "text": "A comprehensive survey of bias mitigation methods for machine learning classifiers and their empirical evaluation supports practitioners in selecting and evaluating fairness interventions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        3,
        5,
        9,
        10,
        11,
        12
      ]
    },
    {
      "id": 1,
      "text": "Systematic literature search and selection methodology: preliminary search, repository searches (IEEE, ACM, ScienceDirect, Scopus, arXiv, Google Scholar) and snowballing, plus author feedback, yielding 341 selected publications",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2
      ]
    },
    {
      "id": 2,
      "text": "Search stages produced 100 papers from prior surveys, two repository searches (Oct 2021 and Jul 2022), snowballing 78, author feedback 32, total 341",
      "role": "Evidence",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "Taxonomy of bias mitigation methods: categorize methods by intervention stage (pre-processing, in-processing, post-processing) and by 13 technique categories",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4
      ]
    },
    {
      "id": 4,
      "text": "Distribution of method types across 341 publications: 123 pre-processing, 212 in-processing, 56 post-processing; 13 technique categories illustrated (e.g., sampling, relabelling, representation, regularization, constraints, adversarial)",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Survey of empirical evaluation practices: datasets, fairness metrics, benchmarking and reproducibility across the collected studies",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8
      ]
    },
    {
      "id": 6,
      "text": "Dataset usage evidence: 83 unique datasets identified; Adult dataset used by 77% of experimental studies, COMPAS by 51%; average of 2.7 datasets used per paper and 59% of datasets used only once",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Fairness metrics evidence: 109 unique fairness metrics used, grouped into six categories (based on dataset labels, predicted outcome, predicted and actual outcomes, predicted probabilities, similarity, causal reasoning); most used metrics include Statistical Parity Difference, Equality of Opportunity, and Equalized Odds",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Benchmarking and reproducibility evidence: common baselines include original (unmitigated) model (used by ~82%), suppressing protected attribute, random; many studies benchmark against prior bias-mitigation methods; 56% (192/341) of papers provide source code",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Observed gaps and methodological opportunities: in-processing methods are most common (largest body of work), post-processing input-correction is rare (only two papers), combinations of pre/in/post methods are underexplored and present opportunities",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Recommended best practices distilled from the survey: evaluate on at least three diverse datasets, state protected attributes, use at least two fairness metrics plus a performance metric, benchmark against original model and relevant prior methods, test multiple classifiers (esp. for pre/post-processing), repeat experiments (ideally 10 runs), and share code and numeric results",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Scope limitation: survey focuses on algorithmic fairness for classification on tabular data and does not cover domain-specific non-tabular methods or broader interdisciplinary aspects (law, health) in depth",
      "role": "Limitation",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Identified challenges from the literature: lack of consensus on fairness definitions and metrics, need for fairness guarantees, dataset limitations (protected attribute availability and representativeness), transferability to real-world applications, and need to extend experiments (more metrics, models, datasets)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    }
  ]
}