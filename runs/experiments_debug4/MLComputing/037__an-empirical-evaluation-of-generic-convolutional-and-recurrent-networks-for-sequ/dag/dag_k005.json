{
  "nodes": [
    {
      "id": 0,
      "text": "Temporal convolutional networks (TCNs) provide a better generic starting architecture than canonical recurrent networks (LSTM/GRU) for sequence modeling across diverse tasks",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "TCN architecture: 1D fully-convolutional network with causal convolutions, dilated convolutions (exponential dilations), residual blocks, ReLU, weight normalization, and spatial dropout",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 2,
      "text": "Systematic empirical evaluation comparing a single generic TCN instantiation to canonical RNNs (LSTM, GRU, vanilla RNN) on a comprehensive suite of synthetic stress tests and real datasets using comparable model sizes and standard regularization",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "Main empirical claim: The generic TCN convincingly outperforms canonical recurrent architectures across a broad range of sequence modeling tasks and datasets",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 4,
      "text": "TCNs exhibit substantially longer effective memory in practice compared to RNNs of comparable capacity, despite RNNs' theoretical infinite-memory property",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "Practical advantages of TCNs include parallelism during training/evaluation, flexible receptive field control (depth, dilation, kernel size), more stable gradients, lower training memory for long sequences, and support for variable-length inputs",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Practical disadvantages of TCNs include higher data storage requirements during evaluation (need raw history up to receptive field) and need to change receptive-field parameters when transferring to domains with different memory requirements",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Conclusion: The association of sequence modeling with RNNs should be reconsidered; convolutional networks (TCNs) are a simpler, clearer, and often more accurate natural starting point for sequence tasks",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Synthetic stress tests: On adding, copy-memory, sequential MNIST, and permuted MNIST, TCNs converged faster and achieved substantially better accuracy or lower loss than LSTM, GRU, and vanilla RNN in matched-size comparisons",
      "role": "Evidence",
      "parents": [
        1,
        2,
        3
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Polyphonic music modeling: On JSB Chorales and Nottingham, the generic TCN outperformed canonical recurrent models and some enhanced RNN variants in negative log-likelihood with minimal tuning",
      "role": "Evidence",
      "parents": [
        1,
        2,
        3
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Language modeling: TCN outperformed GRU and vanilla RNN on character- and word-level tasks and, without heavy tuning, outperformed some LSTM baselines on large corpora (WikiText-103, LAMBADA) though optimized LSTMs can still beat TCN on smaller PTB when heavily tuned",
      "role": "Evidence",
      "parents": [
        1,
        2,
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Memory experiments: On the copy memory stress task TCNs achieved near 100% accuracy across sequence lengths where LSTM and GRU degenerated to random guessing; on LAMBADA TCN achieved substantially lower perplexity than canonical LSTM baselines, supporting longer effective memory",
      "role": "Evidence",
      "parents": [
        1,
        2,
        4
      ],
      "children": null
    }
  ]
}