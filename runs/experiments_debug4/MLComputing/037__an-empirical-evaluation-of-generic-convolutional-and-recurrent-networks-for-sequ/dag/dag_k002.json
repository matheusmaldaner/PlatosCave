{
  "nodes": [
    {
      "id": 0,
      "text": "Temporal convolutional networks (TCNs) can serve as a more effective default architecture than canonical recurrent networks (LSTMs/GRUs) for a broad range of sequence modeling tasks",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        3,
        4,
        7,
        12
      ]
    },
    {
      "id": 1,
      "text": "Define a generic TCN architecture: 1D fully-convolutional network with causal convolutions, dilated convolutions, residual blocks, ReLU, weight normalization, and spatial dropout",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2,
        9,
        10,
        11
      ]
    },
    {
      "id": 2,
      "text": "Use exponentially increasing dilation factors and residual connections to achieve very large effective receptive fields (long effective history) while keeping networks deep but trainable",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        7,
        11
      ]
    },
    {
      "id": 3,
      "text": "Systematic empirical evaluation protocol: compare the same generic TCN to canonical RNNs (LSTM, GRU, vanilla RNN) across many standard synthetic stress tests and real datasets, keeping model sizes similar and using minimal architecture-specific tuning",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        4,
        5,
        6,
        13
      ]
    },
    {
      "id": 4,
      "text": "Across the evaluated tasks, the generic TCN consistently outperforms or matches canonical recurrent architectures with similar capacity",
      "role": "Result",
      "parents": [
        3,
        1
      ],
      "children": [
        5,
        6,
        7,
        12
      ]
    },
    {
      "id": 5,
      "text": "Synthetic stress-test evidence: on adding problem, copy memory, sequential MNIST and permuted MNIST, TCN converges faster and attains substantially better accuracy or lower loss than LSTM/GRU/RNN baselines of similar size",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": [
        8
      ]
    },
    {
      "id": 6,
      "text": "Real-data evidence: on polyphonic music (JSB Chorales, Nottingham), character-level and word-level language modeling (PTB, text8, WikiText-103, LAMBADA) TCNs with minimal tuning outperform canonical recurrent baselines on multiple metrics (NLL, bpc, perplexity) and on large-context datasets yield substantial gains",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": []
    },
    {
      "id": 7,
      "text": "In practice TCNs exhibit substantially longer effective memory than canonical recurrent networks of similar capacity, despite RNNs having theoretical infinite memory",
      "role": "Claim",
      "parents": [
        0,
        2,
        4
      ],
      "children": [
        8,
        12
      ]
    },
    {
      "id": 8,
      "text": "Copy memory task evidence for memory: TCN achieves 100% accuracy for long sequence lengths while LSTM and GRU degenerate to random guessing as sequence length increases",
      "role": "Evidence",
      "parents": [
        5,
        7
      ],
      "children": []
    },
    {
      "id": 9,
      "text": "Practical advantages of TCNs: parallelism for training/evaluation, flexible receptive field control (via depth, dilation, filter size), more stable gradients (backpropagation path not tied to time), and lower training memory footprint compared to gated RNNs",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        12
      ]
    },
    {
      "id": 10,
      "text": "Notable limitations of TCNs: at evaluation time they may require storing raw inputs up to the receptive field (higher evaluation memory), and transferring to domains requiring larger history may need parameter changes (filter size, dilation, depth)",
      "role": "Limitation",
      "parents": [
        1
      ],
      "children": [
        12
      ]
    },
    {
      "id": 11,
      "text": "Implementation details used in experiments: causal 1D FCN with zero padding, dilation d = 2^i per layer i, residual blocks containing two dilated causal convolutions, optional 1x1 conv for dimension matching, weight normalization, spatial dropout, Adam optimizer for TCN, and comparable hyperparameter search for RNN baselines",
      "role": "Method",
      "parents": [
        1,
        3
      ],
      "children": [
        4,
        5,
        6,
        13
      ]
    },
    {
      "id": 12,
      "text": "Conclusion: Given empirical superiority on diverse benchmarks, simpler design, and practical advantages, convolutional TCNs should be reconsidered as a natural and often preferable starting point for sequence modeling instead of defaulting to recurrent networks",
      "role": "Conclusion",
      "parents": [
        0,
        4,
        7,
        9,
        10
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Experimental consistency evidence: the same generic TCN architecture with modest adjustments to depth and kernel size was applied across tasks and still outperformed canonical recurrent baselines without extensive task-specific architectural changes",
      "role": "Evidence",
      "parents": [
        3,
        11
      ],
      "children": [
        4,
        12
      ]
    }
  ]
}