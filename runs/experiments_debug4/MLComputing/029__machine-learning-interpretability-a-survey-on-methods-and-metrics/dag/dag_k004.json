{
  "nodes": [
    {
      "id": 0,
      "text": "Machine learning interpretability is essential for societal trust, auditability, fairness, safety, and regulatory compliance, and the field needs standardized methods and metrics to evaluate explanation quality",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
      ]
    },
    {
      "id": 1,
      "text": "ML systems have become ubiquitous and more complex (eg deep residual networks) leading to higher predictive power but increased opacity that impacts high-stakes domains",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        2
      ]
    },
    {
      "id": 2,
      "text": "Societal and regulatory pressure (eg DARPA XAI, EU guidelines, GDPR) demand explainability, transparency, accountability, and the ability to audit algorithmic decisions",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        3
      ]
    },
    {
      "id": 3,
      "text": "Lack of interpretability causes harms: biased or incorrect decisions in criminal justice, healthcare, finance, and other high-impact applications, and prevents meaningful recourse and auditing",
      "role": "Claim",
      "parents": [
        0,
        1,
        2
      ],
      "children": [
        4
      ]
    },
    {
      "id": 4,
      "text": "Explainable AI (XAI) emerged to produce interpretable models and post hoc explanation methods that preserve predictive performance while providing human-understandable explanations",
      "role": "Context",
      "parents": [
        0,
        1,
        2,
        3
      ],
      "children": [
        5,
        8
      ]
    },
    {
      "id": 5,
      "text": "Interpretability is subjective and domain-specific, so methods and metrics must consider audience, use case, and context rather than a single universal definition",
      "role": "Assumption",
      "parents": [
        0
      ],
      "children": [
        6,
        7
      ]
    },
    {
      "id": 6,
      "text": "There are conceptual taxonomies for interpretability: timing (pre-model, in-model, post-model), nature (intrinsic vs post hoc), scope (global vs local), and model dependence (model-specific vs model-agnostic)",
      "role": "Claim",
      "parents": [
        0,
        5
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 7,
      "text": "Interpretability methods produce different kinds of explanation outputs: feature summaries (importance, partial dependence), model internals, data-point examples or prototypes, and surrogate interpretable models",
      "role": "Claim",
      "parents": [
        0,
        6
      ],
      "children": [
        8
      ]
    },
    {
      "id": 8,
      "text": "Properties useful to characterize explanation methods and explanations include expressive power, translucency, portability, complexity, and explanation properties such as fidelity, accuracy, stability, consistency, comprehensibility, certainty, novelty, representativeness, and importance",
      "role": "Claim",
      "parents": [
        0,
        4,
        6,
        7
      ],
      "children": [
        9
      ]
    },
    {
      "id": 9,
      "text": "Human-friendly explanation characteristics include contrastiveness (counterfactuals), selectivity (few causes), social context adaptation, focus on abnormal causes, truthfulness, consistency with prior beliefs, and generality",
      "role": "Claim",
      "parents": [
        0,
        5,
        8
      ],
      "children": [
        10
      ]
    },
    {
      "id": 10,
      "text": "Common explanation algorithms and families include intrinsic interpretable models (linear models, decision trees, rule sets, scorecards), model-specific methods for DNNs (integrated gradients, Grad-CAM, Guided backprop, TCAV), and model-agnostic post hoc methods (LIME, SHAP/Shapley values, partial dependence, anchors, influence functions, prototypes/criticisms)",
      "role": "Method",
      "parents": [
        0,
        7
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 11,
      "text": "Evaluation approaches are categorized into three levels: application-grounded evaluation with domain experts on real tasks, human-grounded evaluation with simpler tasks and lay users, and functionally-grounded evaluation using proxy metrics without humans",
      "role": "Method",
      "parents": [
        0,
        8,
        9
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Several quantitative axioms and proxy metrics have been proposed to assess explanation quality, including sensitivity and implementation invariance (integrated gradients), identity/separability/stability (explanation consistency), and completeness/correctness/compactness (the three Cs)",
      "role": "Evidence",
      "parents": [
        0,
        8,
        11
      ],
      "children": [
        13
      ]
    },
    {
      "id": 13,
      "text": "Existing assessment work is limited: few papers focus on rigorous, comparable metrics and many methods lack stability, fidelity, or human validation across domains",
      "role": "Result",
      "parents": [
        0,
        12
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Interdisciplinary research (data science, cognitive psychology, HCI, philosophy) is necessary to develop explanations that align technical explainability with human causability and usable interfaces",
      "role": "Claim",
      "parents": [
        0,
        5,
        9
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Conclusion: The field should prioritize developing contextualized, evidence-based evaluation frameworks and a model-agnostic system capable of recommending the best explanation for a given domain, use case, and audience",
      "role": "Conclusion",
      "parents": [
        0,
        13,
        14
      ],
      "children": null
    }
  ]
}