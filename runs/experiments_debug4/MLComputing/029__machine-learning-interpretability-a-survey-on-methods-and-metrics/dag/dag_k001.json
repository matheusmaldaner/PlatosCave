{
  "nodes": [
    {
      "id": 0,
      "text": "There is no consensus on how to assess quality of machine learning explanations; a comprehensive review of interpretability methods, metrics, societal impact, and future directions is needed",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4
      ]
    },
    {
      "id": 1,
      "text": "ML systems are increasingly ubiquitous, more complex, and used in high-impact domains, increasing societal need for interpretability and auditability",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 2,
      "text": "Explainable Artiﬁcial Intelligence (XAI) emerged to create interpretable models and explanation methods that preserve predictive performance while enabling understanding and trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 3,
      "text": "Interpretability is required in many applications because accuracy alone is an incomplete description of real-world tasks; it enables verification, fairness, safety, causality, robustness, privacy, and trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        9
      ]
    },
    {
      "id": 4,
      "text": "Main obstacles: interpretability is subjective, domain specific, lacks formal definition, and there is no agreed set of metrics to compare explanation methods",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 5,
      "text": "Regulatory and public bodies demand transparency and explicability (examples: GDPR right to explanation, EU and national AI strategies, DARPA XAI), motivating research and industry focus on interpretability",
      "role": "Evidence",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "High-stakes and biased outcomes (e.g., COMPAS, incorrect bail or lending decisions) motivate interpretability to detect bias, audit proprietary systems, and provide recourse",
      "role": "Evidence",
      "parents": [
        1,
        3
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Taxonomy of interpretability: pre-model (data), in-model (intrinsic/transparent), post-model (post hoc explanations); model-specific vs model-agnostic; global vs local scope; types of explanation outputs",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 8,
      "text": "XAI research includes model-speciﬁc techniques (e.g., neural network saliency, integrated gradients, Grad-CAM, TCAV) and model-agnostic methods (e.g., LIME, SHAP, partial dependence, counterfactuals, prototypes)",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        13
      ]
    },
    {
      "id": 9,
      "text": "Interpretability desiderata that explanations help to assess: fairness, privacy, reliability/robustness, causality/causability, and trust",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Properties to evaluate explanation methods and explanations include expressive power, translucency, portability, algorithmic complexity, fidelity, accuracy, consistency, stability, comprehensibility, certainty, importance, novelty, and representativeness",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": [
        14
      ]
    },
    {
      "id": 11,
      "text": "Human-centered explanation properties: contrastiveness, selectivity, social context, focus on abnormal causes, truthfulness, consistency with prior beliefs, and generality",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Interpretable models (intrinsic) include linear/logistic regression, decision trees, rule sets and scorecards; interpretability can be increased via constraints (sparsity, monotonicity) or human-in-the-loop priors",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Model-agnostic post-hoc methods produce feature summaries, surrogate interpretable models, example-based explanations, or counterfactuals and are reusable across model classes but may sacrifice model-specific insights",
      "role": "Claim",
      "parents": [
        7,
        8
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Evaluation approaches: three levels — application-grounded (task with domain experts), human-grounded (simpler human-subject studies), and functionally-grounded (proxy metrics without humans); axiom-based quantitative proxies exist (sensitivity, implementation invariance, identity, separability, stability, completeness, correctness, compactness)",
      "role": "Method",
      "parents": [
        10
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Conclusion: progress exists in methods and awareness but the field needs standardized, context-aware assessment frameworks and metrics and interdisciplinary human-centered research to recommend best explanations per domain and audience",
      "role": "Conclusion",
      "parents": [
        14,
        11,
        13,
        9
      ],
      "children": null
    }
  ]
}