{
  "nodes": [
    {
      "id": 0,
      "text": "A comprehensive review of machine learning interpretability, focusing on societal impact, methods, and metrics, can clarify the field and identify future research directions including how to assess explanation quality",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        11
      ]
    },
    {
      "id": 1,
      "text": "Machine learning systems are increasingly ubiquitous and powerful, producing complex black-box models whose decisions have growing societal impact",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        7
      ]
    },
    {
      "id": 2,
      "text": "Interpretability is needed because black-box models hide internal logic, which is problematic for high-stakes decisions, fairness, trust, safety, and regulatory requirements such as GDPR",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 3,
      "text": "The interpretability research field (XAI) has re-emerged recently but lacks consensus on definitions, terminology, and standardized assessment of explanation quality",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 4,
      "text": "Interpretability methods and models are taxonomized along several axes: pre-model/in-model/post-model, intrinsic (transparent) vs post hoc, model-specific vs model-agnostic, and by scope (algorithm, global, local) and explanation result (feature summary, model internals, data-point, surrogate)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 5,
      "text": "Intrinsically interpretable models (e.g., linear models, decision trees, rule lists, scorecards) provide modular/global interpretability but may trade off predictive performance or handle interactions poorly",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Post hoc explanation methods include model-specific techniques (e.g., integrated gradients, Grad-CAM) and model-agnostic techniques (e.g., LIME, Shapley values, partial dependence, counterfactuals) that produce feature attributions, surrogates, or examples",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Societal and industry actors (governments, DARPA, EU bodies, major companies) increasingly demand explainability, funding XAI research and producing guidelines emphasizing transparency, accountability, and trustworthy AI",
      "role": "Evidence",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Interpretability supports desiderata beyond accuracy: fairness, privacy, robustness, causality/causability, and trust, enabling auditing, debugging, and detection of biased or unsafe behavior",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Regulation and legal context, exemplified by GDPR and policy reports, create requirements for retraceable and explainable automated decisions, posing technical and definition challenges about when and what explanations are required",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Interpretability is subjective and domain-specific; multiple valid explanations may exist (Rashomon effect), and explanation utility depends on audience, context, and goals",
      "role": "Assumption",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Existing work proposes properties and proxies to evaluate explanations (accuracy, fidelity, consistency, stability, comprehensibility, novelty, representativeness) and experimental evaluation levels: application-grounded, human-grounded, and functionally grounded",
      "role": "Claim",
      "parents": [
        0,
        3
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 12,
      "text": "Axiomatic and proxy evaluation proposals include sensitivity and implementation invariance (for attributions), identity/separability/stability (explanation consistency), and the three Cs (completeness, correctness, compactness) as measurable indicators",
      "role": "Method",
      "parents": [
        11
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Despite many explanation methods and some proposed proxies, there is little research on systematic, context-aware assessment frameworks and on human-centered experimental validation across domains",
      "role": "Claim",
      "parents": [
        11
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Interdisciplinary collaboration (data science, human sciences, HCI, cognitive psychology, ethics) is necessary to develop human-friendly explanations and rigorous evaluation methods aligned with domain needs",
      "role": "Conclusion",
      "parents": [
        13
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "A recommended long-term goal is a model-agnostic, context-aware framework that can recommend or evaluate the best explanations for a given domain, use case, and user type; current gap motivates future work",
      "role": "Conclusion",
      "parents": [
        0,
        14
      ],
      "children": null
    }
  ]
}