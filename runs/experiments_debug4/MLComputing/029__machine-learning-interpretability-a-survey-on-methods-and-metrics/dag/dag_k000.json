{
  "nodes": [
    {
      "id": 0,
      "text": "A systematic review of machine learning interpretability is needed to survey methods and metrics and emphasize societal impact, and to identify directions for future research",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "ML systems are increasingly ubiquitous, more complex, and have growing societal impact, creating demand for interpretability",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 2,
      "text": "Opaque black-box models (eg ensembles, deep neural networks) prevent humans from understanding and verifying predictions, motivating Explainable AI (XAI)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 3,
      "text": "Regulation and public/industry awareness (eg GDPR, DARPA XAI, EU guidelines) increase demand for transparent, auditable algorithmic decisions",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 4,
      "text": "Interpretability research spans data science, human sciences, and human-computer interaction and requires interdisciplinary collaboration",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "There is no single definition of interpretability; related terms (interpretability, explainability, transparency) are used variably and depend on context and audience",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "Interpretability methods can be organized by when and how they operate: pre-model, in-model (intrinsic), post-model (post hoc); and by model-speciﬁc vs model-agnostic, by scope (global vs local), and by explanation output (feature summaries, model internals, examples, surrogate models)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        13,
        14
      ]
    },
    {
      "id": 7,
      "text": "The research field has produced many interpretability methods but lacks consensus on how to assess explanation quality; more work on metrics and contextualized evaluation is required",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": [
        15
      ]
    },
    {
      "id": 8,
      "text": "High-stakes application examples (healthcare, criminal justice, finance) show errors and biased outcomes that motivate interpretability to ensure fairness, reliability, causality, privacy, and trust",
      "role": "Evidence",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "XAI aims to produce interpretable models or explanations for black boxes while preserving predictive performance",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Legal and policy documents (GDPR, EU Ethics Guidelines) require explainability, traceability, and accountability, implying technical and practical challenges for providing suitable explanations",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Interdisciplinary inputs are necessary because interpretability requires both technical explainability and human-centered, HCI and cognitive considerations to produce useful explanations",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Interpretability is subjective, domain-specific, and audience-dependent; pragmatic, audience-centered explanations are often more useful than single 'correct' formal explanations",
      "role": "Claim",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Taxonomy detail: intrinsic interpretable models include linear models, decision trees, rule lists and scorecards; post hoc model-agnostic methods include feature importance, partial dependence, LIME, SHAP, anchors, counterfactuals, prototypes and inﬂuence functions",
      "role": "Method",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Explanation properties and human-friendly characteristics include fidelity, accuracy, stability, consistency, comprehensibility, contrastiveness, selectivity, focus on abnormal, and social/contextual adaptation",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Evaluation approaches: three levels are proposed—application-grounded (task experts, highest validity), human-grounded (simpler human studies), and functionally-grounded (proxy metrics); existing quantitative proxies include axioms (sensitivity, implementation invariance, identity, separability, stability) and the three Cs (completeness, correctness, compactness)",
      "role": "Result",
      "parents": [
        7
      ],
      "children": null
    }
  ]
}