{
  "nodes": [
    {
      "id": 0,
      "text": "Interpretable machine learning is necessary for societally impactful and regulated applications and the field needs rigorous assessment methods and metrics to evaluate explanations",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        9,
        11
      ]
    },
    {
      "id": 1,
      "text": "Machine learning systems are increasingly ubiquitous, more complex, and achieve high predictive performance across domains",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        2,
        4
      ]
    },
    {
      "id": 2,
      "text": "Many high-performing ML models are black boxes whose internal logic is opaque to users and experts, preventing verification and understanding of individual decisions",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        3,
        4,
        5
      ]
    },
    {
      "id": 3,
      "text": "Societal, legal, and regulatory pressures (e.g., GDPR, public policy reports, national strategies) increase demand for explainability, transparency, auditability, and trust in ML decisions",
      "role": "Context",
      "parents": [
        0,
        1,
        2
      ],
      "children": [
        11
      ]
    },
    {
      "id": 4,
      "text": "Explainable AI (XAI) emerged to produce interpretable models and post hoc explanation methods that preserve predictive performance while enabling human understanding",
      "role": "Claim",
      "parents": [
        1,
        2
      ],
      "children": [
        6,
        7,
        8
      ]
    },
    {
      "id": 5,
      "text": "Opaque ML decisions have caused harms (biased sentencing, incorrect bail/parole, faulty models in health and environment) that motivate the need for explanations to detect bias, ensure fairness, and enable recourse",
      "role": "Evidence",
      "parents": [
        2,
        3
      ],
      "children": [
        11
      ]
    },
    {
      "id": 6,
      "text": "Interpretability methods are taxonomized by timing (pre-model, in-model, post-model), intrinsic vs post hoc, model-specific vs model-agnostic, and by the type and scope of explanations (global vs local)",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": [
        7,
        8,
        9
      ]
    },
    {
      "id": 7,
      "text": "Intrinsic (in-model) interpretability uses interpretable models or constraints (e.g., linear models, decision trees, sparse rules, monotonicity) to provide transparency as model internals",
      "role": "Method",
      "parents": [
        4,
        6
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Post hoc, model-agnostic and model-specific explanation methods generate feature summaries, surrogate models, example-based explanations, or visualizations to explain predictions after training",
      "role": "Method",
      "parents": [
        4,
        6
      ],
      "children": [
        10
      ]
    },
    {
      "id": 9,
      "text": "Explanations should be evaluated with human-centered goals (accuracy/fidelity, understandability/comprehensibility, and efficiency) and aligned with audience and domain context",
      "role": "Claim",
      "parents": [
        0,
        6
      ],
      "children": [
        11
      ]
    },
    {
      "id": 10,
      "text": "Representative explanation methods include partial dependence, individual conditional expectations, accumulated local effects, LIME, SHAP (Shapley values), anchors, counterfactual explanations, prototypes/criticisms, and influence functions",
      "role": "Context",
      "parents": [
        8
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Existing evaluation approaches propose three levels: application-grounded (end task with domain experts), human-grounded (simpler human studies), and functionally grounded (proxy metrics without humans)",
      "role": "Method",
      "parents": [
        0,
        3,
        5,
        9
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 12,
      "text": "Quantitative and axiomatic proxies for explanation quality include sensitivity and implementation invariance (for attributions), identity, separability, stability (consistency axioms), and the three Cs: completeness, correctness, and compactness",
      "role": "Evidence",
      "parents": [
        11
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Human-friendly explanation properties (contrastiveness, selectivity, social context, focus on abnormal cases, plausibility/truthfulness, and alignment with prior beliefs) shape what explanations are useful in practice",
      "role": "Claim",
      "parents": [
        11,
        9
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Current research has produced many methods but comparatively little work rigorously comparing explanation quality across methods, domains, and users using standardized metrics",
      "role": "Claim",
      "parents": [
        6,
        10,
        11,
        12
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "A priority research direction is developing contextualized, model-agnostic assessment frameworks and evidence-based metrics that recommend best explanations given domain, use case, and audience constraints",
      "role": "Conclusion",
      "parents": [
        0,
        14,
        12,
        13
      ],
      "children": null
    }
  ]
}