{
  "nodes": [
    {
      "id": 0,
      "text": "A comprehensive review of machine learning interpretability methods and metrics, focused on societal impact and assessment, will clarify the field and identify future research directions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "Machine learning systems are increasingly ubiquitous and more complex, producing high-impact decisions in diverse domains which raises demand for interpretability",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6
      ]
    },
    {
      "id": 2,
      "text": "Explainable AI (XAI) emerged to address black-box opacity by developing interpretable models and post-hoc explanation methods while preserving predictive performance",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        7
      ]
    },
    {
      "id": 3,
      "text": "Interpretability methods can be taxonomized along multiple criteria: pre-model/in-model/post-model, intrinsic vs post-hoc, model-specific vs model-agnostic, global vs local, and by explanation result types (feature summaries, model internals, example-based, surrogate models)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        4
      ]
    },
    {
      "id": 4,
      "text": "Human-centered properties and desiderata of explanations include fidelity, accuracy, stability, consistency, comprehensibility, contrastiveness, selectivity, focus on abnormal cases, and alignment with prior beliefs; explanations are social and pragmatic",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 5,
      "text": "Interpretability evaluation should be performed at three levels: application-grounded (end task with domain experts), human-grounded (simpler human-subject tasks), and functionally-grounded (proxy metrics without humans)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 6,
      "text": "Societal drivers—high-stakes errors, regulatory requirements (e.g., GDPR), public and industry awareness, and risks of bias and discrimination—create practical need for interpretable and auditable ML systems",
      "role": "Claim",
      "parents": [
        1,
        2
      ],
      "children": [
        11
      ]
    },
    {
      "id": 7,
      "text": "There exist many interpretable-model families (linear models, decision trees, rule sets, scorecards) and many post-hoc explanation methods (partial dependence, PDP, ICE, accumulated local effects, LIME, SHAP, integrated gradients, Grad-CAM, TCAV, influence functions, prototypes/criticisms, counterfactuals, anchors, model distillation)",
      "role": "Method",
      "parents": [
        2,
        3
      ],
      "children": [
        8
      ]
    },
    {
      "id": 8,
      "text": "Model-specific methods (e.g., integrated gradients, Grad-CAM, guided backprop) leverage internals of particular model classes, while model-agnostic methods (e.g., LIME, SHAP, surrogate models, feature importance) apply post-hoc to any model with trade-offs in portability, translucency, and algorithmic complexity",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": [
        9
      ]
    },
    {
      "id": 9,
      "text": "Proposed quantitative evaluation proxies and axioms include sensitivity and implementation invariance (integrated gradients), and identity, separability, stability and explanation consistency; other numeric proxies: completeness, correctness, compactness and representativeness",
      "role": "Evidence",
      "parents": [
        4,
        5,
        8
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 10,
      "text": "Key challenges remain: lack of consensus on definitions and metrics, interpretability subjectivity and domain-specificity, trade-offs among accuracy/understandability/efficiency, instability from randomized methods, and difficulty of measuring human comprehensibility",
      "role": "Limitation",
      "parents": [
        4,
        5,
        9
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Conclusion: the field needs more contextualized, evidence-based assessment of explanation quality, standardized quantitative proxies validated by human-grounded and application-grounded studies, interdisciplinary research, and development of a model-agnostic framework to recommend appropriate explanations per domain and audience",
      "role": "Conclusion",
      "parents": [
        6,
        9,
        10
      ],
      "children": null
    }
  ]
}