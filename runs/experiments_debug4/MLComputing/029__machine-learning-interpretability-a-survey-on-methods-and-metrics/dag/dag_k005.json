{
  "nodes": [
    {
      "id": 0,
      "text": "There is no consensus on how to assess the quality of machine learning explanations; this survey reviews the state of research on interpretability methods and metrics, focusing on societal impact, methods, metrics, and future directions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        10,
        12
      ]
    },
    {
      "id": 1,
      "text": "Machine learning systems are increasingly ubiquitous, more powerful, and more complex, leading to greater societal impact when used in high-stakes domains",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        2,
        4
      ]
    },
    {
      "id": 2,
      "text": "Many high-performing models are black boxes (ensembles, deep neural networks) whose internal logic is opaque, motivating demand for interpretability and explainability",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        3,
        4,
        5
      ]
    },
    {
      "id": 3,
      "text": "Regulatory, public, and industry actors (e.g., GDPR, DARPA XAI, EU bodies, Google, IBM) have increased awareness and requirements for transparency, accountability, and explainability",
      "role": "Evidence",
      "parents": [
        0,
        1,
        2
      ],
      "children": [
        4,
        5
      ]
    },
    {
      "id": 4,
      "text": "Interpretability serves multiple desiderata beyond accuracy: fairness, privacy, reliability/robustness, causality, trust; explanations help detect bias, debug models, and satisfy legal/social requirements",
      "role": "Claim",
      "parents": [
        0,
        2,
        3
      ],
      "children": [
        5,
        6,
        9
      ]
    },
    {
      "id": 5,
      "text": "Interpretability is domain- and audience-specific and subjective, so there is no single formal definition or one-size-fits-all explanation",
      "role": "Assumption",
      "parents": [
        0,
        2,
        4
      ],
      "children": [
        6,
        9,
        12
      ]
    },
    {
      "id": 6,
      "text": "Interpretability methods and models can be taxonomized by several criteria: pre-model/in-model/post-model; intrinsic (transparent) vs post hoc; model-specific vs model-agnostic; global vs local scope; and by explanation outputs (feature summaries, model internals, example-based, surrogate models)",
      "role": "Claim",
      "parents": [
        0,
        4,
        5
      ],
      "children": [
        7,
        8,
        9
      ]
    },
    {
      "id": 7,
      "text": "Intrinsic interpretable models include linear/logistic regression, decision trees, rule sets and scorecards; constraints like sparsity or monotonicity increase transparency but may trade off predictive power",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": [
        9
      ]
    },
    {
      "id": 8,
      "text": "Post hoc explanation methods include model-specific techniques for DNNs (integrated gradients, Grad-CAM, guided backprop) and model-agnostic methods (LIME, SHAP, partial dependence, counterfactuals, prototypes, influence functions)",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 9,
      "text": "Properties to evaluate explanations include fidelity/accuracy, consistency, stability, comprehensibility, certainty, importance, novelty, and representativeness; human-friendly qualities include contrastiveness, selectivity, social context, focus on abnormal, truthfulness, and alignment with prior beliefs",
      "role": "Claim",
      "parents": [
        4,
        5,
        6,
        7,
        8
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 10,
      "text": "Evaluations can be structured at three levels: application-grounded (real end-users, highest validity, costly), human-grounded (simpler tasks with lay users), and functionally grounded (proxy metrics, no humans, lower validity)",
      "role": "Method",
      "parents": [
        0,
        9,
        8
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 11,
      "text": "Existing quantitative/proxy metrics and axioms for explanations include integrated-gradients axioms (sensitivity, implementation invariance), identity/separability/stability axioms for consistency, and the three Cs (completeness, correctness, compactness) for rule/example explanations",
      "role": "Evidence",
      "parents": [
        9,
        10
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Despite many explanation methods and some proposed proxies, the literature shows limited work on systematic, contextualized assessment and comparison of explanation methods across domains and audiences",
      "role": "Claim",
      "parents": [
        0,
        5,
        8,
        11
      ],
      "children": [
        13,
        14
      ]
    },
    {
      "id": 13,
      "text": "The interpretability research field needs interdisciplinary collaboration (data science, human sciences, HCI) and contextualized evaluation protocols to recommend appropriate explanations for a given domain, use case, and audience",
      "role": "Conclusion",
      "parents": [
        0,
        12
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "A long-term goal and future direction is a model-agnostic, evidence-based framework that selects or recommends the best explanation method for a problem context and user, acknowledging limitations such as subjectivity and domain-specificity",
      "role": "Conclusion",
      "parents": [
        0,
        12,
        13
      ],
      "children": null
    }
  ]
}