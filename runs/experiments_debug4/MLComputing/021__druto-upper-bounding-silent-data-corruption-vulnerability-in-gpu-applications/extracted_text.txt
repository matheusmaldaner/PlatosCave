--- Page 1 ---
DRUTO: Upper-Bounding Silent Data Corruption
Vulnerability in GPU Applications
Md Hasanur Rahman∗, Sheng Di†, Shengjian Guo‡, Xiaoyi Lu§, Guanpeng Li∗, Franck Cappello†
∗University of Iowa, IA, USA
†Argonne National Laboratory, IL, USA
‡Amazon Web Services, CA, USA
§University of California, Merced, CA, USA
mdhasanur-rahman@uiowa.edu, sdi1@anl.gov, sjguo@amazon.com,
xiaoyi.lu@ucmerced.edu, guanpeng-li@uiowa.edu, cappello@mcs.anl.gov
Abstract—Due to the increasing scale of high-performance
computing (HPC) systems, transient hardware faults have be-
come a major reliability concern. Consequently, Silent Data
Corruptions (SDCs) due to these faults have been a common
insidious consequence in GPU applications. Developers often
measure the application resilience with a set of program test
inputs available in the benchmark suite, assuming the resilience
would not ﬂuctuate much among different inputs. However, we
observe that this assumption often results in an over-optimistic
evaluation for GPU applications. As a result, the subsequent
SDC protection following the evaluation can hardly meet the
expected reliability bar in the production environment, where
applications would run with potentially arbitrary input values.
To this end, we propose DRUTO – a compiler-based automated
technique that searches for inputs to incrementally approach
the upper bound of a GPU application’s SDC probability. We
develop DRUTO based on the property that the resilience proﬁles
of a small group of representative threads in a GPU kernel can
approximately rank various inputs in terms of the overall SDC
probability. Therefore, DRUTO strategically steers the search
towards new program inputs that efﬁciently portray the overall
SDC probability. Evaluation shows that the SDC probability
derived from DRUTO’s input generation is as much as 74× higher
than that from existing techniques. Moreover, existing techniques
cannot ﬁnd our generated inputs even given 5× more search time.
Index Terms—GPU Applications, GPU Resilience, SDC Bound-
ing, Multiple Inputs
I. INTRODUCTION
Among various types of errors [1]–[3], the risk of transient
hardware faults (e.g., soft errors) has been amplifying in line
with the increasing scale of modern computing systems [4],
[5]. For example, Venkatesha et al. [5] shows that, at 16nm
process node size, a 100-core chip could come across one
failure/hour due to soft errors. As of today, the situation can
only be worse since the node size is becoming even smaller.
Unavoidably, the soft errors could propagate through program
executions and impact computation results. In particular, once
certain vulnerable instructions get executed, wrong program
outputs (i.e., silent data corruption or SDC) may appear and
cause severe consequences [6]–[9]. Thus, harmful soft errors,
when they occur, can seriously threaten the trustworthiness of
the computation in HPC systems. Even worse, the problem is
exacerbated in the case of Graphics Processing Units (GPUs)
that play the role of essential cornerstones in today’s high-
performance systems [8], [10]–[12]. The execution units in
GPUs consist of many cores that are apt to expose greater
surface to soft errors [13]–[16], seriously compromising HPC
system reliability.
While hardware-level protection techniques have demon-
strated effectiveness, practitioners and researchers anticipate
modern software to withstand hardware faults for several
reasons: (1) Software-level techniques can capitalize on
hardware-level redundancy, such as those resources that are
under-utilized, to achieve better efﬁciency; (2) Software pro-
tection technique is ﬂexible [17], [18], allowing conﬁgurability
in the protection (e.g., selectively protect the most vulnerable
parts of the program); (3) Finally, SDCs are application spe-
ciﬁc, requiring protection based on the reliability requirements
of speciﬁc applications. In contrast, an on-or-off hardware
protection fashion does not provide ﬂexibility in these sce-
narios. As a result, there is a growing demand for software-
level resilience and protection techniques [19]–[23]. Random
fault injection (FI) is commonly used to assess a program’s
resilience to SDCs [19], [24]. Typically, the program is repeat-
edly executed with test inputs during numerous FI campaigns
to achieve statistical signiﬁcance in SDC measurements. If the
results fall short of reliability targets, developers must selec-
tively enhance protection for vulnerable program sections until
reaching the desired reliability level. Past evaluations have
relied on reference inputs from benchmark suites. However,
as these inputs are primarily designed for performance and
functionality testing, they often fail to reveal hardware faults,
resulting in overly optimistic SDC assessments and weak
protection [15], [21], [25]. Consequently, production HPC ap-
plications may face higher SDC risks [14], [26]. SDC incidents
can lead to data loss, service outages, inaccurate results, safety
breaches, and mission failures [27], [28]. Industry leaders such
as Meta and Google [4], [29] have reported unexpectedly
high SDC rates, incurring substantial costs for critical failure
investigations and months of debugging efforts. To address
the above problem, developers should ﬁrst thoroughly assess
the SDC probability of a program, especially understanding
the upper bound of SDC probability that indicates worst-
case impacts. The program input that can depict the upper
582
2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
1530-2075/24/$31.00 ©2024 IEEE
DOI 10.1109/IPDPS57955.2024.00058
2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS) | 979-8-3503-8711-7/24/$31.00 ©2024 IEEE | DOI: 10.1109/IPDPS57955.2024.00058
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 2 ---
bound of a program’s SDC probability is so called SDC-bound
input. In reliability evaluation, being conservative is important
and often required to prevent programs from unexpected
computation results in production, thus meeting the reliability
target. Therefore, SDC-bound input is highly needed for a
thorough evaluation to protect programs from worst-case SDC
probability [23], [25].
However, identifying the SDC-bound input is challenging:
(1) The input space of a program is often huge. Searching
for a particular input value has to efﬁciently address the
state space explosion problem; (2) The SDC evaluation for
even a single input is expensive. With a naive approach,
evaluating a candidate input during the search requires massive
amount of FIs to the GPU program at both kernel and thread
levels. Conducting repeated evaluations for multiple input
candidates to examine the best input that introduces highest
SDC possibility makes the whole evaluation process extremely
time-consuming and thereby impractical [15], [21].
Recently, Rahman et al. [25] proposed PEPPA-X that identi-
ﬁes the SDC-bound input for CPU programs. PEPPA-X lever-
ages FIs to ﬁrst evaluate SDC probability of each instruction
and compute the SDC distribution across the instructions of
a program. Then, based on the distribution data, PEPPA-X
guides the input search toward the program regions owning
higher vulnerable potentials. Their key insight is that the per-
instruction SDC distribution in a program tends to be stable
across different inputs. While PEPPA-X works well for CPU
applications, we identify that their conclusions no longer hold
true for GPU applications. In other words, PEPPA-X becomes
ineffective for GPU applications (ref. Section III). With the in-
creasing signiﬁcance of GPUs in modern HPC systems, there’s
an imminent need for a specialized and efﬁcient technique to
determine upper bound of GPU kernel SDC probability.
In this paper, we propose DRUTO1 – an efﬁcient compiler-
based technique that automatically searches for the SDC-
bound input of GPU kernels. DRUTO incrementally calculates
the upper bound of the SDC probability without ever requiring
any time-consuming FIs. The key insight of DRUTO is that, for
a GPU application, the relative kernel SDC probability ranking
in terms of different inputs can be approximated by proﬁling
the kernel-level behaviors of a small group of representative
threads. Based on the insight, DRUTO ﬁrst leverages program
analysis techniques to identify and proﬁle the representative
threads. Then, DRUTO builds the dynamic ranking of the
implied SDC probability with respect to each candidate input.
After that, DRUTO steers dynamic input fuzzing to locate
SDC-bound inputs with the ranking-based feedback in the
search. To the best of our knowledge, DRUTO is the ﬁrst
automated, effective, and efﬁcient search technique that upper-
bounds the GPU kernel SDC probability.
We summarize the main contributions as follows:
• We propose DRUTO, a novel method for incrementally
estimating the upper bound of GPU kernel SDC prob-
ability without resorting to costly FIs. Our approach
1Code is available at https://github.com/hasanur-rahman/DRUTO
combines compiler-based static and dynamic program
analysis techniques for an automated search.
• We compare DRUTO to two baseline methods (random
input generation and PEPPA-X). Our experiments demon-
strate that given same amount of time, DRUTO outper-
forms these baselines by up to 74× in upper-bounding
SDC probabilities. The superiority holds even when the
baselines are given 5× more search time.
• We conduct two case studies: (1) We stress test the
widely used selective instruction duplication technique
using SDC-bound inputs from DRUTO, and show obvious
deﬁciencies in the protection. (2) We compare DRUTO
with state-of-the-art SDC modeling technique, GPU-
Trident [21], in upper-bounding SDC probability for GPU
kernels. We ﬁnd that GPU-Trident, while proﬁcient with
single program input, lacks sensitivity to diverse inputs,
making it less effective in upper-bounding kernel SDCs.
II. BACKGROUND AND MOTIVATION
This section ﬁrst reviews GPU, SDCs, fault model, FI
methodology, and genetic algorithm. Further, we discuss lim-
itations of the related work, PEPPA-X, to motivate our work.
A. GPU Architecture
This paper focuses on GPU programs using the NVIDIA
CUDA programming model, but our technique is applicable
to any generic GPUs. The CUDA programming model consists
of multiple streaming multiprocessors (SMs), each organized
with a hierarchy of threads. These threads are grouped into
cooperative thread arrays (CTAs) or thread blocks. Within
each CTA, multiple warps exist, which are ﬁxed groups of
32 threads. CUDA follows single instruction multiple threads
(SIMT) execution model, meaning threads in a warp execute
the same instructions with different data values.
B. Silent Data Corruptions (SDCs)
In this paper, we adhere to standard reliability terminology
followed by the related studies [15], [20], [22], [30]–[35]. We
deﬁne SDC as a discrepancy between a faulty program’s output
and a fault-free execution. SDC probability represents the
probability of SDC occurrence given a fault is exercised by an
executed instruction. Conversely, benign faults occur when the
faulty execution matches the fault-free output due to masking
or overwriting by subsequent instructions. A static instruction
refers to a compiled program code instruction, while a dynamic
instruction is an instance of a static instruction executed during
program execution. Additionally, we introduce the concepts
of dominant thread and non-dominant thread in GPU kernel
execution, where dominant threads have the most dynamic
instructions. These distinctions play a role in our technique
to upper bound the SDC probability. Lastly, SDC coverage is
deﬁned as the percentage of SDCs that is mitigated by the
deployed protection technique [36].
583
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 3 ---
C. Fault Model
We employ a standard fault model commonly used in
reliability research [21], [32], [37]–[41]. Our focus is primarily
on transient hardware faults affecting processor components,
including architectural registers and functional units such as
ALUs, LSUs, and GPRs, impacting program execution and
data integrity. We do not account for faults in memory or
caches, as modern GPUs often use ECC to protect these areas.
Additionally, we exclude faults in program counters and GPU
control logic, as they can be detected by control-ﬂow checking
techniques [42], [43]. Regarding control-ﬂow anomalies, we
differentiate between two types: (1) faults causing program
jump to arbitrary or illegal addresses and (2) faults leading to
incorrect but legal branch selections. Our fault model focuses
on the second type, allowing program execution to take an
incorrect yet legal branch due to soft error propagation. We
deﬁne a fault as a single-bit ﬂip in the destination register
of a randomly selected instruction. Consistent with prior
research [19], [32], [44], we consider at most one fault per
program execution. Some studies suggest that multiple bit-
ﬂip faults may become more prevalent in future computing
systems. However, other research [45] indicates that the assess-
ment of SDC probability may not be adversely affected even as
the number of bit-ﬂips increases. Most evaluation methods and
conclusions on SDCs for single bit-ﬂip faults may still remain
applicable in the cases of multiple bit-ﬂips, according to these
studies. Conversely, some literature calls for a more systematic
study to understand this new trend [46], [47]. We acknowledge
this as an open question and express our intention to extend
our studies to multiple bit-ﬂips in future work. However, for
this study, we opt for the single bit-ﬂip model. This decision is
grounded in the fact that the single bit-ﬂip fault model has been
the de facto standard in the majority of current studies [21],
[32], [37]–[40], making our choice appropriate.
D. Fault Injection Methodology
We use LLFI-GPU, an open-source fault injector commonly
used in GPU program studies [21], [40], [48] for fault in-
jections. To ensure a fair comparison with PEPPA-X [25] (a
baseline in our evaluation) which uses LLFI for FIs in CPU
programs, we use LLFI-GPU [19] for GPU program fault in-
jections. Our FI methodology involves injecting 1,000 random
faults to measure overall GPU kernel SDC probability, 100
random faults to assess SDC probability of a static instruction,
and 1,000 random faults to evaluate SDC probability of a
thread. Our FI results exhibit an error bar ranging from 0.10%
to 3.10% with 95% conﬁdence interval, which is comparable
with recent studies [19], [21], [22], [25]. Since we consider
faults on GPUs, we only consider the soft errors occurred in
the kernel part of a GPU program.
LLFI-GPU performs FIs at LLVM IR level of GPU kernel
code. However, FIs can also be performed at other layers such
as micro-architectural [49] or RTL-level [50]. The measure-
ment is speciﬁc to the layer and fault model assumed. Derating
factors can be applied when correlating measurement across
different layers, which is out of scope of this study. Our fault
model is clearly described and commonly used in the area.
E. Genetic Algorithm (GA)
GA is an optimization algorithm [51] that efﬁciently ex-
plores search space to attain a speciﬁc objective. Similar
to natural selection, GA repeatedly seeks better solutions
from a large populations of program inputs. Each iteration
involves improving the population for the next generation. In
our context, program inputs are termed individuals, and their
quality is gauged by a ﬁtness function. GA employs selec-
tion and recombination operations to reﬁne the population,
gradually phasing out individuals with lower ﬁtness scores.
Recombination includes crossover, where two randomly cho-
sen inputs exchange values, and mutation, where parts of
a randomly selected input are altered. Crossover aims for
optimal solutions, while mutation broadens the search. In this
paper, we set the mutation rate at 0.4 and the crossover rate
at 0.05, following the heuristics proposed by [52]–[54]. Note
that DRUTO is not tied to GA; any search-based optimization
algorithm would work.
F. Bounding SDC Vulnerability in CPU programs
PEPPA-X [25] identiﬁes the SDC-bound input of a given
CPU program. Instead of emulating all possible program
inputs and conducting FIs on each candidate input, PEPPA-X
utilizes the observation that the relative ranking of the per-
instruction SDC probability stays stable when the program
input changes. PEPPA-X calculates the SDC ranking of each
instruction, and leverages this information to approximate the
overall SDC probability when the input is changed. Hence, if
an input exercises the vulnerable instructions (instructions with
high ranking in SDC) more frequently, it will result in a higher
overall program SDC probability. Based on this observation,
PEPPA-X searches for SDC-bound inputs without conducting
FIs every time an input is tested. As a result, it is much faster
than a traditional FI-based technique in generating SDC-bound
inputs for a CPU program.
While PEPPA-X is proved effective for CPU programs,
it is unclear whether this technique remains effective for
GPU programs, as GPU programs follow a different program-
ming model and run on a different architecture [14], [19].
Consequently, the error propagation behaviors might differ
from those in CPU programs when dealing with multiple
diverse inputs. This gap mainly motivates our preliminary
study (Section III), and leads to propose a new design to
address the challenges of upper bounding SDCs for GPU
programs.
III. PRELIMINARY FAULT INJECTION EXPERIMENTS AND
OUR OBSERVATIONS
In this section, we assess state-of-the-art PEPPA-X’s perfor-
mance on GPU programs. Then, we discuss our heuristics that
accelerates the search for bounding GPU kernel SDCs.
584
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 4 ---
A. Experimental Setup
1) Hardware Details: We run our experiments on Linux
machine equipped with Intel 20-core processors and 64GB
memory. To execute GPU applications and inject faults, we
use NVIDIA GeForce RTX 2080 Ti driver.
2) Benchmarks: We conduct experiments using 11 GPU
benchmarks from 5 benchmark suites, totaling 14 GPU ker-
nels. Table I provides the benchmark details. In the paper,
when discussing GPU applications with only one kernel,
we omit kernel ID. Our benchmark selection adheres to
speciﬁc criteria: (1) widely used in recent studies on GPU
resiliency [14], [19], [21], [26]. (2) compatible with LLFI-
GPU [19] for FIs. (3) capable of generating diverse set of
inputs. Moreover, we try to add all benchmarks used in PEPPA-
X [25] for a fair comparison. However, only ﬁve benchmarks,
namely Pathﬁnder, Needle, Particleﬁlter, FFT, and XSBench,
have GPU CUDA versions in the benchmark suites. Moreover,
we encounter compatibility issues with XSBench when using
LLFI-GPU for FIs. For these reasons, we exclude three
benchmarks (CoMD, Hpccg, XSBench) used in PEPPA-X.
TABLE I: Benchmarks
Benchmark (Suite)
Kernels
Kernel ID
Pathﬁnder(Rodinia)
dynproc kernel
K1
Needle(Rodinia)
needle cuda shared 1 & needle cuda shared 2
K1 & K2
Particleﬁlter(Rodinia)
kernel
K1
FFT(Gearshifft)
fft kernel
K1
Backprop(Rodinia)
bpnn layerforward CUDA & bpnn adjust weights cuda
K1 & K2
BFS(Rodinia)
Kernel & Kernel2
K1 & K2
Jmeint(AxBench)
jmeint kernel
K1
BlackScholes(CUDA)
BlackScholesGPU
K1
2DCONV(PolyBench)
convolution2D kernel
K1
GEMM(PolyBench)
gemm kernel
K1
MVT(PolyBench)
mvt kernel1
K1
3) Program Input Generation: We generate program in-
puts with the following rules: (1) we randomize each input
argument from the value domain for the benchmarks taking
numerical inputs. This rule implies that the mutated input
values would not cause unexpected program behavior such as
program exceptions; (2) for benchmarks with structured non-
numeric inputs (e.g., graph input for BFS benchmark), we
re-use the benchmarks’ input generation scripts, and (3) we
only generate the inputs that comply with the FI tool LLFI-
GPU. Based on the above criteria, we generate 30 random and
valid inputs for each benchmark to perform our preliminary
FI study. Overall, our input generation method and choice of
30 random inputs for the preliminary FI study are inline with
prior works [20], [23], [25].
B. SDC Variation of GPU Kernels among inputs
We ﬁrst examine whether SDC variation exists and how
wide the variation could be in GPU kernels. To examine, for
each GPU kernel, we conduct FIs and measure kernel SDC
probabilities with 30 random inputs (Section III-A3). Fig. 1
shows the range of those SDC probabilities for each GPU
kernel. As we can see, the SDC probability of the GPU kernels
vary in a wide range, thus input sensitive. For example, the
kernels in Backprop varies from 0.10% to 77.70%, whereas
the result is only between 0.20% and 2.30% in Needle kernels.
0%
20%
40%
60%
80%
Pathfinder
Needle-K1
Needle-K1
Particlefilter
FFT
Backprop-K1
Backprop-K2
BFS-K1
BFS-K2
Jmeint
BlackScholes
2DCONV
GEMM
MVT-K1
SDC Probability
GPU Kernels
Fig. 1: Overall GPU Kernel SDC Probability Range; Red bar
indicates kernel SDC probability using reference inputs.
Moreover, the red line in each bar in Figure 1 represents
the SDC probability measured with the default reference input
provided in benchmark suite. As seen, the SDC probability
measured with reference input cannot reach the upper bound of
the SDC probability range for most of the kernels. Moreover,
the SDC probability measured with reference input can be
sometimes in the lower half of the range. Therefore, we
see that SDC variation exists across program inputs in GPU
kernels, and using only reference input for evaluating kernel
SDC probability may not be a viable way to gain a pessimistic
picture of GPU kernel resilience.
C. Ranking Stability of Per-instruction SDCs
Recall that PEPPA-X [25] observes that the ranking of per-
instruction SDC probabilities in a CPU program remains stable
across program inputs; and so the observation can be used
to guide the search of SDC-bound inputs (Section II-F). We
now examine whether the observation still holds for GPU
applications. In order to assess the ranking stability, the SDC
probabilities of instructions in a kernel are measured with each
of 30 inputs. Then, we obtain the ranking of each instruction’s
SDC probability with each input of the kernel. After that, we
compute Spearman ranking correlation coefﬁcients between
the pairwise input rankings. An average coefﬁcient is obtained
to measure the per-instruction ranking stability of the kernel.
This measurement is consistent with [25] for a fair comparison.
TABLE II: Average Spearman Ranking Correlation Coefﬁcient
of Per-instruction SDC Probability across Inputs
Pathﬁnder
Needle-
K1
Needle-K2
Particleﬁlter
FFT
Backprop-
K1
Backprop-
K2
0.59
0.37
0.33
0.79
0.60
0.27
0.64
BFS-
K1
BFS-
K2
Jmeint
BlackScholes 2DCONV GEMM
MVT-
K1
0.84
0.74
0.73
0.25
0.69
0.75
0.77
Table II shows the results. The correlation coefﬁcients range
from 0.25 (BlackScholes) to 0.84 (BFS-K1), with an average
of only 0.59 among all GPU kernels. The average correlation
is signiﬁcantly lower than the coefﬁcient reported for CPU
585
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 5 ---
programs (0.83 on average) in PEPPA-X [25]. Note that we
have four benchmarks (Pathﬁnder, Needle, Particleﬁlter and
FFT) used in both this study and [25]. By comparison, in these
benchmarks, the correlations in GPU versions are considerably
lower than the result in the CPU versions [25]. We observe
0.59, 0.35 (averaging Needle-K1 and Needle-K2), 0.79, and
0.60 in GPU versions, whereas the results are 0.92, 0.79, 0.90,
and 0.77 in their CPU versions [25]. The main reason stems
from the SIMD model that GPU application follows. Although
different threads in a kernel execute the same set of static
instructions, dynamic data values are different in inter-thread
data-ﬂow, thereby affecting error propagation and the relative
positions in instruction SDCs ranking.
In conclusion, we observe that the stable instructions-level
ranking of SDC probabilities in CPU programs no longer holds
in GPU kernels. Hence, PEPPA-X would be less effective in
bounding GPU kernel SDCs (conﬁrmed later in Section V).
D. Relationship: SDC of Dominant Thread and Kernel SDC
As seen above, we cannot use the heuristics from PEPPA-X
to bound overall SDC probability for GPU kernels. In what
follows, we investigate resilience properties related to GPU
kernels to gain some key observations to efﬁciently design a
search method that can bound GPU kernel SDC probability. A
GPU kernel consists of multiple threads, and a soft error may
randomly hit any thread at runtime. We hypothesize that the
dominant thread (see deﬁnition in Section II-B) often exerts
dominance in determining the overall kernel SDC probability.
Our rationale roots from the random nature of soft errors
in the fault model: any thread t that has the most dynamic
instructions should be more likely the target of random faults;
hence the errors will more likely propagate in t’s execution,
affecting the ﬁnal kernel computation results.
We test our hypothesis in the following steps. First, we
proﬁle the number of dynamic instructions in each thread of a
kernel. Second, we choose the thread with the most dynamic
instructions, it is called dominant thread of the kernel. Third,
we measure the SDC probability of the dominant thread in the
kernel, as well as the overall SDC probability of the kernel,
with each of 30 inputs. Finally, we rank these measurements
among 30 inputs, and calculate the Spearman correlation
coefﬁcient between the two ranking lists.
TABLE III: Ranking Correlation Coefﬁcient between SDC
Prob. of Dominant Thread and Kernel across Inputs
Pathﬁnder
Needle-
K1
Needle-K2
Particleﬁlter
FFT
Backprop-
K1
Backprop-
K2
0.77
0.78
0.75
0.83
0.83
0.86
0.86
BFS-
K1
BFS-
K2
Jmeint
BlackScholes 2DCONV GEMM
MVT-
K1
0.92
0.81
0.87
0.80
0.80
0.81
0.71
Table III shows the results. As seen, the correlations range
from 0.71 to 0.92, with an average of 0.81, indicating a strong
correlation between the SDC probability of the dominant
thread and the kernel. We conclude our observation:
Observation (O1): The SDC probability of a kernel
can be approximated by the SDC probability of its
dominant thread.
E. Relationship: Thread Instruction Count and Kernel SDC
As discussed in Section I, FI is extremely expensive in the
resilience evaluation. To efﬁciently identify the SDC-bound
input among the huge input space of a GPU program, one
needs to avoid FIs as much possible during the search.
Yang et al. [14] and Nie et al. [15] observed that the number
of dynamic instructions of a thread is highly correlated to
the SDC probability of that thread. That is, in a GPU kernel,
the higher the number of dynamic instructions of a thread is,
the higher the SDC probability of that thread shows. Hence,
we hypothesize that the number of dynamic instructions in a
thread can rank the thread among other threads in terms of
the thread-level SDC probability within a GPU kernel.
We run the followings to verify our hypothesis. First, for
a given GPU kernel, we sample (a) 10 threads with the
highest number of dynamic instructions, (b) 10 threads with
least number of dynamic instructions, and (c) 10 threads with
medium number of dynamic instructions under each of 30
random inputs. Thus, for each input, we have dynamic instruc-
tion counts for 30 sampled threads. Second, we measure the
SDC probabilities of these 30 threads under each input. Third,
for each input, we rank these two measurements (number of
dynamic instructions and SDC probabilities) among these 30
threads. Next, we measure the Spearman correlation coefﬁcient
between two ranking lists under each input. Finally, we take
the average of these measurements.
TABLE IV: Average Ranking Correlation between Thread-
level SDC Probability and Dynamic Inst. Count across Inputs
Pathﬁnder
Needle-
K1
Needle-K2
Particleﬁlter
FFT
Backprop-
K1
Backprop-
K2
0.77
0.60
0.69
0.62
0.78
0.79
0.75
BFS-
K1
BFS-
K2
Jmeint
BlackScholes 2DCONV GEMM
MVT-
K1
0.75
0.69
0.78
0.61
0.92
0.68
0.75
Table IV shows the results where the ranking ranges from
0.60 to 0.79, with an average of 0.73 across all kernels. The
results suggest that one can use the relative ranking of the
number of dynamic instructions in a thread to approximate
the relative ranking of the thread-level SDC probability in a
GPU kernel, as we conclude in observation (O2).
Observation (O2): The relative ranking of the dy-
namic instruction counts of threads can be used to
approximate the relative ranking of the threads’ SDC
probabilities across inputs in a GPU kernel.
F. Our Heuristic
Figure 2 shows step by step how each of our observations
individually contributes to the overarching goal of DRUTO.
The small vertical box beside each step in Figure shows the
586
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 6 ---
5DQNNHUQHO6'&
ZLWKFXUUHQWLQSXW
(DFK*38NHUQHO
KDVORWVRIWKUHDGV
8SSHUERXQGRI
*38NHUQHO6'&
DFURVVLQSXWV
21XPEHURI
G\QDPLFLQVWGULYHV
WKUHDG¶V6'&
2'RPLQDQW
WKUHDGGULYHV
NHUQHO6'&RILQSXW
2EWDLQWKHNHUQHO
6'&UDQNLQJ
DFURVVLQSXWV
6WDUW
(QG
7KHDG
.HUQHO
,QSXW
7KHDG
.HUQHO
,QSXW
Fig. 2: Contribution of Each Observation to Upper Bound
Kernel SDC Probability across Program Inputs.
direction (program input, kernel, or thread-wise) that the step
is applied. For example, third step states: Each GPU kernel
has lots of threads, considering the step as thread-wise. Recall
that our goal in searching SDC-bound input is to ﬁnd the
highest SDC probability that an input can introduce into a
GPU kernel. Hence, we only care about the relative ranking of
the SDC probability that each input may result in compared to
other inputs. Also, we do not necessarily require a quantitative
value of the actual SDC probability measurement. Therefore,
we can leverage O1 and O2 in designing the ﬁtness function in
the search, without the need of any expensive FIs. We present
our logic as follows. First, we proﬁle the amount of dynamic
instructions in each thread to rank the threads. Second, the
dominant thread owns the highest SDC probability among
all the threads in the kernel (O2). Third, based on (O1), this
dominant thread also correlates to the SDC probability of the
kernel. Hence, we summarize our heuristic:
Our Heuristic: By proﬁling the number of dynamic
instructions of dominant thread of a kernel, we can
identify the relative ranking of the kernel SDC proba-
bility for a given program input.
In next section, we leverage the heuristic in our proposed
search method to ﬁnd SDC-bound inputs for GPU kernels.
IV. DRUTO DESIGN
This section presents the design of DRUTO. We ﬁrst intro-
duce the overall workﬂow of DRUTO and then explain the core
components in details. We make the code publicly available
at https://github.com/hasanur-rahman/DRUTO.
&RPSLOHWR
//90,5
&RGH
,QVWUXPHQWDWLRQ
*HQHWLF6HDUFK(QJLQH
.HUQHO7UDFH3UXQLQJ
7KUHDG/HYHO'\QDPLF
,QVWUXFWLRQ3URILOLQJ
5DQNLQJ(VWLPDWHRI
.HUQHO6'&
)LWQHVVVFRUH
(DFK,QSXW
6'&%RXQG,QSXW
&RPSLODWLRQ
)X]]LQJ(QJLQH
*383URJUDP
6RXUFH&RGH
Fig. 3: DRUTO Workﬂow
A. Design Overview
Figure 3 depicts the overall workﬂow of DRUTO. Initially,
DRUTO inputs the source code of a GPU application and
compiles the code to the corresponding LLVM IR with
our static instrumentation passes. After compiling the GPU
application, DRUTO starts running a fuzzing engine which
leverages Genetic Algorithm (GA) for input generation. Note
that fuzzing engine in DRUTO is not tied to GA, but can
be integrated to any search-based optimization algorithm that
provides a ﬁtness function to evaluate the input of interest.
For a generated input at each generation of GA, DRUTO
assigns a score to this candidate input for the later ranking
purpose. To determine the score, DRUTO performs proﬁling
on the speciﬁed kernel by tracing the number of dynamic
instructions of each thread to identify the dominant threads.
Then, DRUTO applies a pruning strategy (will be discussed
later in Section IV-B3) on the dynamic analysis of kernel
trace to reduce the computation overhead in the proﬁling and
analysis phases. Finally, with the information of dominant
threads, DRUTO evaluates the ﬁtness score for the current
candidate input from GA. The ﬁtness score for that candidate
input is a proxy to estimate its ranking of the SDC probability
among other GA explored inputs of the speciﬁc kernel. Recall
that this ranking strategy is based on our proposed heuristic
(Section III-F). The measured ﬁtness scores for explored inputs
will be used as the feedback to the GA to continue and
further optimize the search to subsequent generations. Usually,
the number of GA generations is speciﬁed by the users,
depending on the time allowance of the search. The entire
process requires no FI, making it more efﬁcient than existing
methods for bounding SDC probability of GPU kernels.
B. Core Components
In this subsection, we describe each of our DRUTO compo-
nents in details.
1) Genetic Search Engine: We use GA to guide the search
for the SDC-bound input. Speciﬁcally, we drive the search
engine by a ﬁtness score-based feedback mechanism to explore
the search space towards ﬁnding optimal result. Note that for
the very ﬁrst generation of GA, we choose a set of inputs
(which is called population) generated uniformly at random
from the entire search space. In our context, the search space
is the input value space. For an input inp, our choice of ﬁtness
function (to be described in Section IV-B4) should provide a
score that should be able to approximate the relative ranking of
the kernel SDC probability under inp among all other possible
inputs. The optimal result is the program input with a highest
ﬁtness score, which is indeed an alternative measurement of
the upper bound of SDC probability. In this way, we can
guide the search towards ﬁnding SDC-bound inputs. To design
a effective ﬁtness function, we need to consider different
resilience characteristics of a GPU kernel. In the subsequent
steps, we present how to design ﬁtness function to assign a
reasonable score for each explored input.
2) Thread-Level Dynamic Instruction Proﬁling: For each
input inp generated by the genetic search engine, we run
587
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 7 ---
the program with inp and proﬁle the number of dynamic
instructions of each thread in a kernel. Then we ﬁnd the
dominant threads of the kernel from the proﬁling. Recall that
our heuristic implies that the number of dynamic instructions
in dominant threads can rank the overall kernel SDC proba-
bility across program inputs (ref. Section III-F). Hence, after
ﬁnding dominant threads, we can approximate the ranking of
the overall kernel SDC probability for inp among all the other
inputs. Note that we do not seek for the exact overall kernel
SDC probability for inp, as we only need to approximate
the ranking for the search. In this way, we can incrementally
explore and guide the search to ﬁnd the SDC-bound input.
3) Kernel Trace Pruning: A static kernel can have many
dynamic instances during the program execution. That is, a
static kernel can be invoked many times at runtime. Each
dynamic instance of a static kernel may also have a large
amount of threads. We observe that the number of dynamic
instances of a static kernel is highly application-speciﬁc.
That is, some GPU kernels may contain more than hundreds
of instances, whereas others may have only one dynamic
instance. The reason of having one dynamic instance for some
GPU kernels is that the input arguments’ values does not
have direct inﬂuence on determining the number of invoked
dynamic instances. We illustrate this below with an example.
Figure
4
shows
an
example
static
kernel
needle cuda shared 1 from Needle benchmark (ref. Table I).
In the example, the static kernel has block width number
of dynamic instances after the kernel is launched, where
block width value is directly related to the Needle-K1 input
arguments. As such, block width could be a large integer
value, then the needle cuda shared 1 kernel could potentially
have thousands of dynamic instances. Consequently, proﬁling
all the threads in each dynamic kernel instance would be very
time-consuming.
Fig. 4: Example of a Static Kernel of Needle-K1
...
for (int i = 1; i <= block_width; i++){
...
needle_cuda_shared_1<<dimGrid, dimBlock
>>>(...);
...
}
...
In our experiment, we observe that given an input, the
distribution of dynamic instructions across different dynamic
kernel instances of a static kernel do not vary much. Figure 5
validates our above observation. Speciﬁcally, it shows dynamic
instruction count across different dynamic kernel instances
of a static kernel for Pathﬁnder and Particleﬁlter. We show
results from two benchmarks for brevity; Others show similar
results. Note that readers should not be confused by the fact
that we discuss here the distribution of dynamic instructions
across dynamic kernel instances of a static kernel rather
than the distribution of dynamic instructions across threads
– latter is discussed earlier in Section IV-B2. Based on our
above observation, DRUTO proﬁles the dominant threads from
only one of the dynamic kernel instances, and use it as a
representative. This kernel trace pruning in the proﬁling allows
us to further accelerate DRUTO in the search. We will show
the effectiveness of this pruning strategy in Section V-D.
 4000
 6000
 8000
 10000
 0
 250  500  750  1000 1250
Dynamic Inst. Count
Dynamic Kernel Instance No.
(a) Pathﬁnder
 100000
 120000
 140000
 160000
 180000
 0  150 300 450 600 750 900
Dynamic Inst. Count
Dynamic Kernel Instance No.
(b) Particleﬁlter
Fig. 5: Illustrating Similarity among Dynamic Kernel Instances
of a static kernel in terms of Their Dynamic Instruction Count
4) Deriving Fitness Score: In this step, we obtain the ﬁtness
score of the generated input inp by GA by relating it to the
information of dominant threads of a chosen dynamic kernel
instance. We then rank the score for inp among other generated
input candidates by GA. In this way, we gradually steer the
search in GA towards ﬁnding the SDC-bound input based on
the quantitative score-based ranking feedback.
Fitness Score of an Input =
n

j=1
DF(d,j)
(1)
Equation 1 calculates the ﬁtness score of a generated input
by relying on information from a chosen dynamic kernel
instance d. Here, n is the number of dominant threads in
d, DF represents the number of dynamic instructions of a
thread j within d. Note that the ﬁtness score in equation 1
requires no FIs to obtain, unlike the prior work PEPPA-X [25].
Thereby, our DRUTO is expected to run much faster than
existing techniques in ﬁnding SDC-bound inputs in GPU.
V. EVALUATION
In this section, we ﬁrst compare DRUTO’s capability in
bounding the overall GPU kernel SDC probability compared to
baselines. Then, we evaluate the efﬁciency of DRUTO. Finally,
we verify the effectiveness of our pruning strategy in DRUTO.
A. Baselines for Evaluation
In our problem setting, we compare two state-of-the-art
baselines: PEPPA-X [25] and the RANDOM method. Our goal
is to determine the upper bound of SDC probability for a
given GPU kernel across the input space. To achieve this, we
re-implement PEPPA-X for GPUs, adapting it from its orig-
inal CPU-based implementation. Additionally, we utilize the
straightforward RANDOM approach, which involves ranking
inputs by conducting FIs to identify the SDC-bound input.
RANDOM also serves as a baseline in PEPPA-X. Note that
while we discussed earlier other related works [14] and [15],
they do not align with our problem setting and are only
referenced to provide context for our observations.
588
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 8 ---
0%
10%
20%
30%
40%
50%
10
50
100
(a) Pathﬁnder
0%
2%
4%
6%
8%
10
50
100
(b) Needle-K1
0%
2%
4%
6%
8%
10%
12%
10
50
100
(c) Needle-K2
0%
15%
30%
45%
60%
10
50
100
(d) Particleﬁlter
0%
20%
40%
60%
80%
10
50
100
(e) FFT
0%
20%
40%
60%
10
50
100
(f) Backprop-K1
0%
20%
40%
60%
10
50
100
(g) Backprop-K2
0%
10%
20%
30%
10
50
100
(h) BFS-K1
0%
10%
20%
30%
10
50
100
(i) BFS-K2
0%
3%
6%
9%
12%
15%
10
50
100
(j) Jmeint
0.0%
0.3%
0.6%
0.9%
10
50
100
(k) BlackScholes
0%
20%
40%
60%
10
50
100
(l) 2DCONV
0%
20%
40%
60%
10
50
100
(m) GEMM
0%
20%
40%
60%
80%
10
50
100
(n) MVT-K1
Fig. 6: The Comparison of Upper-Bounding Kernel SDC Probabilities of DRUTO, PEPPA-X and RANDOM. X-axis and Y-axis
Correspond to Number of Gen. in GA and Bounded Kernel SDC Probability by Different Techniques respectively.
B. Accuracy: Upper Bounding GPU Kernel SDC Probability
with DRUTO
This section studies the accuracy of DRUTO on bounding
the SDC probability of GPU kernels. As mentioned previously,
to present a quantitative result, we compare DRUTO with
two baselines: (1) The PEPPA-X [25] method, and (2) The
random input generation method (RANDOM). We give all three
methods the same search time as DRUTO takes to produce
SDC-bound inputs at different generations in GA for fairness.
We inject 1,000 random FIs with the SDC-bound inputs
reported by DRUTO, PEPPA-X and RANDOM, to evaluate
the overall kernel SDC probabilities with those inputs. This
FI measurement is consistent with prior works [14], [19],
[21], [25]. Recall that DRUTO needs no FI during the search.
PEPPA-X requires no FI during the search but still needs a
reduced FI campaign in the initial phase to design the ﬁtness
function [25]. For both PEPPA-X and DRUTO, FIs are used to
evaluate the SDC-bound input once it is determined at the end
of the search to report the actual overall SDC probability. In
contrast, RANDOM requires FIs for each input to rank inputs.
Figure 6 shows the comparison among DRUTO, PEPPA-X
and RANDOM in bounding kernel SDC probability. We obtain
the results given the search time budgets same as taken by
DRUTO to reach 10, 50 and 100 generations in GA. We show
the results up to 100 generations time, as we observe that GA
in DRUTO reaches saturation to bound kernel SDCs after 100
generations for most of GPU kernels. In Figure 6, each subﬁg-
ure represents a GPU kernel. X-axis in each subﬁgure presents
the number of generations in DRUTO GA, and y-axis shows the
bounding SDC results of three techniques with the same search
time budget. We can see that, in most GPU kernels, DRUTO
ﬁnds the bounding kernel SDC probabilities while other two
methods fail to ﬁnd such SDC bounding results. For example,
at the search time budget of 10 generations in DRUTO GA,
DRUTO evaluates a bounding SDC probability of 74.40% for
Backprop-K2, but PEPPA-X and RANDOM evaluate that to
only 0.60% and 0.40%, respectively. On average, at the time
budget of 100 generations in DRUTO GA, DRUTO evaluates
bounding SDC probabilities across all GPU kernels of 38.98%,
while other twos evaluate that of only 26.44% and 24.41%
respectively. This fact clearly shows that DRUTO is far more
effective in bounding kernel SDC probability than the other
two methods. Figure 6 also shows a few GPU kernels, such as
FFT, BFS-K1, BFS-K2, where baselines could report similar
bounding SDC probabilities as DRUTO could. The reason of
these similar bounding results can be explained with the help
of Figure 1. In Figure 1, for kernels such as mentioned above,
the overall kernel SDC probabilities span a very limited range.
It implies that ﬁnding inputs that can upper bound kernel SDC
probabilities is easy for those kernels. Hence, for DRUTO,
PEPPA-X and RANDOM, any explored input would likely to
expose similar SDC probabilities independent of underlying
search technique.
However, it remains unclear whether investing more time
in the baselines would yield similar or superior results. To
this end, we examine whether baselines can ﬁnd an input that
upper bounds higher SDC probability than DRUTO, if given 5x
more search time than DRUTO search time. Here, 5x time is
a empirical balance between reasonable result and experiment
time. Figure 7 shows the data for this experiment. We see that
baselines (PEPPA-X and RANDOM) still cannot ﬁnd inputs that
yield comparable or better SDC bounding results for most of
the kernels. For instance, SDC bounding results by baselines
with Backprop-K2 remain still signiﬁcantly lower than that by
DRUTO. The above results demonstrates the effectiveness of
our heuristics in designing ﬁtness score for DRUTO search.
589
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 9 ---
0%
20%
40%
60%
80%
100%
Pathfinder
Needle-K1
Needle-K2
Particlefilter
FFT
Backprop-K1
Backprop-K2
BFS-K1
BFS-K2
Jmeint
BlackScholes
2DCONV
GEMM
MVT-K1
SDC Probability
GPU Kernels
DRUTO at GA Gen. 100
Peppa-X with 5x Time
RANDOM with 5x Time
Fig. 7: Accuracy Comparison between DRUTO at 100 Gener-
ations in GA Time and Baselines with 5x More Search Time
C. Efﬁciency Comparison
In this section, we compare the efﬁciency between PEPPA-
X and DRUTO. Note that, we exclude RANDOM method in
this evaluation. Intuitively, RANDOM cannot be faster than
DRUTO and PEPPA-X at any given search state because
RANDOM approach needs time-consuming FIs to measure
SDC probability for each search input.
Table V shows the average time consumption of DRUTO
and PEPPA-X to evaluate each explored input across 10, 50
and 100 generations in GA. For a generation g: (1) average
input evaluation time by DRUTO is the ratio of total search
time by GA to the number of unique inputs explored up to g,
(2) average input evaluation time by PEPPA-X is the one-time
reduced FI time plus the ratio of total search time by GA to the
number of unique inputs explored up to g As seen in the Table,
DRUTO is much faster than PEPPA-X for each GPU kernel.
On average across all GPU kernels, DRUTO takes only about
6 seconds to evaluate an input, whereas PEPPA-X takes about
260 seconds. The reason for such performance gap roots from
the designs of the two techniques. As discussed earlier, PEPPA-
X [25] requires a one-time reduced FI campaign to design its
ﬁtness function, whereas DRUTO requires no FI campaigns. In
other words, DRUTO greatly beneﬁts from our aforementioned
observations and heuristics, which fundamentally empower
DRUTO to outperform the state-of-the-art PEPPA-X.
TABLE V: Comparison of Average Per-input Evaluation Time
(in sec) by DRUTO and Peppa-X across Different Generations
Kernel
Path-
ﬁnder
Needle-
K1
Needle-
K2
Particle-
ﬁlter
FFT
Backprop-
K1
Backprop-
K2
DRUTO
8.74
5.89
5.85
3.35
2.44
10.04
10.28
PEPPA-X
269.88 376.63
300.74
26.20
454.58
847.42
395.32
Kernel
BFS-
K1
BFS-
K2
Jmeint
Black-
Scholes
2DCONV GEMM
MVT-
K1
DRUTO
4.60
4.50
7.45
3.05
6.60
3.24
3.43
PEPPA-X
13.58
7.76
633.82
197.51
40.14
15.78
56.78
D. Verifying Pruning Strategy
Recall that we propose a pruning strategy to prevent the
dynamic analysis (Section IV-B3) from incurring large compu-
TABLE VI: Computation Slowdown in Non-pruned DRUTO
Pathﬁnder
Needle-
K1
Needle-
K2
Particleﬁlter
FFT
Backprop-
K1
Backprop-
K2
30.16x
216.97x
290.14x
50.73x
1.00x
1.00x
1.00x
BFS-
K1
BFS-
K2
Jmeint
BlackScholes 2DCONV
GEMM
MVT-
K1
14.89x
14.46x
1.00x
429.01x
1.00x
1.00x
1.00x
TABLE VII: Bounding SDC Probabilities at 100 Generations
by DRUTO with Pruning vs DRUTO without Pruning
Kernel
W/O
Pruning
With
Pruning
Pathﬁnder
26.50%
25.00%
Needle-K1
3.80%
2.70%
Needle-K2
6.60%
6.50%
Particleﬁlter
35.90%
39.40%
Kernel
W/O
Pruning
With
Pruning
BFS-K1
22.40%
22.00%
BFS-K2
22.10%
20.20%
BlackScholes 0.70%
0.40%
tation overhead in DRUTO. Next, we analyze the effectiveness
of our pruning strategy by comparing incurred computation
slowdown and changes in bounding SDC probability results
by DRUTO with and without pruning.
Table VI shows the results. As seen in the Figure, the
computation slowdown ranges from 1.00× to 429.01× across
different GPU kernels. On average, the slowdown by DRUTO
without pruning is 80.96x across all kernels. The reason for
this extremely high cost in non-pruned DRUTO is that a static
GPU kernel can have hundreds of dynamic kernel instances,
and dynamic information tracing from all of these instances
incurs an unavoidably huge slowdown. Also, the computation
slowdown by DRUTO without pruning can sometimes be the
same (e.g., 1.00×) as the slowdown by DRUTO with pruning.
The reason is that the number of dynamic kernel instances
itself is benchmark-speciﬁc. Some static kernels launched only
once (one dynamic instance). Hence, they expose no pruning
opportunity. Such behaviors are highly application-speciﬁc.
We now verify whether our pruning affects the accuracy
of the bounding in DRUTO. In Table VII, we compare the
bounding results using DRUTO with and without the pruning
strategy. Note that we only show the results of GPU kernels
that present pruning opportunity (computation slowdown is
more than 1.00× in Table VI). As seen, the bounding SDC
probability obtained with and without the pruning are rather
similar, with only an average of 1.28% in difference, showing
that our proposed pruning technique does not affect the
accuracy of the bounding in DRUTO.
VI. CASE STUDY 1: STRESS-TESTING SELECTIVE
INSTRUCTION DUPLICATION
In this section, we conduct a case study using DRUTO to
stress-test the soft error protection technique –selective in-
struction duplication or SID – adopted extensively in resilience
research [21], [22], [36], [38], [55]. SID involves duplicating
only a subset of critical static instructions to balance run-
time performance overhead and SDC coverage achieved by
the protection. Given the impracticality of duplicating every
instruction in programs with millions of instructions [44], SID
emerges as a cost-effective solution. The selective nature of
590
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 10 ---
0%
20%
40%
60%
80%
100%
Pathfinder
Needle-K1
Needle-K2
Particlefilter
FFT
Backprop-K1
Backprop-K2
BFS-K1
BFS-K2
Jmeint
BlackScholes
2DCONV
GEMM
MVT-K1
SDC Coverage
GPU Kernel
Expected Coverage
Actual Coverage
Fig. 8: Stress Testing SID with DRUTO SDC-bound Inputs
SID is based on the observation that a small set of static
instructions in a program are responsible for the majority of
SDCs [21], [55]. Hence, identifying and duplicating those set
of instructions responsible for the majority of SDCs helps
minimizing the overhead compared to full duplication. The
selection process resembles classical 0-1 knapsack problem,
where the cost of an instruction is the overhead incurred by
duplicating the instruction, and the beneﬁt is the SDC coverage
achieved by doing it. Speciﬁcally, we use dynamic instruction
ratio of a static instruction as a proxy for its overhead.
Unfortunately, researchers often underestimate the SDC
resilience by implementing and evaluating the SID using
the reference input only [21], [38], [40]. We ﬁnd that the
protection would be compromised if we use DRUTO provided
SDC-bound inputs to stress-test the protected GPU kernels.
To verify this, we ﬁrst evaluate and protect each kernel using
FI with the reference input at 10%, 30%, 50%, 70% and
90% protection levels. After applying protection scheme, we
measure and report the SDC coverage (referred to as expected
coverage) with the same reference input in each GPU kernel at
those protection levels. Finally, we use the SDC-bound inputs
to conduct FI evaluation over each of these protected GPU
kernels at those protection levels. With that, we measure the
SDC coverage again, which we call actual coverage.
Figure 8 shows results for 30% protection levels for the
brevity. Others show similar corresponding results. On average
across all kernels, expected SDC coverage is 85.58%, whereas
the actual SDC coverage is only 59.94% respectively when the
protected programs run with the SDC-bound inputs. Our case
study shows that conventional SID protection scheme in GPU
kernel may underestimate the true SDC vulnerability. We refer
the resolution of the problem in GPU kernels to future work.
VII. CASE STUDY 2: COMPARISON WITH GPU-TRIDENT
In this section, we conduct a case study to compare DRUTO
with a state-of-the-art SDC modeling technique called GPU-
Trident [21]. GPU-Trident is a recent analytical modeling
approach that does not require fault injections to predict the
SDC probability of GPU kernels. So, its core heuristics-based
kernel SDC prediction under a given input can be used to
guide the search in a similar setup of DRUTO. GPU-Trident
capitalizes common execution patterns in GPU kernels, opti-
mizing performance by pruning the state space. However, it’s
worth noting that GPU-Trident’s design is primarily focused
on single program input, and its applicability to bounding SDC
probability across multiple inputs remains an open question.
To ensure a fair comparison between GPU-Trident and our
tool DRUTO, we employ GPU-Trident to predict SDC proba-
bility for the target program with a given input. Essentially, we
replace our heuristics in the ﬁtness function (Equation 1) with
GPU-Trident’s predicted SDC probability to guide the GA
search accordingly. All other GA parameters and components
remain unchanged for an end-to-end comparison. For each
generation g up to 100 generations, we measure the time
taken by DRUTO, denoted as DRUTOT g, to ﬁnd an SDC-
bound input IA. Then, using the same time limit DRUTOT g,
we use GPU-Trident to guide GA in searching for SDC-bound
input IB. Finally, we compare the measured SDC probabilities
obtained from FI results under the SDC-bound inputs IA
and IB respectively. We execute GPU-Trident following the
information provided in their repository [56] and paper [21].
Figure 9 shows the results. The x-axis of each sub-ﬁgure
denotes the number of generations in GA; the y-axis shows
the measured SDC probabilities under the SDC-bound inputs
obtained from DRUTO and GPU-Trident. Note that we are
unable to make GPU-Trident work with FFT, Backprop, and
Jmeint, so we exclude them in the comparison. In Figure 9,
it is clear that GPU-Trident cannot upper-bound the SDC
probability as high as of that bounded by DRUTO in most
cases. The reason is that the design of GPU-Trident is based on
only single program input, assuming the SDC characteristics
are similar across different inputs (Section V-A(3) in their
paper [21]). Consequently, their technique is insensitive to di-
verse program inputs, leading to much worse results. However,
GPU-Trident demonstrates slightly better results for kernels
such as BlackScholes. The key reason can be explained with
Figure 1. More speciﬁcally, the range of SDC probability in
BlackScholes is only 0.40%, which is a very narrow range.
In contrast, other kernels such as 2DCONV has a range of
52.70%, which is a very wide range. Therefore, even a small
noise can exacerbate the SDC bounding for kernels such as
BlackScholes during the fuzzing.
Moreover, on average, across all kernels, GPU-Trident takes
about 15 seconds to evaluate a single input, which is about
3× higher than DRUTO because of GPU-Trident’s higher
execution overhead with larger program inputs due to tracing
large memory dependency graph. Consequently, the state-
of-the-art modeling approach cannot replace the proposed
heuristics in DRUTO. Therefore, it is imperative to have an
automated, efﬁcient, and precise technique such as DRUTO.
VIII. RELATED WORKS
Program SDC resiliency: Numerous studies have explored
program-level SDC resilience [15], [55], [57]–[59]. Lu et
al. [55] introduced SDCTune, a model using error propagation
heuristics to estimate SDC probabilities efﬁciently. Hari et
al. [32] proposed pruning techniques based on error propaga-
tion patterns to reduce FI campaigns for CPU programs. Nie
et al. [15] developed progressive pruning techniques for GPU
591
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 11 ---
0%
10%
20%
30%
40%
50%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(a) Pathﬁnder
0%
2%
4%
6%
8%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(b) Needle-K1
0%
2%
4%
6%
8%
10%
12%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(c) Needle-K2
0%
15%
30%
45%
60%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(d) Particleﬁlter
0%
10%
20%
30%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(e) BFS-K1
0%
10%
20%
30%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(f) BFS-K2
0.0%
0.4%
0.8%
1.2%
1.6%
 10  25  40  55  70  85  100
DRUTO
GPU-Trident
(g) BlackScholes
0%
20%
40%
60%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(h) 2DCONV
0%
20%
40%
60%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(i) GEMM
0%
20%
40%
60%
80%
 10
 25
 40
 55
 70
 85  100
DRUTO
GPU-Trident
(j) MVT-K1
Fig. 9: The Comparison of Upper-Bounding SDC Probabilities between DRUTO and GPU-TRIDENT. X-axis and Y-axis
Correspond to Number of Gen. in GA and Bounded Kernel SDC Probability by Different Techniques respectively.
programs, considering inter-thread resilience properties. An-
wer et al. [21] devised an analytical model for GPU programs
to estimate SDC probability without requiring FIs. While
these studies contribute signiﬁcantly to the understanding of
program-level SDC resilience, they are limited to analyzing
SDC characteristics with single program input.
SDC variation across multiple program inputs: Some recent
works have considered multiple inputs when studying SDC
resilience. The closest related study to this work is PEPPA-
X [25]. The authors proposed a technique that bounds the
overall SDC probability across multiple inputs in CPU pro-
grams. In this paper, we show that the assumptions made
by PEPPA-X no longer holds for GPU programs, thereby the
effectiveness of PEPPA-X suffers in GPU programs. On the
other hand, Previlon et al. [26] conducted a comprehensive
analysis to show that GPU resiliency could vary signiﬁcantly
when the program inputs are changed. Yang et al. [14]
developed SUGAR based on the observation that analyzing a
small amount of inputs are sufﬁcient to extrapolate the overall
GPU kernel resilience across input space. While these studies
are insightful, none of the existing works proposed an efﬁcient
technique that upper-bounds the SDC probability in GPUs.
IX. CONCLUSION
We
propose
an
automated
compiler-based
technique,
DRUTO, which can efﬁciently ﬁnd inputs that bound GPU
kernel SDC probabilities. Our main observation to design
DRUTO is that we can rank inputs of a GPU kernel to
approximate their SDC probabilities by extracting kernel-
level characteristics obtained from resilience characteristics of
small number of representative threads. Because of our careful
insights, DRUTO does not need any FI during the search, while
other existing techniques still need a reasonable amount of FIs.
The results of our experiments show that DRUTO can bound
GPU kernel SDC probabilities that existing techniques cannot
bound even with 5x more search time.
ACKNOWLEDGMENTS
This research was supported by the U.S. Department of
Energy, Ofﬁce of Science, Advanced Scientiﬁc Computing
Research (ASCR), under contract DE-AC02-06CH11357, DE-
SC0024207, and DE-SC0024559. We acknowledge Argonne
Leadership Computing Facility for providing the Mira System
Log. Shengjian Guo participated in this study while he was in
Baidu USA, California.
REFERENCES
[1] B. Shan, A. Shamji, J. Tian, G. Li, and D. Tao, “Lcﬁ: A fault injection
tool for studying lossy compression error propagation in hpc programs,”
in 2020 IEEE International Conference on Big Data (Big Data). IEEE,
2020, pp. 2708–2715.
[2] S. Li, S. Di, K. Zhao, X. Liang, Z. Chen, and F. Cappello, “Towards
end-to-end sdc detection for hpc applications equipped with lossy com-
pression,” in 2020 IEEE International Conference on Cluster Computing
(CLUSTER).
IEEE, 2020, pp. 326–336.
[3] M. H. Rahman, S. Di, K. Zhao, R. Underwood, G. Li, and F. Cappello,
“A feature-driven ﬁxed-ratio lossy compression framework for real-
world scientiﬁc datasets,” in 2023 IEEE 39th International Conference
on Data Engineering (ICDE).
IEEE, 2023, pp. 1461–1474.
[4] H. D. Dixit, S. Pendharkar, M. Beadon, C. Mason, T. Chakravarthy,
B. Muthiah, and S. Sankar, “Silent data corruptions at scale,” arXiv
preprint arXiv:2102.11245, 2021.
[5] S. Venkatesha and R. Parthasarathi, “Design of low-cost reliable and
fault-tolerant 32-bit one instruction core for multi-core systems,” 2022.
[6] M. Snir, R. W. Wisniewski, J. A. Abraham, S. V. Adve, S. Bagchi,
P. Balaji, J. Belak, P. Bose, F. Cappello, B. Carlson et al., “Addressing
failures in exascale computing,” The International Journal of High
Performance Computing Applications, vol. 28, no. 2, pp. 129–173, 2014.
[7] S. Jha, S. Banerjee, T. Tsai, S. K. Hari, M. B. Sullivan, Z. T.
Kalbarczyk, S. W. Keckler, and R. K. Iyer, “Ml-based fault injection
for autonomous vehicles: A case for bayesian fault injection,” in 2019
49th annual IEEE/IFIP international conference on dependable systems
and networks (DSN).
IEEE, 2019, pp. 112–124.
[8] Y. Ibrahim, H. Wang, J. Liu, J. Wei, L. Chen, P. Rech, K. Adam, and
G. Guo, “Soft errors in dnn accelerators: A comprehensive review,”
Microelectronics Reliability, vol. 115, p. 113969, 2020.
[9] A. Nappa, C. Hobbs, and A. Lanzi, “Deja-vu: A glimpse on radioactive
soft-error consequences on classical and quantum computations,” arXiv
preprint arXiv:2105.05103, 2021.
[10] F. Kastensmidt and P. Rech, “Radiation effects and fault tolerance
techniques for fpgas and gpus,” FPGAs and Parallel Architectures for
Aerospace Applications: Soft Errors and Fault-Tolerant Design, pp. 3–
17, 2016.
592
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 12 ---
[11] Y. You, A. Buluc¸, and J. Demmel, “Scaling deep learning on gpu and
knights landing clusters,” in Proceedings of the International Conference
for High Performance Computing, Networking, Storage and Analysis,
2017, pp. 1–12.
[12] K. Adam, I. I. Mohd, and Y. M. Younis, “The impact of the soft errors in
convolutional neural network on gpus: Alexnet as case study,” Procedia
Computer Science, vol. 182, pp. 89–94, 2021.
[13] J. Tan, N. Goswami, T. Li, and X. Fu, “Analyzing soft-error vulnerability
on gpgpu microarchitecture,” in 2011 IEEE International Symposium on
Workload Characterization (IISWC).
IEEE, 2011, pp. 226–235.
[14] L. Yang, B. Nie, A. Jog, and E. Smirni, “Sugar: Speeding up gpgpu
application resilience estimation with input sizing,” Proceedings of the
ACM on Measurement and Analysis of Computing Systems, vol. 5, no. 1,
pp. 1–29, 2021.
[15] B. Nie, L. Yang, A. Jog, and E. Smirni, “Fault site pruning for
practical reliability analysis of gpgpu applications,” in 2018 51st Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO).
IEEE, 2018, pp. 749–761.
[16] F. Previlon, C. Kalra, D. Tiwari, and D. Kaeli, “Characterizing and
exploiting soft error vulnerability phase behavior in gpu applications,”
IEEE Transactions on Dependable and Secure Computing, vol. 19, no. 1,
pp. 288–300, 2020.
[17] C. Chen, G. Eisenhauer, M. Wolf, and S. Pande, “Ladr: Low-cost
application-level detector for reducing silent output corruptions,” in
Proceedings of the 27th International Symposium on High-Performance
Parallel and Distributed Computing, 2018, pp. 156–167.
[18] D. Li, Z. Chen, P. Wu, and J. S. Vetter, “Rethinking algorithm-based fault
tolerance with a cooperative software-hardware approach,” in SC’13:
Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis.
IEEE, 2013, pp. 1–
12.
[19] G. Li, K. Pattabiraman, C.-Y. Cher, and P. Bose, “Understanding
error propagation in GPGPU applications,” in International Conference
for High Performance Computing, Networking, Storage and Analysis.
IEEE, 2016, pp. 240–251.
[20] G. Li and K. Pattabiraman, “Modeling input-dependent error propagation
in programs,” in 2018 48th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks (DSN).
IEEE, 2018, pp. 279–
290.
[21] A. R. Anwer, G. Li, K. Pattabiraman, M. Sullivan, T. Tsai, and S. K. S.
Hari, “Gpu-trident: efﬁcient modeling of error propagation in gpu
programs,” in SC20: International Conference for High Performance
Computing, Networking, Storage and Analysis.
IEEE, 2020, pp. 1–15.
[22] H. Yue, X. Wei, G. Li, J. Zhao, N. Jiang, and J. Tan, “G-sepm: building
an accurate and efﬁcient soft error prediction model for gpgpus,” in
Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, 2021, pp. 1–15.
[23] Y. Huang, S. Guo, S. Di, G. Li, and F. Cappello, “Mitigating silent
data corruptions in hpc applications across multiple program inputs,”
in Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis, 2022, pp. 1–14.
[24] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the
accuracy of high-level fault injection techniques for hardware faults,” in
2014 44th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks.
IEEE, 2014, pp. 375–382.
[25] M. H. Rahman, A. Shamji, S. Guo, and G. Li, “Peppa-x: ﬁnding
program test inputs to bound silent data corruption vulnerability in hpc
applications,” in Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis, 2021, pp.
1–13.
[26] F. G. Previlon, C. Kalra, D. R. Kaeli, and P. Rech, “A comprehensive
evaluation of the effects of input data on the resilience of gpu appli-
cations,” in 2019 IEEE International Symposium on Defect and Fault
Tolerance in VLSI and Nanotechnology Systems (DFT).
IEEE, 2019,
pp. 1–6.
[27] “Summary of the Amazon S3 Service Disruption in the Northern Vir-
ginia (US-EAST-1) Region,” https://aws.amazon.com/message/41926/,
[Online].
[28] S. Jha, S. Cui, T. Tsai, S. K. S. Hari, M. B. Sullivan, Z. T. Kalbarczyk,
S. W. Keckler, and R. K. Iyer, “Exploiting temporal data diversity for
detecting safety-critical faults in av compute systems,” in 2022 52nd
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN).
IEEE, 2022, pp. 88–100.
[29] P. H. Hochschild, P. Turner, J. C. Mogul, R. Govindaraju, P. Ran-
ganathan, D. E. Culler, and A. Vahdat, “Cores that don’t count,” in
Proceedings of the Workshop on Hot Topics in Operating Systems, 2021,
pp. 9–16.
[30] S. K. S. Hari, S. V. Adve, and H. Naeimi, “Low-cost program-
level detectors for reducing silent data corruptions,” in International
Conference on Dependable Systems and Networks (DSN).
IEEE, 2012,
pp. 1–12.
[31] J. Wei, A. Thomas, G. Li, and K. Pattabiraman, “Quantifying the
accuracy of high-level fault injection techniques for hardware faults,”
in 44th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN).
IEEE, 2014, pp. 375–382.
[32] S. K. S. Hari, S. V. Adve, H. Naeimi, and P. Ramachandran, “Relyzer:
exploiting application-level fault equivalence to analyze application
resiliency to transient faults,” in ACM SIGARCH Computer Architecture
News, vol. 40, no. 1.
ACM, 2012, p. 123.
[33] S. Laskar, M. H. Rahman, B. Zhang, and G. Li, “Characterizing deep
learning neural network failures between algorithmic inaccuracy and
transient hardware faults,” in 2022 IEEE 27th Paciﬁc Rim International
Symposium on Dependable Computing (PRDC).
IEEE, 2022, pp. 54–
67.
[34] S. Laskar, M. H. Rahman, and G. Li, “Tensorﬁ+: A scalable fault
injection framework for modern deep learning neural networks,” in 2022
IEEE International Symposium on Software Reliability Engineering
Workshops (ISSREW).
IEEE, 2022, pp. 246–251.
[35] M. H. Rahman, S. Laskar, and G. Li, “Investigating the impact of
transient hardware faults on deep learning neural network inference,”
Software Testing, Veriﬁcation and Reliability, p. e1873, 2024.
[36] C.-K. Chang, G. Li, and M. Erez, “Evaluating compiler ir-level se-
lective instruction duplication with realistic hardware errors,” in 2019
IEEE/ACM 9th Workshop on Fault Tolerance for HPC at eXtreme Scale
(FTXS).
IEEE, 2019, pp. 41–49.
[37] G. Georgakoudis, I. Laguna, D. S. Nikolopoulos, and M. Schulz,
“Reﬁne: Realistic fault injection via compiler-based instrumentation for
accuracy, portability and speed,” in Proceedings of the International
Conference for High Performance Computing, Networking, Storage and
Analysis, 2017, pp. 1–14.
[38] Guanpeng Li, Karthik Pattabiraman, Siva Kumar Sastry Hari, Michael
Sullivan and Timothy Tsai, “Modeling soft-error propagation in pro-
grams,” in IEEE/IFIP International Conference on Dependable Systems
and Networks (DSN).
IEEE, 2018.
[39] J. J. Cook and C. Zilles, “A characterization of instruction-level error
derating and its implications for error detection,” in 2008 IEEE Inter-
national Conference on Dependable Systems and Networks With FTCS
and DCC (DSN).
IEEE, 2008, pp. 482–491.
[40] C. Kalra, F. Previlon, N. Rubin, and D. Kaeli, “Armorall: Compiler-based
resilience targeting gpu applications,” ACM Transactions on Architecture
and Code Optimization (TACO), vol. 17, no. 2, pp. 1–24, 2020.
[41] R. A. Ashraf, R. Gioiosa, G. Kestor, R. F. DeMara, C.-Y. Cher, and
P. Bose, “Understanding the propagation of transient errors in hpc
applications,” in Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis.
ACM,
2015, p. 72.
[42] N. Oh, P. P. Shirvani, and E. J. McCluskey, “Control-ﬂow checking by
software signatures,” IEEE transactions on Reliability, vol. 51, no. 1,
pp. 111–122, 2002.
[43] Y. Xia, Y. Liu, H. Chen, and B. Zang, “Cﬁmon: Detecting violation
of control ﬂow integrity using performance counters,” in IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN
2012).
IEEE, 2012, pp. 1–12.
[44] S. Feng, S. Gupta, A. Ansari, and S. Mahlke, “Shoestring: probabilistic
soft error reliability on the cheap,” in ACM SIGARCH Computer
Architecture News, vol. 38, no. 1.
ACM, 2010, p. 385.
[45] B. Sangchoolie, K. Pattabiraman, and J. Karlsson, “One bit is (not)
enough: An empirical study of the impact of single and multiple bit-
ﬂip errors,” in 2017 47th annual IEEE/IFIP international conference on
dependable systems and networks (DSN).
IEEE, 2017, pp. 97–108.
[46] H. Cho and K.-W. Kwon, “Modeling application-level soft error effects
for single-event multi-bit upsets,” IEEE Access, vol. 7, pp. 133 485–
133 495, 2019.
[47] L. Yang, B. Nie, A. Jog, and E. Smirni, “Practical resilience analysis of
gpgpu applications in the presence of single-and multi-bit faults,” IEEE
Transactions on Computers, vol. 70, no. 1, pp. 30–44, 2020.
593
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 


--- Page 13 ---
[48] K. Ito, Y. Zhang, H. Itsuji, T. Uezono, T. Toba, and M. Hashimoto,
“Analyzing due errors on gpus with neutron irradiation test and fault
injection to control ﬂow,” IEEE Transactions on Nuclear Science,
vol. 68, no. 8, pp. 1668–1674, 2021.
[49] M. Kaliorakis, S. Tselonis, A. Chatzidimitriou, N. Foutris, and D. Gi-
zopoulos, “Differential fault injection on microarchitectural simulators,”
in 2015 IEEE International Symposium on Workload Characterization.
IEEE, 2015, pp. 172–182.
[50] C.-K. Chang, S. Lym, N. Kelly, M. B. Sullivan, and M. Erez, “Eval-
uating and accelerating high-ﬁdelity error injection for hpc,” in SC18:
International Conference for High Performance Computing, Networking,
Storage and Analysis.
IEEE, 2018, pp. 577–589.
[51] J. H. Holland, Adaptation in natural and artiﬁcial systems: an intro-
ductory analysis with applications to biology, control, and artiﬁcial
intelligence.
MIT press, 1992.
[52] J. J. Grefenstette, “Optimization of control parameters for genetic algo-
rithms,” IEEE Transactions on systems, man, and cybernetics, vol. 16,
no. 1, pp. 122–128, 1986.
[53] J. D. Schaffer, R. Caruana, L. J. Eshelman, and R. Das, “A study of
control parameters affecting online performance of genetic algorithms
for function optimization,” in Proceedings of the 3rd international
conference on genetic algorithms, 1989, pp. 51–60.
[54] R. L. Haupt, “Optimum population size and mutation rate for a simple
real genetic algorithm that optimizes array factors,” in IEEE Antennas
and Propagation Society International Symposium. Transmitting Waves
of Progress to the Next Millennium. 2000 Digest. Held in conjunction
with: USNC/URSI National Radio Science Meeting (C, vol. 2.
IEEE,
2000, pp. 1034–1037.
[55] Q. Lu, K. Pattabiraman, M. S. Gupta, and J. A. Rivers, “Sdctune: a
model for predicting the sdc proneness of an application for conﬁgurable
protection,” in Proceedings of the 2014 International Conference on
Compilers, Architecture and Synthesis for Embedded Systems, 2014, pp.
1–10.
[56] “GPU-Trident
Artifacts,”
https://github.com/DependableSystemsLab/
GPU-Trident, [Online].
[57] Q. Guan, N. Debardeleben, S. Blanchard, and S. Fu, “F-seﬁ: A ﬁne-
grained soft error fault injection tool for proﬁling application vul-
nerability,” in 2014 IEEE 28th International Parallel and Distributed
Processing Symposium.
IEEE, 2014, pp. 1245–1254.
[58] N. DeBardeleben, S. Blanchard, Q. Guan, Z. Zhang, and S. Fu, “Ex-
perimental framework for injecting logic errors in a virtual machine to
proﬁle applications for soft error resilience,” in Euro-Par 2011: Paral-
lel Processing Workshops: CCPI, CGWS, HeteroPar, HiBB, HPCVirt,
HPPC, HPSS, MDGS, ProPer, Resilience, UCHPC, VHPC, Bordeaux,
France, August 29–September 2, 2011, Revised Selected Papers, Part II
17.
Springer, 2012, pp. 282–291.
[59] Q. Guan, N. DeBardeleben, S. Blanchard, and S. Fu, “Empirical studies
of the soft error susceptibility of sorting algorithms to statistical fault
injection,” in Proceedings of the 5th Workshop on Fault Tolerance for
HPC at eXtreme Scale, 2015, pp. 35–40.
594
Authorized licensed use limited to: University of Florida. Downloaded on December 29,2025 at 23:06:47 UTC from IEEE Xplore.  Restrictions apply. 
