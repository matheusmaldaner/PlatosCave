{
  "nodes": [
    {
      "id": 0,
      "text": "For high-stakes decisions, inherently interpretable machine learning models should be used or preferred to posthoc explanations of black-box models because they enable better troubleshooting, safer deployment, and allow humans to decide whether to trust predictions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 1,
      "text": "Interpretable ML models are models that obey domain-specific constraints (e.g., sparsity, monotonicity, decomposability, generative constraints) to make a model or its predictions more understandable to humans",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 2,
      "text": "Interpretability does not automatically create trust; interpretable models allow users to decide whether to trust them and can also enable distrust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "There is no general scientific evidence of a universal accuracy-versus-interpretability tradeoff when considering the full data-science process; interpretability can improve accuracy via better troubleshooting",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "Interpretability and performance metrics should be iteratively redefined and reﬁned during the full knowledge-discovery/data-science process with domain expert feedback",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "For high-stakes decisions, interpretable models should be preferred to explained black boxes because posthoc explanations are often misleading, unreliable, or provide false authority",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "A general constrained optimization formulation for interpretable supervised learning: minimize empirical loss plus a weighted interpretability penalty subject to interpretability constraints (loss + C*InterpretabilityPenalty subject to InterpretabilityConstraint)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Empirical and case evidence shows black-box failures in practice (e.g., COMPAS complexities and typographical errors, medical imaging saliency map unreliability, black-box domain-shift and fairness issues), motivating inherently interpretable models for safety and accountability",
      "role": "Evidence",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Challenge cluster A: Sparse logical and linear interpretable models (decision trees, decision lists/sets, and scoring systems) require global optimization for sparsity and accuracy but face NP-hardness and scalability, handling continuous variables, and incorporating domain constraints",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Challenge cluster B: Additive and case-based interpretable models (generalized additive models and modern case-based/prototype methods) enable visualization and human-like reasoning but need better control of smoothness/sparsity, troubleshooting for noisy complex datasets, prototype supervision, part-based prototypes and extensions to sequences/video",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Challenge cluster C: Disentanglement and interpretable latent representations — supervised disentanglement aims to align neurons with human concepts but must scale to whole networks, choose concepts and map concepts to outputs; unsupervised disentanglement aims to discover meaningful factors but lacks evaluation metrics and general architectures for complex real-world data",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Challenge cluster D: Tools for visualization, physics/causal constraints, Rashomon-set analysis, and interpretable reinforcement learning are needed: (a) dimension-reduction for trustworthy visualization and hyperparameter selection, (b) physics-informed and constraint-infused ML (PINNs) with improved training and experimental design integration, (c) methods to characterize and explore the Rashomon set of equally-good models, and (d) inherently interpretable policies for RL",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    }
  ]
}