{
  "nodes": [
    {
      "id": 0,
      "text": "For high-stakes decisions, machine learning models should be inherently interpretable (designed with domain-specific constraints) because interpretable models enable effective troubleshooting, safer deployment, and do not necessarily sacrifice accuracy compared to black box models",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13
      ]
    },
    {
      "id": 1,
      "text": "Principle 1: An interpretable ML model obeys domain-specific constraints (soft penalties or hard constraints) so its predictions or reasoning are more understandable to humans",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        2
      ]
    },
    {
      "id": 2,
      "text": "Principle 2: Interpretable models do not automatically create trust but enable users to decide whether to trust them; they allow assessment of trustworthiness rather than providing trust itself",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "Principle 3: There is no scientific evidence of a general tradeoff between accuracy and interpretability over the full data science process; interpretability often promotes accuracy by enabling troubleshooting",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 4,
      "text": "Principle 4: In the full data science process both performance metrics and interpretability metrics should be iteratively reﬁned, using domain expert feedback to shape interpretability constraints",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Principle 5: For high-stakes decisions prefer interpretable models when possible rather than posthoc explanations of black boxes, because explanations can be misleading or insufficient",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6
      ]
    },
    {
      "id": 6,
      "text": "Evidence: Posthoc explanations for black boxes (e.g., saliency maps, LIME, SHAP) can be unreliable or misleading and can give false authority to black boxes, hurting safety in high-stakes settings",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Challenge summary: Ten grand technical challenges in interpretable ML include optimization of sparse logical models, scoring systems, constrained GAMs, modern case-based reasoning, supervised and unsupervised disentanglement, dimension reduction for visualization, physics/causal constraints, characterizing the Rashomon set, and interpretable reinforcement learning",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16
      ]
    },
    {
      "id": 8,
      "text": "Challenge 1 (Claim/Method): Optimize sparse logical models (decision trees, decision lists, decision sets) with global constraints and scalability, because greedy heuristics can produce suboptimal and less interpretable trees",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Challenge 2 (Claim/Method): Create optimized scoring systems (sparse integer-coefﬁcient linear models) that balance predictive performance, sparsity, integer constraints, and user constraints without naive rounding",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Challenge 3 (Claim/Method): Design generalized additive models (GAMs) with controllable sparsity, smoothness, monotonicity and user-deﬁned constraints to improve interpretability and troubleshooting of complex tabular and clinical datasets",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Challenge 4 (Claim/Method): Modern case-based reasoning and prototype / part-based prototype networks provide interpretable decisions for raw data (images, sequences) by comparing test cases to prototypical training cases or parts",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Challenge 5 (Claim/Method): Supervised disentanglement: align neurons or latent axes with human-deﬁned concepts so each concept's signal travels through designated pathways, enabling intervenability and interpretability",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Challenge 6 (Claim/Method): Unsupervised disentanglement: discover human-interpretable latent factors (concepts) without labels for domains where concepts are unknown or labels are biased, e.g., materials science or complex scenes",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Challenge 7 (Claim/Method): Dimension reduction for visualization: develop DR methods that reliably preserve local and global structure, reduce sensitivity to hyperparameters, and provide interpretable mappings to facilitate data understanding and avoid misleading projections",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Challenge 8 (Claim/Method): Incorporate physics, generative, or causal constraints (e.g., PDE/ODEs via physics-informed neural networks) into ML models to produce interpretable, data-efﬁcient, and physically consistent predictions and surrogates",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 16,
      "text": "Challenge 9 (Claim/Method): Characterize and explore the Rashomon set (the set of almost-equally-good models) to identify interpretable models within it, measure its size, visualize it, and enable selection of models with desirable properties (fairness, sparsity)",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 17,
      "text": "Challenge 10 (Claim/Method): Develop interpretable reinforcement learning methods (interpretable policies/value functions, constrained state representations, hierarchical/relational policies) because RL decisions have long-term consequences and current deep RL is largely opaque",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 18,
      "text": "Evidence: Practical harms and failures of black box models (e.g., COMPAS, clinical black boxes, air-quality models) illustrate that opacity complicates troubleshooting, leads to errors from input noise/typos, and impedes accountability in high-stakes domains",
      "role": "Evidence",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 19,
      "text": "Evidence/Result: The Rashomon effect implies that many problems admit a large set of models with near-minimal loss; when the Rashomon set is large there is increased likelihood that a simple interpretable model exists with comparable accuracy",
      "role": "Result",
      "parents": [
        16
      ],
      "children": null
    },
    {
      "id": 20,
      "text": "Assumption: Interpretability is domain-dependent; appropriate interpretability constraints (e.g., sparsity, monotonicity, decomposability, case-based reasoning, disentanglement, physics) must be chosen per domain and task",
      "role": "Assumption",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 21,
      "text": "Limitation: Optimization for interpretable models can be computationally hard (NP-complete for optimal trees) and scaling exact methods (MIP, dynamic programming) to large datasets or many continuous variables remains an open challenge",
      "role": "Limitation",
      "parents": [
        8,
        9
      ],
      "children": null
    }
  ]
}