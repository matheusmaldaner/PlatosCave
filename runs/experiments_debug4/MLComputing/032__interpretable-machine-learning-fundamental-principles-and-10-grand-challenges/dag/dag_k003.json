{
  "nodes": [
    {
      "id": 0,
      "text": "Interpretable machine learning models should be preferred for high-stakes decisions because they enable safer, more actionable, and more trustworthy decision-making without necessarily sacrificing predictive accuracy",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15
      ]
    },
    {
      "id": 1,
      "text": "Principle 1: An interpretable ML model obeys domain-specific constraints so model, predictions, or data are more easily understood by humans",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 2,
      "text": "Principle 2: Interpretable models do not automatically create trust but allow users to decide whether to trust them",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "Principle 3: There is no general scientific tradeoff between interpretability and accuracy; interpretability often improves accuracy across the full data science process",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "Principle 4: Performance and interpretability metrics should be iteratively refined during the full data science process",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Principle 5: For high-stakes decisions prefer inherently interpretable models over post-hoc explanations of black boxes",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Interpretable modeling can be formulated as constrained optimization: minimize loss plus interpretability penalty subject to interpretability constraints (general formulation for supervised and unsupervised problems)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Evidence from real-world cases (COMPAS, medical imaging, air-quality models) shows black box models can harm people, be unreliable under domain shift, be hard to troubleshoot, and sometimes be no more accurate than much simpler interpretable models",
      "role": "Evidence",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Claim: Post-hoc explanations for black boxes (local approximations, saliency maps, SHAP/LIME) are often misleading, unreliable, or insufficient for high-stakes use and can increase misplaced trust",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Claim: The Rashomon effect implies that many problems admit a large set of nearly-equally-accurate models (the Rashomon set), and a large Rashomon set increases the likelihood that a simple interpretable model exists with competitive accuracy",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Survey claim: There are ten central technical challenge areas in interpretable ML including optimal sparse logical models, scoring systems, constrained GAMs, modern case-based reasoning, supervised and unsupervised disentanglement, dimension reduction for visualization, physics/causal constraints, Rashomon characterization, and interpretable reinforcement learning",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Challenge (Decision trees and logical models): Optimizing sparse logical models is computationally hard (NP-complete); advances include MIP, SAT, stochastic search, and branch-and-bound dynamic programming, but scalability, handling continuous variables, and global constraints remain open problems",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Challenge (Scoring systems): Creating small integer-coefficient scoring systems requires discrete optimization; exact MIP approaches (e.g., RiskSLIM) can produce clinically useful risk scores but face scalability and constraint-elicitation challenges compared with rounding or approximation heuristics",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Challenge (Modern case-based reasoning): Prototype and nearest-neighbor approaches (including deep prototypes and part-based prototypes) yield interpretable models for raw and tabular data; open issues include human-in-the-loop prototype curation, extending to video/sequence data, and systematic prototype troubleshooting/replacement",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Challenge (Disentanglement): Supervised disentanglement can align latent axes with human concepts and enables interventions, but challenges include disentangling entire networks, selecting concept hierarchies and continuous concepts, and making the mapping from concepts to outputs interpretable",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Challenge (Unsupervised disentanglement, DR, physics, RL & Rashomon): Important unsupervised problems include discovering concepts without labels, reliable dimension-reduction for visualization (hyperparameter tuning, local/global structure), physics-informed ML (PINNs) training and experiment design, and interpretable reinforcement learning; additionally, characterizing and visualizing the Rashomon set is essential for selecting interpretable models",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    }
  ]
}