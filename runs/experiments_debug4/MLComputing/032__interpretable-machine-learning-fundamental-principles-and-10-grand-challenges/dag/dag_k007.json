{
  "nodes": [
    {
      "id": 0,
      "text": "Interpretable machine learning (ML) models should be designed and preferred for high-stakes decisions and troubleshooting, guided by fundamental principles and prioritized research on ten grand technical challenges",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1
      ]
    },
    {
      "id": 1,
      "text": "Fundamental principles: (1) interpretable ML = domain-specific constraints that make models or predictions easier for humans to understand; (2) interpretability permits users to decide trust; (3) no general accuracy-interpretability tradeoff across the full data science process; (4) interpretability and performance metrics should be iteratively reﬁned in the data science loop; (5) for high-stakes decisions prefer inherently interpretable models over post-hoc explanations of black boxes",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 2,
      "text": "Challenge 1: Sparse logical models (decision trees, decision lists, decision sets) require optimization approaches (MIP, SAT, branch-and-bound, dynamic programming, stochastic search) to jointly maximize accuracy and sparsity; open issues include scalability, handling continuous variables and enforcing global constraints",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "Challenge 2: Scoring systems (sparse linear integer-coef models for medicine and justice) need optimization formulations (e.g., mixed-integer programs, RiskSLIM) or improved approximations/rounding and interactive tools to produce accurate, constrained, integer scoring rules; key problems are scaling, discretization of continuous covariates, and constraint elicitation",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "Challenge 3: Generalized additive models (GAMs) offer interpretable component functions for continuous features; research needs include controlling sparsity, smoothness and monotonicity of components and using GAMs to troubleshoot complex, biased medical datasets",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Challenge 4: Modern case-based reasoning (nearest-neighbor, prototype, part-based prototypes, deep prototype networks and ProtoPNet) enables interpretable predictions for raw and tabular data by comparing to exemplars or prototypical parts; open problems include video/sequential extensions, human-in-the-loop prototype supervision, and prototype troubleshooting/replacement without full retraining",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Challenge 5: Supervised disentanglement of neural networks aims to align neurons or latent axes with user-speciﬁed concepts (concept whitening, concept classiﬁers) so concepts are 'pure' and manipulable; open problems include making whole layers interpretable, disentangling all layers hierarchically, selecting useful concepts (including continuous concepts), and making the concept-to-output mapping interpretable",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Challenge 6: Unsupervised disentanglement seeks to discover human-relevant latent factors without labeled concepts using generative models, compositional architectures (capsules, slot attention) or other inductive biases; key issues are evaluation without ground truth, adapting compositional architectures across domains, and learning part-whole disentanglement at scale for realistic images and scientific data",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Challenge 7: Dimension reduction for visualization (PCA, MDS, t-SNE, UMAP, PaCMAP, TriMap) is an interpretable constraint mapping high-dimensional data to 2D/3D for human inspection; open problems are preserving both local and global structure, robust automatic hyperparameter selection, and explaining the mapping from original features to low-dimensional axes",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Challenge 8: Physics- or generative-constrained ML (e.g., physics-informed neural networks, PINNs, and Gaussian-process variants) embed differential or domain laws into learning for interpretable, data-efﬁcient models; open problems include PINN training pathologies, integrating experimental/simulation design to reduce uncertainty, and incorporating PDE or other constraints into non-differentiable/nonparametric models",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Challenge 9: Characterize and exploit the Rashomon set (the set of models within a small loss tolerance) to find simpler or fairer models; research needs include measuring Rashomon set size (volume, pattern ratio, variable-importance clouds), visualizing its structure, and interactive selection of preferred models from the set",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Challenge 10: Interpretable reinforcement learning requires policies and value functions that are transparent (e.g., decision trees, relational forms, hierarchical/skill decompositions); open questions include what constraints permit accurate interpretable policies, under what assumptions such policies exist, and how to simplify or interpret large/high-dimensional state representations",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Conclusion: For many real problems, especially high-stakes domains, inherently interpretable models are necessary and feasible; research advancing the ten challenges will enable trustworthy, accurate, and actionable ML that supports human oversight",
      "role": "Conclusion",
      "parents": [
        1
      ],
      "children": null
    }
  ]
}