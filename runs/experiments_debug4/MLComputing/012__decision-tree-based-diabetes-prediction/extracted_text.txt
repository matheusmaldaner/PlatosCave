--- Page 1 ---
Page 1
Decision Tree Based Diabetes Prediction
ABSTRACT: Data mining is the one of the major
field
used
in
various
areas
like
medicine,
education, engineering, science, and business. In
medicine data mining plays a major role of
prediction
of
various
kinds
of
diseases
like
diabetes, heart attack, sinusitis, cancer and so on.
Data
mining
is
mainly
used
to
find
hidden
information in database by creating a model with
the help of training set and test data. Diabetes is a
disease in which blood glucose levels are going
high. Mostly our food is turned as glucose or sugar
for our body that is converted as energy. The
pancreas, an organ makes a hormone known as
insulin to get glucose to our bodies. If a person is
affected by diabetes then he or she will not get
sufficient insulin for his body. This will lead to
increases sugar in your blood. Data Mining is
classified into predictive and descriptive in which
predictive
uses
Classification
and
prediction.
Prediction is used to predict the future whether the
patient is affected by the disease or not. Here we
use famous data mining classification technique
called Decision tree for prediction of diabetes. To
measure the impurity of the attribute we use
entropy and information gain to decide which
attribute to be the root node
Keywords - C4.5, Classification, Data mining,
Decision Tree, Diabetes, ID3, J48, Prediction.
1. INTRODUCTION
Data mining is the task of discovering interesting
patterns from large amounts of data, where the data
can be stored in databases, data ware houses, or
other information repositories. Classification and
prediction are two forms of data analysis that can
be used to extract models describing important data
classes
or
to
predict
future
data
trends.
Classification is the process of finding a model (or
function) that describes and distinguishes data
classes or concepts, for the purpose of being able to
use the model to predict the class of objects whose
class
label
is
unknown[11].
In
classification
learning, the learning scheme is presented with a
set of classified examples from which it is expected
to learn a way of classifying unseen examples. In
numeric prediction, the outcome to be predicted is
not a discrete class but a numeric quantity [10].
Classification
techniques
are
most
suited
for
predicting or describing data sets with binary or
nominal
categories.
Classifier
is
a
systematic
approach to build classification models from an
input data set [13]. The classification techniques
used here is decision tree classifier and it create a
model between attribute and class label. The main
objective of the model is to accurately predict the
class.J48 (C4.5) is an open source algorithm in
WEKA data mining tool. A decision tree can be
generated from the input data by C4.5 progamme.
It is an algorithm used to generate a decision tree
and is an extension of Quinlan’s earlier ID3
Algorithm. The decision trees generated by this can
be used for classification and so referred to as
statistical classifier [5].
2. LITERATURE REVIEW
Performance Analysis of NaiveBayes and
J48
Classification
Algorithm
for
Data
Classification,
Tina R. Patil, Mrs. S. S. Sherekar, Sant Gadgebaba
Amravati University, Amravati
J48 classifier is a simple C4.5 decision tree for
classification. It creates a binary tree. The decision
tree approach is most useful in classification
problem. With this technique, a tree is constructed
to model the classification process. Once the tree is
built, it is applied to each tuple in the database and
results in classification for that tuple.
A Data Mining Approach for the Diagnosis
of Diabetes Mellitus
SonuKumari, IT, CPJCHS, Delhi AlIT Archana
Singh, Amity University, Noida, U.P
Diabetes is a chronic disease that occurs when the
pancreas does not produce enough insulin, or when
the body cannot effectively use the insulin it
produces. Hyperglycemia, or raised blood sugar, is
a common effect of uncontrolled Diabetes and over
time leads to serious damage to many of the body's
systems, especially the nerves and blood vessels.
Decision Support System for Medical
Diagnosis Using Data Mining,
D.Senthil Kumar, G.Sathyadevi and S.Sivanesh
A
distinguished
confusion
matrix
(sometimes
called contingency table) is obtained to calculate
the four measures. Confusion matrix is a matrix
representation
of
the
classification
results.
It
contains information about actual and predicted
classifications done by a classification system. The
cell which denotes the number of samples classifies
as true while they were true (i.e., TP), and the cell
that denotes the number of samples classified as
false while they were actually false (i.e., TN). The
other two cells denote the number of samples


--- Page 2 ---
Page 2
misclassified. Specifically, the cell denoting the
number of samples classified as false while they
actually were true (i.e., FN), and the cell denoting
the number of samples classified as true while they
actually were false (i.e., FP). Once the confusion
matrixes were constructed, the precision, recall, F -
measure are easily calculated
Implementation of ID3 Algorithm
Rupali Bhardwaj, Sonia Vatta
The order in which attributes are chosen determines
how complicated the tree is.ID3 uses information
theory to determine the most informative attribute.
A measure of the information content of a message
is the inverse of the probability of receiving the
message: Information1 (M) = 1/probability (M)
Taking logs (base 2) make information correspond
to the number of bits required to encode a message:
information (M) =-log2 (probability (M))
Information: The information content of a message
should be related to the degree of surprise in
receiving the message. Messages with a high
probability of arrival are not as informative as
messages with low probability. Learning aims to
predict accurately i.e. reduce surprise. Probabilities
are multiplied to get the probability of two or more
things both/all happening. Taking logarithms of the
probabilities
allows
information
to
be
added
instead of multiplied. Entropy: Different messages
have different probabilities of arrival. Overall level
of uncertainty (termed entropy) is: -Σi Pi log2Pi.
Frequency can be used as a probability estimate.
E.g. if there are 5 positive examples and 3 negative
examples in a node the estimated probability of
positive is 5/8 = 0.625.
3. PROPOSED SYSTEM
3.1Classification and Prediction
Classification is process of grouping together
documents or data that have similar properties or
are related [12]. Diabetes is a chronic condition
associated with abnormally high levels of sugar
(glucose) in the blood. When a person has diabetes,
their body either cannot make insulin or the insulin
that their body makes does not work properly. The
body needs insulin to function properly and convert
food to energy. In the absence of insulin or if the
insulin produced is unable to function properly, a
person’s blood glucose level will rise above normal
values. Data mining algorithm is used for testing
the accuracy in predicting diabetic status. We can
predict whether a new patient would test positive
for diabetes. Various parameters are collected from
patients and predict whether they affected by
diabetes.
In
this
study,
popular
data
mining
techniques,
prediction,
and
decision
tree
are
applied and compared to each other based on their
predictive accuracy on confusion matrix basis. The
Pima Indian diabetes database is a collection of
medical diagnostic reports of 768 examples from a
population living near Phoenix, Arizona, USA. The
Pima Indian Diabetes dataset available from the
UCI
Server
and
can
be
downloaded
at
www.ics.uci.edu/~mlearn/MLRepository.html [8].
The data source uses 768 samples with two class
problems to test whether the patient would test
positive or negative for diabetes. All the patients in
this database are Pima Indian women at least 21
years old. The database has 9 numeric variables:
1. Number of times pregnant
2. Plasma glucose concentration a 2 hours in an
oral glucose tolerance test
3. Diastolic blood pressure (mm Hg)
4. Triceps skin fold thickness (mm)
5. 2-Hour serum insulin (mu U/ml)
6. Body mass index (weight in kg/(height in m)^2)
7. Diabetes pedigree function
8. Age (years)
9. Class variable (0- Tested Negative or 1- Tested
Positive)
The database consists of two categories in the
dataset (i.e. Tested Positive, Tested Negative) each
having 9 features. Where '1' means a positive test
for diabetes and '0' is a negative test for diabetes.
There are 268 cases in class '1' and 500 cases in
class '0'. The aim is to use the first 8 variables to
predict 9.
3.2WEKA
(Waikato
Environment
for
Knowledge Analysis)
Weka was developed at the University of Waikato
in New Zealand. The system is written in Java and
distributed under the terms of the GNU General
Public
License.
Weka
is
available
from
http://www.cs.waikato.ac.nz/ml/ weka. The easiest
way to use Weka is through a graphical user
interface called the Explorer. This gives access to
all of its facilities using menu selection and form
filling. For example, you can quickly read in a
dataset from an ARFF file (or spreadsheet) and
build a decision tree from it. But learning decision
trees is just the beginning: there are many other
algorithms to explore. The Explorer interface helps
you do just that. There are two other graphical user
interfaces to Weka [10]. J48 [1] implements


--- Page 3 ---
Page 3
Quinlan s C4.5 algorithm ‟ [2] for generating a
pruned or unpruned C4.5 decision tree [9]. C4.5 is
an extension of Quinlan's earlier ID3 algorithm.
The decision trees generated by J48 can be used for
classification. J48 builds decision trees from a set
of labeled training data using the concept of
information entropy. It uses the fact that each
attribute of the data can be used to make a decision
by splitting the data into smaller subsets. J48
examines
the
normalized
information
gain
(difference in entropy) that results from choosing
an attribute for splitting the data. To make the
decision, the attribute with the highest normalized
information gain is used. Then the algorithm recurs
on the smaller subsets. The splitting procedure
stops if all instances in a subset belong to the same
class. Then a leaf node is created in the decision
tree telling to choose that class. But it can also
happen
that
none
of
the
features
give
any
information gain. In this case J48 creates a decision
node higher up in the tree using the expected value
of the class. J48 can handle both continuous and
discrete
attributes,
training
data
with
missing
attribute values and attributes with differing costs.
Further it provides an option for pruning trees after
creation. For further information, we refer to the
original publications [1].
4. METHODOLOGY
4.1 Decision Tree
Decision tree algorithms, such as ID3, C4.5, and
CART, were originally intended for classification.
Decision tree induction constructs a flow chart like
structure
where
each
internal
(nonleaf)
node
denotes
a
test
on
an
attribute,
each
branch
corresponds to an outcome of the test, and each
external (leaf) node denotes a class prediction. At
each node, the algorithm chooses the “best”
attribute to partition the data into individual
classes. When decision tree induction is used for
attribute subset selection, a tree is constructed from
the given data. All attributes that do not appear in
the tree are assumed to be irrelevant. The set subset
of
attributes
[11].
A
“divide-and-conquer”
approach to the problem of learning from a set of
independent instances leads naturally to a style of
representation called a decision tree. Nodes in a
decision tree involve testing a particular attribute.
The problem of constructing a decision tree can be
expressed recursively. First, select an attribute to
place at the root node and make one branch for
each possible value. This splits up the example set
into subsets, one for every value of the attribute.
Now the process can be repeated recursively for
each branch, using only those instances that
actually reach the branch [10].
4.1.1 Decision Tree Algorithm
During the late 1970s and early 1980s, J. Ross
Quinlan,
a
researcher
in
machine
learning,
developed a decision tree algorithm known as ID3
(Iterative Dichotomiser) [11]. This work expanded
on earlier work on concept learning systems,
described by E. B. Hunt, J. Marin, and P. T. Stone.
Quinlan later presented C4.5 (a successor of ID3),
which became a benchmark to which newer
supervised learning algorithms are often compared.
In 1984, a group of statisticians (L. Breiman, J.
Friedman, R. Olshen, and C. Stone) published the
book Classification and Regression Trees (CART),
which described the generation of binary decision
trees. ID3 and CART were invented independently
of one another at around the same time, yet follow
a similar approach for learning decision trees from
training tuples. These two cornerstone algorithms
spawned
a
flurry
of
work
on
decision
tree
induction. ID3, C4.5, and CART adopt a greedy
(i.e., nonbacktracking) approach in which decision
trees are constructed in a top-down recursive
divide-and-conquer manner. Most algorithms for
decision tree induction also follow such a top-down
approach, which starts with a training set of tuples
and their associated class labels. Decision tree
algorithm [12] is given below:
Algorithm Decision Tree (J48)
INPUT:
D //Training data
OUTPUT
T //Decision tree
DTBUILD (*D)
{
T=φ;
T= Create root node and label with splitting
attribute;
T= Add arc to root node for each split predicate
and
Label;
For each arc do
D=
Database
created
by
applying
splitting
predicate to D;
If stopping point reached for this path, then
T’= create leaf node and label with appropriate
class;
Else
T’= DTBUILD (D);
T= add T’ to arc;
}
While building a tree, J48 ignores the missing
values i.e. the value for that item can be predicted


--- Page 4 ---
Page 4
based on what is known about the attribute values
for the other records. The basic idea is to divide the
data into range based on the attribute values for that
item that are found in the training sample. J48
allows classification via either decision trees or
rules generated from them [5] [1].
4.2 Confusion Matrix
In the field of machine learning, a confusion
matrix, also known as a contingency table or an
error matrix , is a specific table layout that allows
visualization of the performance of an algorithm,
typically
a
supervised
learning
one
(in
unsupervised
learning
it
is
usually
called
a
matching matrix). Each column of the matrix
represents the instances in a predicted class, while
each row represents the instances in an actual class.
In
predictive
analytics,
a
table
of
confusion
(sometimes also called a confusion matrix), is a
table with two rows and two columns that reports
the number of false positives, false negatives, true
positives, and true negatives. This allows more
detailed analysis than mere proportion of correct
guesses (accuracy) [7]. It is termed as Gold
standard as mentioned below in the following
figure 1.
Figure 1.Confusion Matrix (Gold Standard)
4.3 Sample Training Data
In particular, all patients here are females at least
21
years
old
of
Pima
Indian
heritage.
The
diagnostic, binary-valued variable investigated is
whether
the
patient
shows
signs
of
diabetes
according to World Health Organization criteria
(i.e., if the 2 hour post-load plasma glucose was at
least 200 mg/dl at any survey examination or if
found during routine medical care.
Sample Pima Indians Diabetes Database
6, 148, 72,35,0, 33.6, 0.627,50, tested_positive
1, 85, 66, 29, 0, 26.6, 0.351,31, tested_negative
8, 183, 64, 0, 0, 23.3, 0.672,32, tested_positive
1, 89, 66, 23, 94, 28.1, 0.167, 21, tested_negative
0, 137, 40, 35, 168, 43.1, 2.288, 33, tested_positive
5, 116, 74, 0, 0, 25.6, 0.201, 30, tested_negative
3, 78, 50, 32, 88, 31, 0.248, 26, tested_positive
10, 115, 0, 0, 0, 35.3, 0.134, 29, tested_negative
4.4 Launching WEKA Explorer
The WEKA GUI Chooser appears like figure 2
Figure 2.WEKA Explorer
1. Explorer - is an environment for exploring data.
2.
Experimenter
-
is
an
environment
for
performing experiments and conducting statistical
tests between learning schemes.
3. Knowledge Flow - is a Java-Beans-based
interface for setting up and running machine
learning experiments.
4. Simple CLI - provides a simple command-line
interface and allows direct execution of Weka
commands.[5]
Click on the Explorer button. This will bring up the
main screen. Load the diabetes data by clicking on
"Open file...” navigating to the data folder, and
selecting diabetes.arff. The main screen should
now look like this. This file contains 768 records of
PIMA Dataset. We need to divide up our records so
some data instances are used to create the model,
and some are used to test the model to ensure that
we didn't overfit it. Your screen should look like
Figure
after
loading
the
data.
The
WEKA
preprocessing window with diabetes dataset shown
below in figure 3
Figure 3.WEKA with Diabetes data


--- Page 5 ---
Page 5
We select the Classify tab, and then we select the
trees node, then the J48 leaf. The WEKA j48
windows run information with confusion matrix is
shown
below
in
figure
4
Figure 4.WEKA Confusion Matrix for Diabetes
Confusion Matrix Explained
Correctly Classified Instances = 84.1146 %
Incorrectly Classified Instances = 15.8854 %
True Positive = 468, True Negative = 178
False Positives= 32, False Negatives = 90
Based on our accuracy rate of 84.1146 %, we can
say that this is a pretty good model to predict
whether a new patient will be positive or negative
for diabetes. You can see the tree by right-clicking
on the model you just created, in the result list. On
the pop-up menu, select Visualize tree. You'll see
the classification tree as shown in figure 5
4.5. WEKA Output
Diabetes decision tree is generated using WEKA
environment is given below in figure 5.
Figure 5.WEKA Decision Tree for Diabetes
4.6 Evaluation Criteria
The evaluation criterion for proposed method is
shown below in the table. Compared to existing
method [3] the proposed method has advantage
over accuracy in 0.48 % and the time taken to build
the
model
has
0.02
second
increased.
The
evaluation result is given in table 1.Figure 6
specifies accuracy % in Diabetes
Table 1. Evaluation Criteria for Diabetes
Figure 6 Accuracy % for Existing & Proposed
system for Diabetes
Existing
Method [3]
Proposed
Method
Time to Build
in Seconds
0.03 Seconds
0.05 Seconds
Accuracy (%)
83.63 %
84.1146 %


--- Page 6 ---
Page 6
4.6.1 Entropy and Gain for Diabetes
Dataset
The entropy and information gain is calculated for
various attributes. The attribute has the highest gain
is selected as the decision attribute in the root node
and the entropy calculations are given below in the
tables. Entropy = -Σi Pi log2Pi
4.6.1.1 Entropy (class):
Table. 2 Entropy (class)
Entropy (class) = Entropy (268, 500)
Entropy (class) = Entropy (0.34, 0.65)
Entropy (class) = (0.34log 2 (0.34))-(0.65log2 (0.65))
Entropy (class) = 0.9328
4.6.1.2 Entropy (class, Plasma)
Table. 3 Entropy (class, Plasma)
Entropy (class, Plasma) = Entropy (485, 283) = 0.7996
Gain (t/x) = Entropy (class)-Entropy (class, Plasma)
Gain (t/x) = 0.1332
4.6.1.3 Entropy (class, BMI)
Table. 4 Entropy (class, BMI)
Entropy (class, BMI) = Entropy (170, 598) = 0.8604
Gain (t/x) = Entropy (class)-Entropy (class, BMI)
Gain (t/x) = 0.0724
4.6.1.4 Entropy (class, Pregnancy)
Table. 5 Entropy (class, Pregnancy)
Entropy (class, Pregnancy) = Entropy (682, 86) = 0.8962
Gain (t/x) = Entropy (class)-Entropy (class, Pregnancy)
Gain (t/x) = 0.0366
5. CONCLUSION & FUTURE
ENHANCEMENT
The
study
and
implementation
conducted
in
prediction of diabetes using decision tree with the
help of WEKA tool has the advantages over the
existing
system.
Hence
the
classification
techniques improve the functionality with the help
of entropy and gain. The PIMA diabetes dataset
consist of class attribute which has two categories
(i.e. Tested Positive has 268 values and Tested
Negative has 500 values). Even though it is not
correct
because
the
diabetes
dataset
contain
missing values. A total of 768 cases are available in
PIMA. 5 patients had a glucose of 0, 11 patients
had a body mass index of 0, 28 others had a
diastolic blood pressure of 0, 192 others had skin
fold thickness reading of 0, and 140 others had
serum insulin level s of 0. After deleting these
cases there were 392 cases with no missing values
(103 tested positive cases and 262 tested negative)
[6]. In order to get the exact prediction of diabetes
the missing values should be avoided in the dataset
and they can be tested in different environment.


--- Page 7 ---
Page 7
REFERENCES
Journal Papers:
[1] Ross Quinlan (1993). C4.5: Programs for Machine Learning.
Morgan Kaufmann Publishers, San Mateo, CA.
[2] Ross J. Quinlan: Learning with Continuous Classes. In: 5th
Australian
Joint
Conference
on
Artificial
Intelligence,
Singapore,343-348,1992.
[3]. ShravanKumar Uppin, Anusuya, Expert System Design to
Predict Heart and Diabetes Diseases, International Journal of
Scientific Engineering and Technology Volume No.3 Issue No.8,
pp: 1054-1059 1 Aug 2014
[4]. Svetlana S. Aksenova Machine Learning with WEKA,
WEKA Explorer Tutorial for WEKA Version 3.4.3
[5]. VelidePhani Kumar and Lakshmi Velide, A data mining
approach for prediction and treatment of diabetes disease,
international journal of science inventions today,2014,3(1),073-
079.
Proceedings Papers:
[6]. Asha Gowda Karegowda,Vidya T.Sharma, M.A.Jayaram,
and
A.S.Manjunath,
Improving
Perfomance
of
K-Means
Clustering
by
Initalizing
Cluster
Centers
Using
Genetic
Algorithm
and
Entrophy
Based
Fuzzy
Clustering
for
Categorization
of
Diabetic
Patients
,
Proceedings
of
International Conference on Advances in Computing Advances
in Intelligent Systems and Computing Volume 174, 2012, pp
899-904
Websites:
[7] http://en.wikipedia.org/wiki/Confusion_matrix
[8]
http://www.ics.uci.edu/~mlearn/MLRepository.html.,
UCI
Machine Learning Repository
[9].http://www.opentox.org/dev/documentation/components/j48
Books:
[10]. Ian H. Witten, Eibe Frank, Data Mining Practical Machine
Learning Tools and Techniques, Second Edition,
[11] JiaweiHan &Micheline Kamber, Data mining: concepts
and techniques 2rd edition
[12]
Margaret
H.
Danham,S.
Sridhar,
”
Data
mining,
introductory and advanced topics”, person education , 1st ed.,
2006
[13].
Pang-Ning
Tan,
Michael
Steinbach,
Vipin
Kumar,
Introduction to data mining
