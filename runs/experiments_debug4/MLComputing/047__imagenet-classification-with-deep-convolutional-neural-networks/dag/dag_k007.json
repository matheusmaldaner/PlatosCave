{
  "nodes": [
    {
      "id": 0,
      "text": "A large, deep convolutional neural network trained with practical engineering choices can achieve substantially better image classification performance on ImageNet than prior methods",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6
      ]
    },
    {
      "id": 1,
      "text": "We designed an eight-layer CNN with five convolutional layers followed by three fully-connected layers and a final 1000-way softmax to maximize multinomial log-likelihood on ImageNet",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2,
        3,
        4
      ]
    },
    {
      "id": 2,
      "text": "Key architectural and training choices: use of ReLU nonlinearities, local response normalization, overlapping max-pooling, training across two GPUs with partitioned connectivity, and highly optimized GPU 2D convolution implementation",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": [
        7,
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "To reduce overfitting we applied extensive data augmentation (random 224x224 crops with reflections and PCA-based RGB intensity perturbation) and dropout in the first two fully-connected layers",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 4,
      "text": "Training used stochastic gradient descent with batch size 128, momentum 0.9, weight decay 0.0005, learning rate schedule reduced by factor 10 on plateau, weight initialization from small gaussian and bias settings to speed early learning",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        6
      ]
    },
    {
      "id": 5,
      "text": "The network was trained on the ILSVRC subsets of ImageNet: 1.2M training, 50k validation, and 150k testing images with inputs resized and center-cropped to 256 then 224 pixel patches, mean-subtracted RGB pixels",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        6
      ]
    },
    {
      "id": 6,
      "text": "Training the described network on two GTX 580 3GB GPUs required about five to six days for roughly 90 epochs over 1.2M images",
      "role": "Result",
      "parents": [
        4,
        5
      ],
      "children": [
        14
      ]
    },
    {
      "id": 7,
      "text": "ReLU (f(x)=max(0,x)) nonlinearity substantially speeds learning compared to saturating nonlinearities, enabling practical training of very deep networks",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 8,
      "text": "Spreading the network across two GPUs with selective cross-GPU connectivity allowed training a larger model than a single GPU and decreased top-1/top-5 error by about 1.7% and 1.2% compared to a single-GPU half-sized alternative",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        6,
        14
      ]
    },
    {
      "id": 9,
      "text": "Local response normalization, applied after some ReLU layers with hyperparameters k=2, n=5, alpha=1e-4, beta=0.75, implements lateral inhibition and reduced top-1/top-5 error by about 1.4% and 1.2%",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 10,
      "text": "Overlapping pooling with window size 3 and stride 2 reduced top-1/top-5 error by about 0.4% and 0.3% and slightly reduced overfitting relative to non-overlapping pooling",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 11,
      "text": "A highly optimized GPU implementation of convolution enabled practical training of the large model",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        6
      ]
    },
    {
      "id": 12,
      "text": "Random 224x224 crops and horizontal reflections effectively increased training variations (equivalent to many transformed samples) and at test time predictions were averaged over ten crops",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 13,
      "text": "PCA-based RGB intensity augmentation (adding principal component offsets scaled by eigenvalues times gaussian noise) captures illumination variation and reduced top-1 error by over 1%; dropout in fully-connected layers reduced overfitting but doubled iterations to converge",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "Empirical performance: on ILSVRC-2010 the single network achieved 37.5% top-1 and 17.0% top-5 test error; on ILSVRC-2012 variants and ensembling achieved down to 15.3% top-5 test error, substantially outperforming prior state-of-the-art",
      "role": "Result",
      "parents": [
        6,
        7,
        8,
        9,
        10,
        12,
        13,
        11
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Conclusions: network depth is important (removing any convolutional layer degrades performance by about 2% top-1), and further gains are expected from larger datasets, more compute, and deeper models despite current GPU memory and time limits",
      "role": "Conclusion",
      "parents": [
        14,
        1
      ],
      "children": null
    }
  ]
}