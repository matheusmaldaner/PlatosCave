{
  "nodes": [
    {
      "id": 0,
      "text": "An error-bounded GPU lossy compressor (cuSZp) can achieve ultra-fast end-to-end throughput while also maintaining high compression ratio and high reconstructed data quality for scientific HPC datasets",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "Modern HPC applications generate vast volumes of scientific data creating critical storage and communication challenges that motivate error-bounded lossy compression",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        6
      ]
    },
    {
      "id": 2,
      "text": "cuSZp design: put entire compression and decompression pipeline into a single GPU kernel to eliminate CPU involvement, kernel-launch overheads, and CPU-GPU data movement",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        10
      ]
    },
    {
      "id": 3,
      "text": "cuSZp processing pipeline is block-wise and consists of four steps: (S1) pre-quantization and lightweight Lorenzo prediction, (S2) fixed-length encoding per block, (S3) hierarchical global prefix-sum synchronization, and (S4) block bit-shuffle to pack bits into bytes",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        11,
        12
      ]
    },
    {
      "id": 4,
      "text": "Design tradeoffs: fixed-length encoding per block plus bit-shuffle avoids costly variable-length encoders (e.g., Huffman) and enables high parallelism and throughput while leveraging intra-block smoothness",
      "role": "Claim",
      "parents": [
        0,
        3
      ],
      "children": [
        13
      ]
    },
    {
      "id": 5,
      "text": "Evaluation uses NVIDIA A100 GPU and six representative scientific datasets from multiple domains to measure end-to-end throughput, kernel throughput, compression ratio, PSNR, SSIM, and visualization quality",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        14,
        15
      ]
    },
    {
      "id": 6,
      "text": "Error-bounded lossy compression is a promising solution for HPC data: allows controlled error to achieve much higher compression ratios (orders of magnitude) than lossless methods",
      "role": "Context",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Global synchronization (prefix-sum) is implemented hierarchically (thread, warp, device) inside the single kernel to compute per-block compressed offsets with high throughput",
      "role": "Method",
      "parents": [
        2,
        3
      ],
      "children": [
        11
      ]
    },
    {
      "id": 8,
      "text": "Pre-quantization converts floating point to integers bounded by the user error bound and is the only lossy step; a 1D 1-layer Lorenzo prediction records intra-block integer differences to reduce effective bitwidths",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        4
      ]
    },
    {
      "id": 9,
      "text": "Fixed-length encoding per non-zero block stores sign map and keeps only the number of bits equal to the highest required bit among block integers, enabling simple computation of block compressed size",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        4
      ]
    },
    {
      "id": 10,
      "text": "Putting all steps in one kernel removes CPU preprocessing/postprocessing (e.g., Huffman tree building) that in prior GPU compressors caused large end-to-end overheads and low end-to-end throughput",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        14
      ]
    },
    {
      "id": 11,
      "text": "Hierarchical global synchronization achieves high device-level throughput (measured average 208.06 GB/s across profiled datasets) enabling efficient concatenation of variable-length block outputs inside the kernel",
      "role": "Evidence",
      "parents": [
        7
      ],
      "children": [
        14
      ]
    },
    {
      "id": 12,
      "text": "Block bit-shuffle rearranges bits by offset across block integers into aligned bytes, avoiding irregular bit shifts and enabling high parallel writes to final compressed memory",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 13,
      "text": "Scientific datasets show high intra-block smoothness (empirical CDFs for block value range), justifying fixed-length per-block encoding without large loss of compression ratio",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": [
        14
      ]
    },
    {
      "id": 14,
      "text": "End-to-end evaluation results: cuSZp achieves average end-to-end throughput 93.63 GB/s (compression) and 120.04 GB/s (decompression), which is 95.53x faster than cuSZ and 55.18x faster than cuSZx",
      "role": "Result",
      "parents": [
        5,
        10,
        11,
        12,
        13
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Compression ratio and quality results: cuSZp attains highest compression ratio in 16 of 24 benchmark cases and preserves or improves reconstructed data fidelity (higher PSNR and SSIM vs cuZFP and comparable or better visualization than baselines)",
      "role": "Result",
      "parents": [
        5,
        3,
        4,
        14
      ],
      "children": [
        0
      ]
    }
  ]
}