{
  "nodes": [
    {
      "id": 0,
      "text": "A sequence transduction model built solely on attention mechanisms (the Transformer) can replace recurrent and convolutional layers to achieve better or comparable translation quality while enabling greater parallelization and faster training",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "The Transformer architecture consists of stacked encoder and decoder networks using only multi-head self-attention and position-wise feed-forward layers, with residual connections and layer normalization",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8
      ]
    },
    {
      "id": 2,
      "text": "Multi-Head Attention (h parallel attention heads with linear projections to smaller dk and dv) allows the model to attend jointly to information from different representation subspaces and mitigates averaging effects of single-head attention",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 3,
      "text": "Scaled Dot-Product Attention computes attention as softmax(Q K^T / sqrt(dk)) V, which is faster and more space-efficient than additive attention, and the scaling counteracts large dot-product magnitudes for large dk",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "Positional encodings (sinusoids of different frequencies added to input embeddings) provide tokens with order information enabling the non-recurrent Transformer to model sequence order and generalize to longer lengths",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10
      ]
    },
    {
      "id": 5,
      "text": "The Transformer reduces maximum path length between any two positions to O(1) per layer, improving ability to learn long-range dependencies and enabling more parallelization than recurrent and many convolutional alternatives",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 6,
      "text": "Encoder and decoder stacks use N=6 identical layers (encoder: multi-head self-attention + position-wise feed-forward; decoder: masked self-attention, encoder-decoder multi-head attention, and feed-forward), with dmodel=512 for base models",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Position-wise feed-forward networks apply two linear transformations with ReLU per position (dff=2048) and act like convolutions with kernel size 1",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "The model shares input and output embedding weights (scaled by sqrt(dmodel)) and uses dropout, label smoothing, and Adam optimizer with a warming learning rate schedule during training",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        12
      ]
    },
    {
      "id": 9,
      "text": "Ablation experiments show multiple attention heads outperform single-head attention (single-head is about 0.9 BLEU worse) and that too many heads can hurt performance, indicating a trade-off in head count",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Replacing sinusoidal positional encodings with learned positional embeddings yields nearly identical results on development data, indicating both approaches are viable",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Complexity and path-length analysis: self-attention per-layer complexity is O(n^2 d) with O(1) sequential steps and O(1) maximum path length, while recurrent layers require O(n) sequential steps and longer path lengths",
      "role": "Evidence",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Training procedure: trained on WMT14 English-German (4.5M pairs) and English-French (36M pairs) using byte-pair/word-piece vocabularies, batching by token count, on 8 P100 GPUs with base models trained 100k steps (~12 hours) and big models 300k steps (~3.5 days)",
      "role": "Method",
      "parents": [
        8
      ],
      "children": [
        13,
        14
      ]
    },
    {
      "id": 13,
      "text": "Experimental result: Transformer (big) achieves 28.4 BLEU on WMT14 English-to-German, outperforming previous best results (including ensembles) by over 2 BLEU",
      "role": "Result",
      "parents": [
        12
      ],
      "children": [
        15
      ]
    },
    {
      "id": 14,
      "text": "Experimental result: Transformer (big) achieves 41.0 BLEU on WMT14 English-to-French as a single model after 3.5 days on eight GPUs, establishing a new single-model state of the art",
      "role": "Result",
      "parents": [
        12
      ],
      "children": [
        15
      ]
    },
    {
      "id": 15,
      "text": "Compared to prior architectures, the Transformer attains equal or better BLEU while requiring significantly less training cost (FLOPs), enabling faster training and greater parallelizability",
      "role": "Conclusion",
      "parents": [
        13,
        14,
        11
      ],
      "children": null
    }
  ]
}