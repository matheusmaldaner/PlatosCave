{
  "nodes": [
    {
      "id": 0,
      "text": "A sequence transduction model based solely on attention mechanisms (the Transformer) can replace recurrent and convolutional components to improve translation quality while enabling greater parallelization and faster training",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3
      ]
    },
    {
      "id": 1,
      "text": "The Transformer architecture is attention-only, using stacked encoder and decoder blocks built from multi-head self-attention and position-wise feed-forward layers with residual connections and layer normalization",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4,
        5,
        6,
        8
      ]
    },
    {
      "id": 2,
      "text": "Because it removes recurrence and convolution, the Transformer is more parallelizable, requires fewer sequential operations per layer, and can be trained in substantially less wall-clock time and fewer FLOPs",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        11,
        9
      ]
    },
    {
      "id": 3,
      "text": "The Transformer attains state-of-the-art translation results on WMT 2014 benchmarks and outperforms prior models while using less training cost",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": [
        9
      ]
    },
    {
      "id": 4,
      "text": "Encoder and decoder each use N stacked identical layers (N=6), where encoder layers have multi-head self-attention and a position-wise feed-forward sublayer, and decoder layers add encoder-decoder attention with masking to preserve autoregressiveness",
      "role": "Method",
      "parents": [
        1
      ],
      "children": [
        5
      ]
    },
    {
      "id": 5,
      "text": "Attention is implemented as Scaled Dot-Product Attention: Attention(Q,K,V)=softmax(QK^T / sqrt(dk)) V, and multi-head attention projects queries, keys, values into h parallel lower-dimensional heads then concatenates outputs (h=8, dk=dv=64 in base)",
      "role": "Method",
      "parents": [
        1,
        4
      ],
      "children": [
        10
      ]
    },
    {
      "id": 6,
      "text": "Positional information is injected by adding fixed sinusoidal positional encodings (sine and cosine of different frequencies) to token embeddings, enabling the model to use token order without recurrence",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Training setup: trained on WMT14 EN-DE (4.5M sentence pairs) and EN-FR (36M) with byte-pair or word-piece tokenization, batched by approximate sequence length, on 8 P100 GPUs using Adam with warmup learning-rate schedule (warmup_steps=4000)",
      "role": "Method",
      "parents": [
        2
      ],
      "children": [
        9
      ]
    },
    {
      "id": 8,
      "text": "Regularization and output details: label smoothing (epsilon=0.1), dropout on sub-layer outputs and embeddings (base Pdrop=0.1), shared input/output embedding weights and softmax projection, and feed-forward inner size dff=2048 with dmodel=512 (base)",
      "role": "Method",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Empirical results: Transformer (big) achieves 28.4 BLEU on WMT14 English-to-German and 41.0 BLEU on WMT14 English-to-French; base model also surpasses prior published single models while using substantially fewer FLOPs and shorter training times (e.g., big trained 3.5 days on 8 P100 GPUs)",
      "role": "Evidence",
      "parents": [
        3,
        2,
        7
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Ablation and variation results: multi-head attention improves performance over single-head; reducing attention key size dk degrades quality; larger models and appropriate dropout improve BLEU; learned positional embeddings perform similarly to sinusoidal",
      "role": "Evidence",
      "parents": [
        1,
        5
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Theoretical and empirical complexity advantages: self-attention layers have per-layer complexity O(n^2 * d) and constant minimum sequential operations O(1), yielding shorter maximum path lengths for long-range dependencies compared to recurrent or small-k convolutional stacks, and enabling better parallelization when sequence length n is less than representation dimension d",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 12,
      "text": "Transformer limitations and intended future work include investigating restricted/local attention for very long inputs to reduce O(n^2) cost, extending to other modalities (images, audio, video), and making generation less sequential",
      "role": "Limitation",
      "parents": [
        0
      ],
      "children": null
    }
  ]
}