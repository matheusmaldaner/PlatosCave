{
  "nodes": [
    {
      "id": 0,
      "text": "Integrating privacy and fairness simultaneously into machine learning models is necessary but not well understood, requiring a holistic research agenda",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11
      ]
    },
    {
      "id": 1,
      "text": "This paper performs a comprehensive survey of nearly 200 studies to consolidate terminology and evidence on privacy and fairness across supervised, unsupervised, semi-supervised, and reinforcement learning",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        11
      ]
    },
    {
      "id": 2,
      "text": "Differential privacy (DP) methods (e.g., epsilon-delta DP, Laplace and exponential mechanisms, DP-SGD, DP-SVDD) add calibrated noise or perturb model elements to protect individuals during training and release",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        6,
        8,
        9
      ]
    },
    {
      "id": 3,
      "text": "Homomorphic encryption (HE) schemes (FHE, PHE, SHE) enable computation on encrypted data and are applied to privacy-preserving ML with trade-offs in computational cost and supported operations",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 4,
      "text": "Key fairness notions include group-level metrics (statistical parity, equalized opportunity/odds, calibration, balance for clustering) and individual-level notions (individual fairness, counterfactual fairness), with many incompatible definitions",
      "role": "Context",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8,
        9
      ]
    },
    {
      "id": 5,
      "text": "Five representative architectures to combine privacy and fairness: (A) sanitize data then debias, (B) joint private-and-fair training, (C) separate sensitive attribute sanitization, (D) federated learning with local privacy and fairness steps, (E) cryptographic auditing (MPC) for private fairness audits",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 6,
      "text": "Evidence: impact of privacy on fairness is mixed — some studies report alignment (DP or private encodings can reduce bias in certain settings), while others show trade-offs (DP-SGD and added noise can disproportionately degrade accuracy for underrepresented groups and obscure outliers)",
      "role": "Evidence",
      "parents": [
        0,
        2,
        4
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 7,
      "text": "Evidence: impact of fairness measures on privacy is mixed — individual fairness can imply DP under specific metrics, but fairness-aware training can increase membership or attribute leakage for disadvantaged subgroups in other settings",
      "role": "Evidence",
      "parents": [
        0,
        4
      ],
      "children": [
        8,
        10
      ]
    },
    {
      "id": 8,
      "text": "Existing algorithmic approaches to jointly pursue privacy and fairness include: adaptive clipping and bilevel DP+fair optimization (FairDP), functional-mechanism DP with fairness constraints, DP in federated learning combined with fairness selection or local debiasing, and exponential-mechanism-based selection for fairness in some selection tasks",
      "role": "Method",
      "parents": [
        0,
        2,
        3,
        4,
        5,
        6,
        7
      ],
      "children": [
        11,
        10
      ]
    },
    {
      "id": 9,
      "text": "Application domains illustrate varied privacy-fairness interactions: healthcare (synthetic data, EHRs), NLP (private text encoders reduce bias), computer vision (adversarial privacy vs fairness trade-offs), and spatial data (DP noise can change resource allocation fairness)",
      "role": "Claim",
      "parents": [
        0,
        2,
        3,
        4,
        5,
        6
      ],
      "children": [
        11,
        10
      ]
    },
    {
      "id": 10,
      "text": "Open challenges and limitations: incompatible fairness definitions, amplified harms under DP for minority/rare examples, compute and practicality limits of HE/FHE, lack of cryptographic protocols expressing complex fairness constraints, auditing private models, and special considerations for large language models and learning-from-human-feedback pipelines",
      "role": "Limitation",
      "parents": [
        0,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Conclusion: there is no universal resolution; research must characterize when privacy and fairness align versus conflict, develop practical joint architectures and algorithms, and address empirical, theoretical, cryptographic, and large-model challenges",
      "role": "Conclusion",
      "parents": [
        0,
        1,
        8,
        9,
        10
      ],
      "children": null
    }
  ]
}