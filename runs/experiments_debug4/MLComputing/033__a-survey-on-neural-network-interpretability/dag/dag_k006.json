{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent taxonomy and analysis of neural network interpretability can organize methods, clarify definitions, and guide future research to make deep networks more understandable and useful",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        6,
        9,
        14,
        15
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is the ability to provide explanations in understandable terms to a human, where explanations ideally take logical rule form or elements convertible to rules, and understandable terms derive from domain knowledge",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 2,
      "text": "Interpretability is important for three classes of reasons: high-reliability requirements, ethical and legal requirements, and scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        3,
        4,
        5
      ]
    },
    {
      "id": 3,
      "text": "High-reliability systems need interpretability because unexpected failures and adversarial examples can cause catastrophic consequences and interpretability helps detect root causes and guide fixes",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 4,
      "text": "Ethical and legal requirements (e.g., avoiding algorithmic discrimination and the GDPR right to explanation) motivate interpretability to detect bias, provide rationale, and satisfy regulatory needs for domains like medicine and finance",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "In scientific applications (e.g., genomics, astronomy, drug discovery) interpretability enables discovery of domain knowledge embedded by deep models and supports scientific explanation",
      "role": "Claim",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Proposed taxonomy: three dimensions â€” (1) passive vs active approaches, (2) type/format of explanations ordered by explicitness (rules, hidden semantics, attribution, examples), and (3) interpretability scope from local to semi-local to global",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 7,
      "text": "Passive approaches perform post-hoc analysis on trained networks; active approaches change architecture or training (regularizers, prototype layers, filter losses) to encourage interpretability",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": [
        10,
        13
      ]
    },
    {
      "id": 8,
      "text": "Interpretation scope dimension: local explains individual predictions, semi-local explains groups/neighborhoods of inputs, global explains the whole network (e.g., rule sets or decision trees)",
      "role": "Claim",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Evaluation of interpretability uses application-grounded, human-grounded, and functionally-grounded approaches; proxy metrics include rule size, unit-concept alignment, stability and fidelity of attributions, and impact of masking salient regions",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Passive methods produce four major types of explanations: rule extraction (local/semi-local/global), hidden-semantic analysis and visualization, attribution (saliency, gradients, integrated gradients, Shapley-based), and example-based explanations (influence functions, representer points)",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 11,
      "text": "Passive attribution and visualization methods: gradient-based saliency, DeconvNet, Guided Backprop, Grad-CAM, integrated gradients, DeepLIFT, LRP, Shapley approximations, TCAV and concept-based attribution; many are local or semi-local and have desiderata (sensitivity, implementation/input invariance)",
      "role": "Method",
      "parents": [
        10
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Passive rule extraction and hidden-semantics methods: local counterfactual and anchor rules, critical data routing paths, global rule extraction (decompositional and pedagogical approaches), activation maximization and network dissection for assigning concepts to units, and prototype selection",
      "role": "Method",
      "parents": [
        10
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Active interpretability interventions include tree and regional-tree regularization to favor shallow decision-tree approximations, prototype layers to force prototype-based classification, and filter-level losses to encourage disentangled, consistent hidden-unit concepts",
      "role": "Method",
      "parents": [
        7
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Key limitations and challenges: interpretability is hard to evaluate objectively, many attribution methods depend on reference choices and can be fragile to adversarial manipulation, and global explanations may not scale to very deep networks",
      "role": "Limitation",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Conclusions and recommendations: emphasize explanation format and domain concepts, pursue underexplored active interventions that preserve performance, and better integrate domain knowledge and objective evaluation to advance interpretability research",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    }
  ]
}