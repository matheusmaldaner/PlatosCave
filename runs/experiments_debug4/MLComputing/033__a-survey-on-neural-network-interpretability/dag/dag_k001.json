{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent taxonomy and clarified definition of neural network interpretability enable systematic organization, evaluation, and guidance for research on interpreting deep neural networks",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        13
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is the ability to provide explanations in understandable terms to a human; explanations ideally map to logical decision rules or convertible elements and understandable terms derive from domain knowledge",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 2,
      "text": "Interpretability is important for three reasons: high-reliability systems, ethical and legal requirements, and enabling scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "Proposed taxonomy has three dimensions: (1) passive vs active approaches, (2) type/format of explanations ordered by explicitness, and (3) local to global interpretability with semi-local in between",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4,
        5,
        11
      ]
    },
    {
      "id": 4,
      "text": "Four major explanation formats are distinguished in order of decreasing explicitness: logic rules, hidden semantics, attribution, and explanation by examples",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        6,
        7,
        8,
        9
      ]
    },
    {
      "id": 5,
      "text": "The passive vs active dimension separates methods that post hoc explain trained networks from methods that alter architecture or training to induce interpretability",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        6,
        7,
        8,
        9,
        10
      ]
    },
    {
      "id": 6,
      "text": "Passive rule-based explanations include local/semi-local counterfactual and anchor rules and global rule extraction via decompositional and pedagogical methods (decision trees, rule sets, fuzzy or first-order rules)",
      "role": "Method",
      "parents": [
        4,
        5
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Hidden-semantics methods interpret neurons or channels via visualization (activation maximization with priors), concept alignment (network dissection, Net2Vec) and linguistic or receptive-field analyses",
      "role": "Method",
      "parents": [
        4,
        5
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Attribution methods assign feature importance locally or globally using gradients and variants (saliency, Guided Backprop, Grad-CAM), discrete/path-integrated gradients (DeepLIFT, LRP, Integrated Gradients), model-agnostic local surrogates (LIME, MAPLE) and Shapley-based approaches",
      "role": "Method",
      "parents": [
        4,
        5
      ],
      "children": [
        12
      ]
    },
    {
      "id": 9,
      "text": "Example-based explanations include influence functions and representer-point methods for local explanations and prototype layers or prototype networks for global, case-based explanations",
      "role": "Method",
      "parents": [
        4,
        5
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Active interpretability interventions add interpretability losses or architectural components during training, e.g., tree/region regularization, filter mutual-information loss for disentangled filters, attribution priors, and prototype layers",
      "role": "Method",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Evaluation of interpretability uses three approaches: application-grounded (task performance with humans), human-grounded (human subject assessments), and functionally-grounded (proxy metrics such as sparsity or model size)",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        13
      ]
    },
    {
      "id": 12,
      "text": "Existing methods have limitations and failure modes: attribution fragility and adversarial manipulations, dependence on reference choices, scalability limits for global rule extraction, and varying implementation dependence",
      "role": "Limitation",
      "parents": [
        6,
        8
      ],
      "children": [
        13
      ]
    },
    {
      "id": 13,
      "text": "The taxonomy produces a coherent 3D view of interpretability literature, helps identify gaps, and guides future work and evaluation choices",
      "role": "Conclusion",
      "parents": [
        0,
        11,
        12
      ],
      "children": [
        14,
        15
      ]
    },
    {
      "id": 14,
      "text": "Active interpretability interventions are underexplored and present a research direction to make networks interpretable during training without harming performance",
      "role": "Claim",
      "parents": [
        13
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Incorporating richer domain knowledge and task-specific understandable terms into explanations is a key future direction to produce more informative and usable interpretations",
      "role": "Claim",
      "parents": [
        13
      ],
      "children": null
    }
  ]
}