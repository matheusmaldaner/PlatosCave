{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent, three-dimensional taxonomy of neural network interpretability methods (passive vs active; type/format of explanations; local to global interpretability) organizes prior work and suggests research directions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is the ability to provide explanations in understandable terms to a human, where explanations ideally are logical decision rules or transformable to them, and understandable terms come from domain knowledge related to the task",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4
      ]
    },
    {
      "id": 2,
      "text": "Interpretability is important for three reasons: enabling high reliability in safety-critical systems, meeting ethical and legal requirements (e.g., avoiding algorithmic discrimination and satisfying rights to explanation), and facilitating scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5
      ]
    },
    {
      "id": 3,
      "text": "We propose a taxonomy with three dimensions: (1) passive versus active approaches, (2) type/format of explanations ordered by explicitness (rules, hidden semantics, attribution, examples), and (3) interpretability scope from local to semi-local to global",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        7,
        8
      ]
    },
    {
      "id": 4,
      "text": "Explanations should emphasize their format (language) and leverage domain representations; under this view explanations, explainability and related terms are used interchangeably for (deep) neural networks",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Lack of interpretability can cause undetected failures, regulatory and fairness violations, and loss of opportunity to extract domain knowledge from models used in science",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Dimension 1: Passive methods explain trained networks post hoc without changing training; active methods modify architecture or training (e.g., interpretability loss or regularizers) to encourage interpretability",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        9,
        10
      ]
    },
    {
      "id": 7,
      "text": "Dimension 2: Explanation formats ranked by explicitness â€” logic rules (most explicit), hidden semantics, attribution (feature importance, saliency), and explanations by example (least explicit)",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        11
      ]
    },
    {
      "id": 8,
      "text": "Dimension 3: Interpretability scope is ordinal from local (single prediction) through semi-local to global (whole-model explanations), and methods can be positioned along this continuum",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "Passive methods include: rule extraction (local/semi-local/global), hidden-semantics analysis (visualization, network dissection), attribution (gradient-based, model-agnostic, concept attribution), and example-based explanations (influence functions, representer points)",
      "role": "Method",
      "parents": [
        6
      ],
      "children": [
        12
      ]
    },
    {
      "id": 10,
      "text": "Active methods apply interpretability interventions during training such as tree regularization, filter-disentangling losses (one filter one concept), attribution priors, and prototype layers to produce more interpretable models or explanations",
      "role": "Method",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Different explanation formats have trade-offs: logic rules are explicit but risk complexity; hidden semantics provide concept-level global clues; attribution suits local explanation but may be fragile; example-based explanations are intuitive but may lack generality",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Evaluation of interpretability requires human-centered studies, fidelity checks, and task-specific utility metrics; open research directions include benchmarks, combining active and passive methods, and formalizing interpretability objectives",
      "role": "Evidence",
      "parents": [
        9,
        11
      ],
      "children": null
    }
  ]
}