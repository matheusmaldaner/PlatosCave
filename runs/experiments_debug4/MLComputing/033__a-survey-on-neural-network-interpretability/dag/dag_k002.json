{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent taxonomy and synthesis of neural network interpretability methods will clarify definitions, organize existing work along meaningful dimensions, and suggest research directions to improve interpretability in practice",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        9,
        10,
        12
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is important for deep neural networks for three main reasons: high reliability requirements, ethical and legal requirements, and scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12,
        14
      ]
    },
    {
      "id": 2,
      "text": "Interpretability is defined as the ability to provide explanations in understandable terms to a human; explanations should ideally be logical decision rules (or convertible to them) and understandable terms should derive from domain knowledge",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 3,
      "text": "We propose a novel three-dimensional taxonomy for neural network interpretability organized by: (1) passive vs active approaches, (2) type/format of explanations, and (3) local to global interpretability",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4,
        5,
        6,
        7,
        8,
        9
      ]
    },
    {
      "id": 4,
      "text": "Taxonomy dimension details: passive methods analyze trained networks post hoc; active methods modify architecture or training (e.g., regularizers) to encourage interpretability",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        5,
        13
      ]
    },
    {
      "id": 5,
      "text": "Passive approaches start from a trained network and extract explanations without changing training; active approaches intervene during training to improve interpretability (common form: interpretability regularizer added to loss)",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "We identify four major formats of explanations ordered by explanatory power: logic rules, hidden semantics, attribution, and example-based explanations",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Interpretability w.r.t. input space is a continuum from local (single-instance explanations) through semi-local to global (whole-model explanations); methods can be positioned along this axis",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "The 3D taxonomy enables visualization of the distribution of interpretability papers, revealing dense areas of research and gaps where future work is needed",
      "role": "Result",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "The survey synthesizes existing methods, maps representative approaches into the taxonomy, summarizes evaluation methods, and lists specific contributions compared to prior surveys",
      "role": "Conclusion",
      "parents": [
        0,
        3
      ],
      "children": [
        11,
        14
      ]
    },
    {
      "id": 10,
      "text": "Survey methodology: comprehensive literature review, clarification of interpretability definition, construction of the three-dimensional taxonomy, organization of passive and active methods, and mapping of methods to local/semi-local/global explanations",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        3,
        9
      ]
    },
    {
      "id": 11,
      "text": "Aided by the taxonomy and literature review, we identify promising research directions including (a) developing active interpretability interventions that do not hurt performance and (b) better incorporation of domain knowledge into explanations",
      "role": "Conclusion",
      "parents": [
        9
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Empirical phenomena motivating interpretability include adversarial examples that change predictions with imperceptible perturbations and unrecognizable images classified with high confidence, implying learned mechanisms differ from human reasoning",
      "role": "Evidence",
      "parents": [
        0,
        1
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Representative active and passive methods mapped into the taxonomy: passive rule extraction, hidden-unit visualization and network dissection, attribution methods (gradients, LRP, integrated gradients, LIME, SHAP), example-based influence functions; active interventions include tree regularization, prototype layers, and filter-level interpretability losses",
      "role": "Evidence",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Evaluating interpretability is challenging; common evaluation paradigms include application-grounded, human-grounded, and functionally-grounded methods, and proxies (e.g., sparsity, rule size, alignment to concepts) but no universal objective metric exists",
      "role": "Limitation",
      "parents": [
        1,
        9
      ],
      "children": null
    }
  ]
}