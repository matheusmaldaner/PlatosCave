{
  "nodes": [
    {
      "id": 0,
      "text": "A unified survey and novel taxonomy can clarify neural network interpretability research and guide future work",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is the ability to provide explanations in understandable terms to a human, where explanations ideally take logical rule form or elements convertible to rules, and understandable terms are domain knowledge related to the task",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        6,
        4
      ]
    },
    {
      "id": 2,
      "text": "Interpretability is important for three major reasons: high reliability requirements, ethical and legal requirements, and scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        7
      ]
    },
    {
      "id": 3,
      "text": "We propose a three-dimensional taxonomy of interpretability methods: (1) passive vs active approaches, (2) type/format of explanations ordered by explanatory power (rules, hidden semantics, attribution, examples), and (3) local to global interpretability with a continuous semi-local region",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8,
        9,
        10
      ]
    },
    {
      "id": 4,
      "text": "Domain knowledge and the format of explanations are essential axes for categorizing and evaluating interpretability methods",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        11
      ]
    },
    {
      "id": 5,
      "text": "Active interventions (changing architecture or training via interpretability regularizers) and passive post hoc methods are distinct approaches that trade off performance, applicability, and ability to incorporate priors",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12
      ]
    },
    {
      "id": 6,
      "text": "Explanations may be logical rules, hidden semantics (meanings of hidden units), attribution (feature importance / saliency), or explanation by examples, each with different explicitness and use cases",
      "role": "Claim",
      "parents": [
        1,
        3
      ],
      "children": [
        13
      ]
    },
    {
      "id": 7,
      "text": "Interpretability helps detect and diagnose failures for high reliability systems, prevents or exposes algorithmic bias to meet ethical and legal demands, and reveals domain knowledge for scientific discovery",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Passive methods analyze trained networks to extract explanations (rule extraction, visualization of hidden semantics, attribution methods, influence/example-based explanations)",
      "role": "Method",
      "parents": [
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 9,
      "text": "Active methods inject interpretability during training via regularizers or architectural elements to obtain global or semi-global explanations (e.g., tree regularization, prototype layers, filter disentangling losses)",
      "role": "Method",
      "parents": [
        3,
        5
      ],
      "children": [
        15
      ]
    },
    {
      "id": 10,
      "text": "Local interpretability explains individual predictions, global interpretability explains model behavior across input space, and semi-local approaches bridge them (e.g., anchors, clustering of attributions, multilevel explanation trees)",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 11,
      "text": "Using richer domain-specific interpretable terms (beyond raw inputs or simple concepts) would improve explanation informativeness and usefulness",
      "role": "Claim",
      "parents": [
        4
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Active interventions can improve global interpretability or attribution fidelity but require compatibility between model architectures and interpretability constraints and are currently underexplored",
      "role": "Claim",
      "parents": [
        5
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Different explanation types suit different needs: rules are most explicit (good for global understanding), attribution is useful locally, hidden semantics aid global understanding of learned concepts, and examples provide intuitive local support",
      "role": "Conclusion",
      "parents": [
        6
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Representative passive techniques include: rule extraction via decompositional and pedagogical methods, activation maximization and network dissection for hidden semantics, gradient and model-agnostic attribution (LIME, Shapley, IG, LRP, DeepLIFT), and influence/representer methods for example-based explanations",
      "role": "Evidence",
      "parents": [
        8
      ],
      "children": null
    },
    {
      "id": 15,
      "text": "Representative active techniques include: tree and regional-tree regularization for decision-tree amenability, mutual-information or template losses to disentangle filters, prototype layers for case-based reasoning, and attribution priors for stable local explanations",
      "role": "Evidence",
      "parents": [
        9
      ],
      "children": null
    }
  ]
}