{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent taxonomy and survey of neural network interpretability clarifies definitions, organizes methods along three dimensions, and identifies gaps and directions for research",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        8,
        10
      ]
    },
    {
      "id": 1,
      "text": "Interpretability is important for high reliability, ethical/legal compliance, and scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 2,
      "text": "Interpretability should be defined as the ability to provide explanations in understandable terms to a human, where explanations ideally map to logical decision rules and understandable terms derive from domain knowledge",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        5,
        6
      ]
    },
    {
      "id": 3,
      "text": "We propose a novel three-dimensional taxonomy of interpretability methods: (1) passive vs active approaches, (2) type/format of explanations ordered by explicitness, and (3) locality continuum from local to global (including semi-local)",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        4,
        5,
        6
      ]
    },
    {
      "id": 4,
      "text": "Passive approaches post-hoc analyze trained networks; active approaches modify architecture or training (e.g., interpretability regularizers) to produce more interpretable models",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": [
        7,
        9
      ]
    },
    {
      "id": 5,
      "text": "Four major explanation formats are meaningful for categorization and differ in explanatory power: logic rules, hidden semantics, attribution, and examples (in decreasing explicitness)",
      "role": "Claim",
      "parents": [
        2,
        3
      ],
      "children": [
        7
      ]
    },
    {
      "id": 6,
      "text": "Local, semi-local, and global interpretability form an ordinal dimension: local explains individual predictions, semi-local explains groups, global explains whole model behavior",
      "role": "Claim",
      "parents": [
        2,
        3
      ],
      "children": [
        7,
        8
      ]
    },
    {
      "id": 7,
      "text": "Representative methods mapped into the taxonomy: rule extraction (local/semi-local/global), activation maximization and network dissection for hidden semantics, gradient/backprop and model-agnostic methods (LIME, Shapley) for attribution, influence/representer and prototype networks for example-based explanations",
      "role": "Evidence",
      "parents": [
        4,
        5,
        6
      ],
      "children": [
        8
      ]
    },
    {
      "id": 8,
      "text": "Evaluation strategies for interpretability are application-grounded, human-grounded, and functionally-grounded; metrics include rule/model size, concept alignment scores, attribution-based perturbation tests and stability/sanity checks",
      "role": "Claim",
      "parents": [
        1,
        6,
        7
      ],
      "children": [
        10
      ]
    },
    {
      "id": 9,
      "text": "Active interventions include tree regularization to favor tree-approximable models, filter-level losses to promote disentangled 'one-filter-one-concept' representations, attribution priors, and prototype layers to make decisions case-based",
      "role": "Method",
      "parents": [
        4
      ],
      "children": [
        10
      ]
    },
    {
      "id": 10,
      "text": "Passive methods are dominant in the literature, while active interpretability interventions are relatively underexplored and present promising research opportunities",
      "role": "Conclusion",
      "parents": [
        0,
        4,
        7,
        9,
        8
      ],
      "children": [
        11,
        12
      ]
    },
    {
      "id": 11,
      "text": "Challenges and limitations include: lack of consensus on definitions, difficulty scaling global rule extraction to deep models, attribution fragility and dependence on reference choices, and the need for richer domain concepts in explanations",
      "role": "Limitation",
      "parents": [
        10
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Recommended future directions: develop active methods that improve interpretability without degrading performance, integrate richer domain knowledge/concepts into explanations, and build better objective evaluation benchmarks",
      "role": "Conclusion",
      "parents": [
        10
      ],
      "children": null
    }
  ]
}