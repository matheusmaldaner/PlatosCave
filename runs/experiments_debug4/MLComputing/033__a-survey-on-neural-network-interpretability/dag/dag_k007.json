{
  "nodes": [
    {
      "id": 0,
      "text": "A coherent, three-dimension taxonomy and comprehensive survey clarifies neural network interpretability research and suggests directions for future work",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "Interpretability should be defined as the ability to provide explanations in understandable terms, where explanations ideally are logic-like rules and understandable terms are domain concepts",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        8
      ]
    },
    {
      "id": 2,
      "text": "Interpretability is important for three major reasons: high reliability requirements, ethical and legal requirements, and scientific knowledge discovery",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        9,
        10,
        11
      ]
    },
    {
      "id": 3,
      "text": "A novel taxonomy with three dimensions structures interpretability work: (1) passive vs active approaches, (2) format/type of explanations, (3) local to global interpretability",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        12,
        13,
        14
      ]
    },
    {
      "id": 4,
      "text": "Passive methods analyze trained networks post hoc to extract explanations (rule extraction, hidden semantics visualization, attribution, example-based explanations)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 5,
      "text": "Active methods modify architecture or training (regularizers, prototype layers, concept losses) to encourage networks to be more interpretable",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 6,
      "text": "Interpretability evaluation approaches include application-grounded, human-grounded, and functionally-grounded evaluations and proxy measures (e.g., rule size, match to concepts, attribution robustness)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 7,
      "text": "Conclusions and research gaps: active interpretability interventions and incorporation of richer domain knowledge are underexplored and represent key future directions",
      "role": "Conclusion",
      "parents": [
        0
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Emphasizing explanation format (e.g., rule forms) and domain representations helps categorize and compare interpretability methods",
      "role": "Claim",
      "parents": [
        1
      ],
      "children": null
    },
    {
      "id": 9,
      "text": "High reliability requirement: interpretability helps detect and diagnose unexpected failures and aids control in safety-critical systems",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 10,
      "text": "Ethical and legal requirement: interpretability helps detect algorithmic bias and supports legal rights such as the EU right to explanation for automated decisions",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Scientific usage: interpretability can reveal domain knowledge discovered by models and enable use of deep networks in scientific domains (e.g., genomics, drug discovery)",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": null
    },
    {
      "id": 12,
      "text": "Dimension 1: passive versus active distinguishes post-hoc explanation of fixed models from methods that alter training or architecture to promote interpretability",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Dimension 2: types/formats of explanations are ordered by explicitness from examples, attribution, hidden semantics to logic rules (rules provide the most explicit explanations)",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Dimension 3: interpretability spans local, semi-local, and global explanations, allowing methods to explain individual predictions, groups of inputs, or entire model behavior",
      "role": "Claim",
      "parents": [
        3
      ],
      "children": null
    }
  ]
}