{
  "nodes": [
    {
      "id": 0,
      "text": "Hypothesis: Choices elicited in hypothetical choice experiments (CEs) deviate from choices in more realistic settings (hypothetical bias) and thereby affect the external validity of CE-derived measures such as willingness to pay, marginal rates of substitution, market shares and uptake predictions",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "Method: Systematic search and screening of Web of Science and supplementary searches produced a core dataset of 57 empirical CE studies comparing hypothetical and more realistic/incentivised/revealed conditions (searches updated to April 2020); each study was coded for context, benchmark type, design (within/between), opt-out, estimation and bias measures",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2
      ]
    },
    {
      "id": 2,
      "text": "Method: Introduced a unified operational definition of hypothetical bias and a five-class taxonomy of benchmark realism (Class I online hypothetical → Class V naturalistic revealed observations) to standardise comparisons across CE studies",
      "role": "Method",
      "parents": [
        0,
        1
      ],
      "children": [
        4,
        6
      ]
    },
    {
      "id": 3,
      "text": "Result: Overall evidence across disciplines is mixed but leans toward the presence of hypothetical bias in many CE applications; HB prevalence, direction and magnitude are context- and measurement-dependent",
      "role": "Result",
      "parents": [
        0,
        1
      ],
      "children": [
        4,
        5,
        6
      ]
    },
    {
      "id": 4,
      "text": "Evidence: Domain heterogeneity — health economics studies mostly report negligible or no HB (majority within-subject, real uptake benchmarks), transport studies predominantly report significant HB (often downward bias in value of travel time), consumer studies mostly show significant HB (overstated opt-in/TWTP but mixed MWTP), environmental studies show mixed evidence influenced by public vs private and moral aspects",
      "role": "Evidence",
      "parents": [
        0,
        3,
        2
      ],
      "children": [
        5
      ]
    },
    {
      "id": 5,
      "text": "Result: A common empirical regularity is systematic scale differences between hypothetical and more realistic datasets; after accounting for scale some marginal measures (e.g., marginal rates of substitution or MWTP) are often more consistent than aggregate measures (e.g., total WTP, opt-in rates)",
      "role": "Result",
      "parents": [
        0,
        3,
        4
      ],
      "children": [
        6,
        7
      ]
    },
    {
      "id": 6,
      "text": "Claim: Hypothetical bias arises from multiple sources including (a) lack of incentive/payment consequentiality, (b) perceived lack of policy consequentiality, (c) social desirability and warm-glow effects for moral/public goods, (d) hot-cold empathy gaps and lack of experiential/contextual realism, (e) cognitive dissonance and choice-induced preference change, and (f) design artifacts and cognitive simplification",
      "role": "Claim",
      "parents": [
        0,
        2,
        3
      ],
      "children": [
        8,
        9
      ]
    },
    {
      "id": 7,
      "text": "Limitation: Empirical testing of HB is constrained by scarcity of a universal 'true' benchmark (naturalistic revealed data often unavailable), heterogeneity in HB operationalisation across studies (different measures, benchmarks, and metrics), and limited cross-domain comparability",
      "role": "Limitation",
      "parents": [
        0,
        1,
        2
      ],
      "children": null
    },
    {
      "id": 8,
      "text": "Claim: Moderating factors influence HB magnitude and direction — these include product/service characteristics (public vs private, moral content, desirable vs aversive), study design (within vs between, inclusion of opt-out/status quo), incentive alignment (incentive-compatible mechanisms), information salience and contextual tangibility, stake size and payment vehicle, and respondent characteristics (gender, experience, personality)",
      "role": "Claim",
      "parents": [
        0,
        6
      ],
      "children": [
        10
      ]
    },
    {
      "id": 9,
      "text": "Evidence: Psychology and neuroimaging studies provide mechanistic support — real versus hypothetical conditions elicit differential behavioral and neural responses (e.g., stronger activation in valuation and affective regions for real choices; contextual richness reduces gaps; goods vs bads can reverse HB direction), corroborating hot-cold empathy gap, affect and cognitive-control explanations",
      "role": "Evidence",
      "parents": [
        0,
        6
      ],
      "children": [
        10
      ]
    },
    {
      "id": 10,
      "text": "Claim: Bias mitigation strategies should be tailored to identified sources and moderating factors; examples include incentive alignment, consequentiality scripts, cheap talk, certainty calibration, enhancing contextual tangibility (images, VR, simulators), pivot/referencing designs, and timing choices in 'hot' states",
      "role": "Claim",
      "parents": [
        0,
        8,
        9
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Conclusion: Hypothetical bias is an undeniable component of external validity for CEs but does not render CEs unusable; after accounting for scale, selecting appropriate benchmarks, and adopting targeted mitigation, CEs can produce useful estimates for policy and forecasting, though caution and context-specific validation remain necessary",
      "role": "Conclusion",
      "parents": [
        0,
        3,
        5,
        10,
        7
      ],
      "children": [
        12
      ]
    },
    {
      "id": 12,
      "text": "Claim: Pragmatic approach for HB testing — use graded realism benchmarks (Classes I–V) and choose the most realistic feasible comparator for the CE application; weight tests by benchmark realism when synthesising evidence or conducting meta-analysis",
      "role": "Claim",
      "parents": [
        0,
        2,
        11
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Result: Measurement patterns — hypothetical treatments frequently overestimate opt-in rates and total willingness to pay, may not systematically affect marginal trade-offs after scale adjustment, and can produce reversed bias for aversive goods or time valuation under affective states",
      "role": "Result",
      "parents": [
        0,
        4,
        5,
        9
      ],
      "children": null
    }
  ]
}