{
  "nodes": [
    {
      "id": 0,
      "text": "Hypothetical bias in choice experiments (CEs) affects external validity by causing deviations between measures inferred from hypothetical choice data and those from more realistic choice settings",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        6,
        7,
        11
      ]
    },
    {
      "id": 1,
      "text": "Systematic literature search across Web of Science and supplementary forward/backward searches produced a core dataset of empirical CE studies comparing hypothetical and more realistic/incentivised outcomes",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2,
        14
      ]
    },
    {
      "id": 2,
      "text": "Core dataset comprises 57 peer-reviewed articles (58 detailed in analysis) spanning environmental, consumer, health and transport economics plus psychology/neuroscience studies",
      "role": "Method",
      "parents": [
        0,
        1
      ],
      "children": [
        3,
        6
      ]
    },
    {
      "id": 3,
      "text": "Overall empirical evidence on hypothetical bias in CEs is mixed: some studies find large bias, others negligible or reversed bias; results are context- and measurement-dependent",
      "role": "Result",
      "parents": [
        0,
        2
      ],
      "children": [
        4,
        5,
        6
      ]
    },
    {
      "id": 4,
      "text": "Domain patterns: health studies mostly find negligible HB (within-subject, real benchmarks); transport studies frequently find significant HB often as downward bias in value-of-time; consumer studies often find upward HB in opt-in and total WTP but mixed for marginal WTP; environmental studies mixed with evidence for scale adjustments",
      "role": "Evidence",
      "parents": [
        3
      ],
      "children": [
        5,
        7
      ]
    },
    {
      "id": 5,
      "text": "A recurring empirical observation is systematic scale differences between hypothetical (SP) and more realistic (RP or incentivised) datasets, often requiring scale correction before comparing rates of substitution or marginal estimates",
      "role": "Claim",
      "parents": [
        3,
        4
      ],
      "children": [
        14,
        12
      ]
    },
    {
      "id": 6,
      "text": "Operationalisation and measurement of HB in CEs is highly variable across studies (different benchmarks, measures such as TWTP, MWTP, opt-in rates, simulated probabilities, market shares), hindering comparability",
      "role": "Limitation",
      "parents": [
        0,
        2,
        3
      ],
      "children": [
        14
      ]
    },
    {
      "id": 7,
      "text": "Multiple sources and psychological mechanisms proposed for HB include lack of consequentiality/incentive-compatibility, social desirability and warm-glow, strategic response, hot-cold empathy gaps, cognitive dissonance, information and attribute salience, and design-induced simplifying heuristics",
      "role": "Claim",
      "parents": [
        0,
        4,
        3
      ],
      "children": [
        9,
        12
      ]
    },
    {
      "id": 8,
      "text": "Moderating factors that influence magnitude/direction of HB include good type (public vs private; moral vs neutral), stake size and cost range, respondent characteristics (gender, familiarity, personality), experiment design (within vs between, opt-out presence), and benchmark realism",
      "role": "Claim",
      "parents": [
        0,
        7
      ],
      "children": [
        12,
        11
      ]
    },
    {
      "id": 9,
      "text": "Psychology and neuroimaging studies provide mechanistic evidence: some tasks show hypothetical-real consistency, others show differential neural activation or choice divergence supporting hot-cold empathy and affect-related explanations for HB",
      "role": "Evidence",
      "parents": [
        7
      ],
      "children": [
        3,
        12
      ]
    },
    {
      "id": 10,
      "text": "Despite HB being an undeniable issue in many contexts, the empirical evidence does not imply that CEs are unable to represent real-world preferences when appropriately designed and interpreted",
      "role": "Conclusion",
      "parents": [
        3,
        5,
        6,
        7
      ],
      "children": null
    },
    {
      "id": 11,
      "text": "Recommendation: evaluate HB across graded degrees of realism (five-class realism continuum from online hypothetical to naturalistic data) and use more realistic benchmarks when feasible to assess external validity",
      "role": "Method",
      "parents": [
        0,
        6,
        8
      ],
      "children": [
        12,
        13
      ]
    },
    {
      "id": 12,
      "text": "Mitigation strategies (e.g., incentive alignment, cheap talk, certainty calibration, enhanced contextual salience, virtual reality or simulator immersion) can reduce HB in some settings but are not universally effective or applicable",
      "role": "Claim",
      "parents": [
        5,
        7,
        8,
        11
      ],
      "children": null
    },
    {
      "id": 13,
      "text": "Major limitation: in many CE applications a true revealed-preference gold standard does not exist (novel goods or nonmarket goods), so benchmarks are pragmatic realistic treatments rather than perfect 'true' preferences",
      "role": "Limitation",
      "parents": [
        6,
        11
      ],
      "children": null
    },
    {
      "id": 14,
      "text": "Inclusion criteria and extraction protocol: studies had to report empirical comparisons between hypothetical and a less hypothetical or real treatment with comparable discrete choice model measures; extracted fields included context, benchmark type, opt-out inclusion, sample type, within/between design, estimation method and HB findings",
      "role": "Method",
      "parents": [
        1,
        5,
        6
      ],
      "children": null
    }
  ]
}