{
  "nodes": [
    {
      "id": 0,
      "text": "Hypothetical bias (HB) in choice experiments (CEs) affects external validity of CE-derived preference and willingness-to-pay measures, with prevalence, magnitude and direction dependent on context, measurement and design",
      "role": "Hypothesis",
      "parents": null,
      "children": [
        1,
        2,
        3,
        4,
        5,
        6,
        7
      ]
    },
    {
      "id": 1,
      "text": "Systematic multidisciplinary literature review: two Web of Science searches plus Google Scholar, forward/backward citation checks produced a core dataset of 57 peer-reviewed empirical CE studies comparing hypothetical and more realistic choice data (search updated to April 2020)",
      "role": "Method",
      "parents": [
        0
      ],
      "children": [
        2,
        7
      ]
    },
    {
      "id": 2,
      "text": "Overall empirical finding: evidence on HB in CEs is mixed across studies, disciplines and measures rather than uniformly positive or negative",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        3,
        4,
        5,
        6,
        11
      ]
    },
    {
      "id": 3,
      "text": "Health-economics studies predominantly report negligible or no substantial HB when comparing hypothetical CE outcomes to real-world benchmarks (majority of within-subject designs, varied benchmarks like uptake, sensitivity/specificity, market shares)",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": [
        11
      ]
    },
    {
      "id": 4,
      "text": "Transport and many consumer-economics studies frequently report significant HB; transport often finds downward bias in valuation of travel time savings while consumer studies commonly find overstatement of opt-in rates and total WTP",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": [
        6,
        11
      ]
    },
    {
      "id": 5,
      "text": "Environmental/resource CE evidence is mixed: some studies show congruence after scale adjustments, others show significant HB for public goods or donation contexts, and moral goods often elicit larger HB",
      "role": "Evidence",
      "parents": [
        2
      ],
      "children": [
        6,
        11
      ]
    },
    {
      "id": 6,
      "text": "A unifying empirical observation is the frequent presence of scale differences between hypothetical and more realistic data; marginal rates of substitution or scale-adjusted estimates can reduce apparent discrepancies",
      "role": "Claim",
      "parents": [
        2,
        4,
        5
      ],
      "children": [
        11
      ]
    },
    {
      "id": 7,
      "text": "Measurement and operationalisation heterogeneity: studies use varied benchmarks (revealed preference, incentivised SP, lab-in-field, simulator, self-report), different measures (TWTP, MWTP, opt-in rates, market shares, sensitivity/specificity), and inconsistent definitions of HB, hindering comparability",
      "role": "Claim",
      "parents": [
        0,
        1
      ],
      "children": [
        9,
        11,
        12
      ]
    },
    {
      "id": 8,
      "text": "Identified sources and psychological mechanisms of HB include lack of incentive consequentiality (individual and policy), social desirability and warm-glow, strategic response, hot-cold empathy gaps (affect/state-dependent misprediction), cognitive dissonance/choice-induced preference change, information and attribute salience, and design-induced cognitive shortcuts",
      "role": "Claim",
      "parents": [
        0
      ],
      "children": [
        10,
        11
      ]
    },
    {
      "id": 9,
      "text": "Proposed unified definition: hypothetical bias in CEs is the deviation in a predefined aggregate or disaggregate measure due to choice data being collected in a hypothetical setting instead of a more realistic (but not necessarily naturalistic) setting",
      "role": "Claim",
      "parents": [
        7
      ],
      "children": [
        11
      ]
    },
    {
      "id": 10,
      "text": "Degrees of realism taxonomy for benchmarks (Class I online hypothetical to Class V naturalistic revealed data) and recommendation to weight/interpret HB tests by benchmark realism; also use within-subject sensitivity/specificity and aggregate simulation outputs where appropriate",
      "role": "Method",
      "parents": [
        8
      ],
      "children": [
        11
      ]
    },
    {
      "id": 11,
      "text": "Conclusion: HB is an undeniable concern for CEs but does not render CEs unusable; appropriate benchmarking, scale adjustment, context-specific interpretation, and targeted mitigation can preserve CE usefulness for policy and valuation",
      "role": "Conclusion",
      "parents": [
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10
      ],
      "children": [
        13
      ]
    },
    {
      "id": 12,
      "text": "Limitation: lack of universal 'true preference' benchmark (naturalistic RP rarely available), heterogeneous study designs and small number of CE-specific HB tests limit meta-analytic quantification and generalisability",
      "role": "Limitation",
      "parents": [
        1,
        7
      ],
      "children": [
        11
      ]
    },
    {
      "id": 13,
      "text": "Research implication and next steps: identify context-specific moderating factors (good type, moral dimension, sample characteristics, stake size), develop and test mitigation strategies (Part II addresses effectiveness), and incorporate psychology/neuroscience insights (neurocognitive mechanisms) into CE design",
      "role": "Claim",
      "parents": [
        11
      ],
      "children": null
    }
  ]
}