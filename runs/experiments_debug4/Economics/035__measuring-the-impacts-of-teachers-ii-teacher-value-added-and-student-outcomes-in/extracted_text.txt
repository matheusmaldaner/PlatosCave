--- Page 1 ---
American Economic Review 2014, 104(9): 2633–2679 
http://dx.doi.org/10.1257/aer.104.9.2633
2633
Measuring the Impacts of Teachers II: 
Teacher Value-Added and Student Outcomes in Adulthood †
By Raj Chetty, John N. Friedman, and Jonah E. Rockoff *
Are teachers’ impacts on students’ test scores (value-added) a good 
measure of their quality? This question has sparked debate partly 
because of a lack of evidence on whether high value-added (VA) 
teachers improve students’ long-term outcomes. Using school district 
and tax records for more than one million children, we find that 
students assigned to high-VA teachers are more likely to attend college, 
earn higher salaries, and are less likely to have children as teenagers. 
Replacing a teacher whose VA is in the bottom 5 percent with an average 
teacher would increase the present value of students’  lifetime income by 
approximately $250,000 per classroom. (JEL H75, I21, J24, J45)
How can we measure and improve the quality of teaching in primary schools? One 
prominent but controversial method is to evaluate teachers based on their impacts 
on students’ test scores, commonly termed the value-added (VA) approach.1 School 
districts from Washington, DC to Los Angeles have begun to calculate VA measures 
and use them to evaluate teachers. Advocates argue that selecting teachers on the basis 
of their VA can generate substantial gains in achievement (e.g., Gordon, Kane, and 
Staiger 2006; Hanushek 2009), while critics contend that VA measures are poor prox-
ies for teacher quality (e.g., Baker et al. 2010; Corcoran 2010). The debate about 
teacher VA stems primarily from two questions. First, do the differences in test score 
1 Value-added models of teacher quality were pioneered by Hanushek (1971) and Murnane (1975). More recent 
examples include Rockoff (2004), Rivkin, Hanushek, and Kain (2005), Aaronson, Barrow, and Sander (2007), and 
Kane and Staiger (2008).
* Chetty: Harvard University, Littauer Center 226, Cambridge, MA 02138 (e-mail: chetty@fas.harvard.edu); 
Friedman: Harvard University, Taubman Center 356, Cambridge, MA 02138 (e-mail: john_friedman@harvard.
edu); Rockoff: Columbia University, Uris 603, New York, NY 10027 (e-mail: jonah.rockoff@columbia.edu). We 
thank Joseph Altonji, Josh Angrist, David Card, Gary Chamberlain, David Deming, Caroline Hoxby, Guido Imbens, 
Brian Jacob, Thomas Kane, Lawrence Katz, Michal Kolesar, Adam Looney, Phil Oreopoulos, Jesse Rothstein, 
Douglas Staiger, Danny Yagan, anonymous referees, and numerous seminar participants for helpful discussions 
and comments. This paper is the second of two companion papers on teacher quality. The results in the two papers 
were previously combined in NBER Working Paper 17699, entitled “The Long-Term Impacts of Teachers: Teacher 
Value-Added and Student Outcomes in Adulthood,” issued in December 2011. On May 4, 2012, Raj Chetty was 
retained as an expert witness by Gibson, Dunn, and Crutcher LLP to testify about the importance of teacher effec-
tiveness for student learning in Vergara v. California based on the findings in NBER Working Paper 17699. John 
Friedman is currently on leave from Harvard, working at the National Economic Council; this work does not repre-
sent the views of the NEC. All results based on tax data contained in this paper were originally reported in an IRS 
Statistics of Income white paper (Chetty, Friedman, and Rockoff 2011a). Sarah Abraham, Alex Bell, Peter Ganong, 
Sarah Griffis, Jessica Laird, Shelby Lin, Alex Olssen, Heather Sarsons, and Michael Stepner provided outstand-
ing research assistance. Financial support from the Lab for Economic Applications and Policy at Harvard and the 
National Science Foundation is gratefully acknowledged. Publicly available portions of the analysis code are posted 
at: http://obs.rc.fas.harvard.edu/chetty/cfr_analysis_code.zip.
† Go to http://dx.doi.org/10.1257/aer.104.9.2633 to visit the article page for additional materials and author 
disclosure statement(s).


--- Page 2 ---
2634
THE AMERICAN ECONOMIC REVIEW
September 2014
gains across teachers measured by VA capture causal impacts of teachers or are they 
biased by student sorting? Second, do teachers who raise test scores improve their 
students’ outcomes in adulthood or are they simply better at teaching to the test?
We addressed the first question in the previous paper in this volume (Chetty, 
Friedman, and Rockoff 2014) and concluded that VA measures which control for 
lagged test scores exhibit little or no bias. This paper addresses the second question.2 
We study the long-term impacts of teachers by linking information from an admin-
istrative dataset on students and teachers in grades 3–8 from a large urban school 
district spanning 1989–2009 with selected data from United States tax records span-
ning 1996–2011. We match approximately 90 percent of the observations in the 
school district data to the tax data, allowing us to track approximately one million 
individuals from elementary school to early adulthood, where we measure outcomes 
such as earnings, college attendance, and teenage birth rates.
We use two research designs to estimate the long-term impacts of teacher qual-
ity: cross-sectional comparisons across classrooms and a quasi-experimental design 
based on teacher turnover. The first design compares the outcomes of students who 
were assigned to teachers with different VA, controlling for a rich set of student char-
acteristics such as prior test scores and demographics. We implement this approach 
by regressing long-term outcomes on the test-score VA estimates constructed in 
Chetty, Friedman, and Rockoff (2014). The identification assumption underlying this 
research design is selection on observables: unobserved determinants of outcomes in 
adulthood such as student ability must be unrelated to teacher VA conditional on the 
observable characteristics. Although this is a very strong assumption, the estimates 
from this approach closely match the quasi-experimental estimates for outcomes 
where we have adequate precision to implement both designs, supporting its validity.
We find that teacher VA has substantial impacts on a broad range of outcomes. 
A one standard deviation improvement in teacher VA in a single grade raises the 
probability of college attendance at age 20 by 0.82 percentage points, relative to a 
sample mean of 37 percent. Improvements in teacher quality also raise the quality of 
the colleges which students attend, as measured by the average earnings of previous 
graduates of that college. Students who are assigned higher VA teachers have steeper 
earnings trajectories in their 20s. At age 28, the oldest age at which we currently have 
a sufficiently large sample size to estimate earnings impacts, a one standard deviation 
increase in teacher quality in a single grade raises annual earnings by 1.3 percent. If 
the impact on earnings remains constant at 1.3 percent over the life cycle, students 
would gain approximately $39,000 on average in cumulative lifetime income from a 
one standard deviation improvement in teacher VA in a single grade. Discounting at a 
5 percent rate yields a present value gain of $7,000 at age 12, the mean age at which 
the interventions we study occur. We also find that improvements in teacher quality 
significantly reduce the probability of having a child while being a teenager, increase 
the quality of the neighborhood in which the student lives (as measured by the per-
centage of college graduates in that zip code) in adulthood, and raise participation 
rates in 401(k) retirement savings plans.
2 Recent work has shown that early childhood education has significant long-term impacts (e.g., Heckman et al. 
2010a, 2010b, Chetty et al. 2011), but there is no evidence to date on the long-term impacts of teacher quality as 
measured by value-added.


--- Page 3 ---
2635
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Our second design relaxes the assumption of selection on observables by exploit-
ing teacher turnover as a quasi-experimental source of variation in teacher quality. 
To understand this research design, suppose a high-VA fourth grade teacher moves 
from school A to another school in 1995. Because of this staff change, students 
entering grade 4 in school A in 1995 will have lower quality teachers on average 
than those in the prior cohort. If high-VA teachers improve long-term outcomes, we 
would expect college attendance rates and earnings for the 1995 cohort to be lower 
on average than the previous cohort. Building on this idea, we estimate teachers’ 
impacts by regressing changes in mean adult outcomes across consecutive cohorts 
of children within a school on changes in the mean VA of the teaching staff.
Using this design, we find that a one standard deviation improvement in teacher 
VA raises the probability of college attendance at age 20 by 0.86  percentage 
points, nearly identical to the estimate obtained from our first research design. 
Improvements in average teacher VA also increase the quality of colleges which 
students attend. The impacts on college outcomes are statistically significant with 
p < 0.01. Unfortunately, we have insufficient precision to obtain informative esti-
mates for earnings using the quasi-experimental design.
Our quasi-experimental results rest on the identification assumption that 
­high-frequency teacher turnover within school-grade cells is uncorrelated with stu-
dent and school characteristics. Several pieces of evidence support this assump-
tion. First, predetermined student and parent characteristics are uncorrelated with 
changes in the quality of teaching staff. Second, students’ prior test scores and con-
temporaneous scores in the other subject are also uncorrelated with changes in the 
quality of teaching staff in a given subject. Third, changes in teacher VA across 
cohorts have sharp effects on college attendance exactly in the year of the change 
but not in prior years or subsequent years.
The long-term impacts of teacher VA are slightly larger for females than males. 
Improvements in English teacher quality have larger impacts than improvements 
in math teacher quality. The impacts of teacher VA are roughly constant in per-
centage terms by parents’ income. Hence, higher income households, whose chil-
dren have higher earnings on average, should be willing to pay larger amounts for 
higher teacher VA. Teachers’ impacts are significant and large throughout grades 
4–8, showing that improvements in the quality of education can have large returns 
well beyond early childhood.
Our conclusion that teachers have long-lasting impacts may be surprising given 
evidence that teachers’ impacts on test scores fade out very rapidly in subsequent 
grades (Rothstein 2010; Carrell and West 2010; Jacob, Lefgren, and Sims 2010). 
We confirm this rapid fade-out in our data, but find that teachers’ impacts on earn-
ings are similar to what one would predict based on the cross-sectional correlation 
between earnings and contemporaneous test score gains. This pattern of fade-out 
and re-emergence echoes the findings of recent studies of early childhood interven-
tions (Deming 2009; Heckman et al. 2010b; Chetty et al. 2011).
To illustrate the magnitude of teachers’ impacts, we evaluate Hanushek’s (2009) 
proposal to replace teachers in the bottom 5  percent of the VA distribution with 
teachers of average quality. We estimate that replacing a teacher whose current VA 
is in the bottom 5 percent with an average teacher would increase the mean present 
value of students’ lifetime income by $250,000 per classroom over a teacher’s career, 


--- Page 4 ---
2636
THE AMERICAN ECONOMIC REVIEW
September 2014
­accounting for drift in teacher quality over time.3 However, because VA is estimated 
with noise, the gains from deselecting teachers based on data from a limited number of 
classrooms are smaller. The present value gain from deselecting the bottom 5 percent 
of teachers using three years of test score data is $185,000 per classroom on average.4 
This gain is still about ten times larger than recent estimates of the additional salary 
one would have to pay teachers to compensate them for the risk of evaluation based 
on VA measures (Rothstein 2013). This result suggests that VA could potentially be a 
useful tool for evaluating teacher performance if the signal quality of VA for long-term 
impacts does not fall substantially when it is used to evaluate teachers.
We also evaluate the expected gains from policies which pay bonuses to high-VA 
teachers to increase retention rates. The gains from such policies are only slightly 
larger than their costs because most bonus payments end up going to high-VA teach-
ers who would have stayed even without the additional payment. Replacing low-VA 
teachers may therefore be a more cost effective strategy to increase teacher quality 
in the short run than paying bonuses to retain high-VA teachers. In the long run, 
higher salaries could attract more high-VA teachers to the teaching profession, a 
potentially important benefit which we do not measure here.
The paper is organized as follows. In Section I, we formalize our estimating equa-
tions and explain how the parameters we estimate should be interpreted using a simple 
statistical model. Section II describes the data sources and reports summary statis-
tics as well as cross-sectional correlations between test scores and adult outcomes. 
Sections III and IV present results on teachers’ long-term impacts using the two 
research designs described above. We analyze the heterogeneity of teachers’ impacts 
in Section V. Section VI presents policy simulations and Section VII concludes.
I.  Conceptual Framework
In this section, we first present a simple statistical model of students’ long-term 
outcomes as a function of their teachers’ value-added. We then describe how the 
reduced-form parameters of this statistical model should be interpreted. Finally, we 
show how we estimate the impacts of teacher VA on long-term outcomes given that 
each teacher’s true value-added is unobserved.
Statistical Model.—Consider the outcomes of a student i who is in grade g in 
calendar year ​t​i​ (g). Let j = j(i, t) denote student i ’s teacher in school year t; for sim-
plicity, assume that the student has only one teacher throughout the school year, as 
in elementary schools. Let ​μ​jt​ represent teacher j ’s “test-score value-added” in year 
t, so that student i’s test score in year t is
(1)  	
​A​ it​ 
∗ ​  =  β​X​it​  +  ​μ​jt​  +  ​ε​it​.
Here, ​X​it​ denotes observable determinants of student achievement, such as lagged 
3 This calculation discounts the earnings gains at a rate of 5 percent to age 12. The estimated total undiscounted 
earnings gains from this policy are approximately $50,000 per child and $1.4 million for the average classroom.
4 The gains remain substantial despite noise because very few of the teachers with low VA estimates ultimately 
turn out to be excellent teachers. For example, we estimate that 3.2 percent of the math teachers in elementary school 
whose estimated VA is in the bottom 5 percent based on three years of data actually have true VA above the median.


--- Page 5 ---
2637
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
test scores and family characteristics and ​ε​it​ denotes a student-level error which may 
be correlated across students within a classroom and with teacher value-added ​μ​jt​. 
We scale ​μ​jt​ in student test-score standard deviations so that the average 
teacher has ​μ​jt​ = 0 and the effect of a 1 unit increase in teacher value-added on ­ 
end-­of-year test scores is 1. We allow teacher quality ​μ​jt​ to vary with time t to 
account for the stochastic drift in teacher quality documented in our companion 
paper (Chetty, Friedman, and Rockoff 2014).
Let ​Y​ i​ ∗​ denote student i’s earnings in adulthood. Throughout our analysis, we 
focus on earnings residuals after removing the effect of observable characteristics:
(2)  	
​Y​it​  =  ​Y​ i​ ∗​  −  ​β​Y​X​it​ .
The earnings residuals ​Y​it​ vary across school years because the control vector ​X​it​ 
varies across school years. We estimate the coefficient vector ​β​Y​ using variation 
across students taught by the same teacher using an OLS regression:
(3)  	
​Y​ i​ ∗​  =  ​α​j​  +  ​β​Y​X​it​ ,
where ​α​j​ is a teacher fixed effect. Importantly, we estimate ​β​Y​ using within-teacher 
variation to account for the potential sorting of students to teachers based on VA. If 
teacher VA is correlated with ​X​it​, estimates of ​β​Y​ in a specification without teacher 
fixed effects are biased because part of the teacher effect is attributed to the covari-
ates. See Section IB of our companion paper for further discussion of this issue.
We model the relationship between earnings residuals and teacher VA in school 
year t using the following linear specification:
(4)  	
​Y​it​  =  a  +  ​κ​g​ ​m​jt​  +  ​η​it​ ,
where ​m​jt​ = ​μ​jt​/​σ​μ​ denotes teacher j’s normalized value-added (i.e., teacher quality 
scaled in standard deviation (​σ​μ​) units of the teacher VA distribution).
Interpretation of Reduced-Form Treatment Effects.—The parameter ​κ​g​ in (4) rep-
resents the reduced-form impact of a one standard deviation increase in teachers’ 
test-score VA in a given school year t on earnings. There are two important issues to 
keep in mind when interpreting this parameter, which we formalize using a dynamic 
model of education production in online Appendix A.
First, the reduced-form impact combines two effects: the direct impact of hav-
ing a higher-VA teacher in grade g and the indirect impact of endogenous changes 
in other educational inputs. For example, other determinants of earnings such 
as investments in learning by children and their parents might respond endog-
enously to changes in teacher quality. One particularly important endogenous 
response is that a higher achieving student may be tracked into classes taught 
by ­higher-quality teachers. Such tracking would lead us to overstate the impacts 
of improving teacher quality in grade g holding fixed the quality of teachers in 


--- Page 6 ---
2638
THE AMERICAN ECONOMIC REVIEW
September 2014
subsequent grades. In Section VC, we estimate the degree of teacher tracking and 
use these estimates to identify the impact of having a higher-VA teacher in each 
grade holding fixed future teacher quality, which we denote by ​ ˜ κ​g​. The degree of 
tracking turns out to be relatively small in our data, and thus the reduced-form 
estimates of ​κ​g​ reported below are similar to the net impacts of each teacher ​ ˜ κ​g​.5 
Although the net impacts ​ ˜ κ​g​ still combine several structural parameters—such as 
endogenous responses by parents and children to changes in teacher quality—they 
are relevant for policy. For example, the ultimate earnings impact of retaining 
teachers on the basis of their VA depends on ​ ˜ κ​g​.
Second, ​ ˜ κ​g​ measures only the portion of teachers’ earnings impacts which are 
correlated with their impacts on test scores. As a result, ​ ˜ κ​g​ is a lower bound for the 
standard deviation of teachers’ effects on earnings. Intuitively, some teachers may 
be effective at raising students’ earnings even if they are not effective at raising 
test scores, for instance by directly instilling other skills which have ­long-term 
payoffs (Jackson 2013). In principle, one could estimate teacher j’s earnings 
value-added ​( ​μ​ jt​ 
Y ​  )​ based on the mean residual earnings of her students, exactly 
as we estimated test-score VA in our first paper. Unfortunately, the orthogonality 
condition required to obtain unbiased forecasts of teachers’ earnings VA—that 
other unobservable determinants of students’ earnings are orthogonal to earnings 
VA estimates—does not hold in practice, as we discuss in online Appendix A. We 
therefore focus on estimating the effect of being assigned to a high test-score VA 
teacher on earnings (​κ​g​). Although ​κ​g​ does not correspond directly to earnings VA, 
it reveals the extent to which the test-score-based VA measures currently used by 
school districts are informative about teachers’ long-term impacts.
Empirical Implementation.—There are two challenges in estimating ​κ​g​ using (4). 
First, unobserved determinants of earnings ​η​it​ may be correlated with teacher VA ​m​jt​ . 
We return to this issue in our empirical analysis and isolate variation in teacher VA 
which is orthogonal to unobserved determinants of earnings. Second, teachers’ true 
test-score VA ​m​jt​ is unobserved. We can solve this second problem by substituting 
estimates of teacher VA ​  
m​jt​ = ​  
μ​jt​/​σ​μ​ for true teacher VA in (4) under the following 
assumption.
Assumption 1 (Forecast Unbiasedness of Test-Score VA): Test-score 
value-added estimates ​  
μ​jt​ are forecast unbiased:
  	
​ Cov​( ​μ​jt​, ​  
μ​jt​ )​ 
_ 
Var​( ​  
μ​jt​ )​  ​  =  ​ 
Cov​( ​m​jt​, ​  
m​jt​ )​ 
_ 
Var​( ​  
m​jt​ )​  ​  =  1.
5 An alternative approach to identify the direct impacts of teachers in each grade g would be to include teacher 
VA in all grades simultaneously in the model in (4). Unfortunately, this is not feasible because our primary research 
design requires conditioning on lagged test scores and lagged scores are endogenous to teacher quality in the previ-
ous grade. For the same reason, we also cannot identify the substitutability or complementarity of teachers’ impacts 
across grades. See online Appendix A for further details.


--- Page 7 ---
2639
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
In our companion paper, we demonstrate that Assumption 1 holds for the VA estimates 
which we use in this paper. Equation (4) implies that Cov​( ​Y​it​, ​  
m​jt​ )​ = ​κ​g​Cov​( ​m​jt​, ​  
m​jt​ )​ + 
Cov(​η​it​, ​  
m​jt​). Hence, under Assumption 1,
  	
​ 
Cov​( ​Y​it​, ​  
m​jt​ )​ 
_ 
Var​( ​  
m​jt​ )​  ​  =  ​κ​g​  +  ​ 
Cov​( ​η​it​, ​  
m​jt​ )​ 
_ 
Var​( ​  
m​jt​ )​  ​ .
It follows that we can identify the impact of a one standard deviation increase in a 
teacher’s true VA ​m​jt​ from an OLS regression of earnings residuals ​Y​it​ on teacher VA 
estimates ​  
m​jt​,
(5)  	
​Y​it​  =  α  +  ​κ​g​ ​  
m​jt​  +  ​η​ it​ ′ ​,
provided that unobserved determinants of earnings are orthogonal to teacher VA 
estimates ​  
m​jt​.
Intuitively, we identify ​κ​g​ using two-stage least squares (2SLS) by instrumenting 
for true teacher VA ​m​jt​ with the teacher VA estimates we constructed in our compan-
ion paper. Forecast unbiasedness of test-score VA implies that the first stage of this 
2SLS regression has a coefficient of 1. Thus, the reduced form coefficient obtained 
from an ordinary least squares (OLS) regression of earnings on VA estimates identi-
fies ​κ​g​.
The remainder of the paper focuses on estimating variants of (5).6
II.  Data
We draw information from two databases: administrative school district records 
and federal income tax records. This section describes the two data sources and the 
structure of the linked analysis dataset. We then provide descriptive statistics and 
report correlations between test scores and long-term outcomes as a benchmark to 
interpret the magnitude of the causal effects of teachers. Note that the dataset we 
use in this paper is identical to that used in our first paper, except that we restrict 
attention to the subset of students who are old enough for us to observe outcomes in 
adulthood by 2011.
A. School District Data
We obtain information on students, including enrollment history, test scores, and 
teacher assignments from the administrative records of a large urban school dis-
trict. These data span the 1988–1989 to 2008–2009 school years and cover roughly 
2.5 million children in grades 3–8. For simplicity, we refer to school years by the 
year in which the spring term occurs (e.g., the school year 1988–1989 is 1989). 
6 Another way to identify ​κ​g​ is to directly estimate the covariance of teachers’ effects on earnings ​( ​μ​ jt​ 
Y ​  )​  and test 
scores ( ​μ​jt​) in a correlated random effects or factor model. Chamberlain (2013) develops such an approach and 
obtains estimates similar to those reported here.


--- Page 8 ---
2640
THE AMERICAN ECONOMIC REVIEW
September 2014
We ­summarize the key features of the data relevant for our analysis of ­teachers’ 
­long-term impacts here; see Section II of our first paper for a comprehensive descrip-
tion of the school district data.
Test Scores.—The data include approximately 18 million test scores. Test scores 
are available for English language arts and math for students in grades 3–8 in every 
year from the spring of 1989 to 2009, with the exception of seventh grade English 
scores in 2002. We follow prior work by normalizing the official scale scores from 
each exam to have mean zero and standard deviation one by year and grade. The 
within-grade variation in achievement in the district we examine is comparable to 
the within-grade variation nationwide, so our results can be compared to estimates 
from other samples.
Demographics.—The dataset contains information on ethnicity, gender, age, receipt 
of special education services, and limited English proficiency for the school years from 
1989 to 2009. The database used to code special education services and limited English 
proficiency changed in 1999, creating a break in these series which we account for in 
our analysis by interacting these two measures with a post-1999 indicator. Information 
on free and reduced-price lunch is available starting in school year 1999.
Teachers.—The dataset links students in grades 3–8 to classrooms and teachers 
from 1991 through 2009. This information is derived from a data management sys-
tem which was phased in over the early 1990s, so not all schools are included in 
the first few years of our sample. In addition, data on course teachers for middle- 
and junior high school students—who, unlike students in elementary schools, are 
assigned different teachers for math and English—are more limited. Course teacher 
data are unavailable prior to the school year 1994, then grow in coverage to roughly 
60 percent by school year 1998 and 85 percent by 2003. These missing teacher 
links raise two potential concerns. First, our estimates (especially for grades 6–8) 
apply to a subset of schools with more complete information reporting systems and 
thus may not be representative of the district as a whole. Reassuringly, we find that 
these schools do not differ significantly from the sample as a whole on test scores 
and other observables. Second, and more importantly, missing data could generate 
biased estimates. We address this concern by showing that our estimates remain 
similar in a subsample of school-grade-subject cells with little or no missing data 
(online Appendix Table 7).
Sample Restrictions.—Starting from the raw dataset, we make a series of restrictions 
which parallel those in prior work to obtain our primary school district sample. First, 
because our estimates of teacher value-added always condition on prior test scores, 
we restrict our sample to grades 4–8, where prior test scores are available. Second, we 
exclude the 6 percent of observations in classrooms where more than 25 percent of 
students are receiving special education services, as these classrooms may be taught 
by multiple teachers or have other special teaching arrangements. We also drop the 
2 percent of observations where the student is listed as receiving instruction at home, 
in a hospital, or in a school serving disabled students solely. Third, we drop class-
rooms with less than 10 students or more than 50 students as well as ­teachers linked 


--- Page 9 ---
2641
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
with more than 200 students in a single grade, because such students are likely to be 
mislinked to classrooms or teachers (0.5 percent of observations). Fourth, when a 
teacher is linked to students in multiple schools during the same year, which occurs for 
0.3 percent of observations, we use only the links for the school where the teacher is 
listed as working according to human resources records and set the teacher as missing 
in the other schools. Finally, because the adult outcomes we analyze are measured at 
age 20 or afterward, in this paper we restrict the sample to students who would have 
graduated high school by the 2008–2009 school year (and thus turned 20 by 2011) if 
they progressed through school at a normal pace.7
B. Tax Data
We obtain information on students’ outcomes in adulthood from US federal 
income tax returns spanning 1996–2011.8 The school district records were linked to 
the tax data using an algorithm based on standard identifiers (date of birth, state of 
birth, gender, and names) described in online Appendix C of our companion paper, 
after which individual identifiers were removed to protect confidentiality. In all, 
87.4 percent of the students and 89.2 percent of student-subject-year observations 
in the sample used to analyze long-term impacts were matched to the tax data.9 We 
define students’ outcomes in adulthood as follows.
Earnings.—Individual wage earnings data come from W-2 forms, which are avail-
able from 1999 to 2011. Importantly, W-2 data are available for both tax filers and 
non-filers, eliminating concerns about missing data on formal sector earnings. We 
cap earnings in each year at $100,000 to reduce the influence of outliers; 1.3 percent 
of individuals in the sample report earnings above $100,000 at age 28. We measure 
all monetary variables in 2010 US$, adjusting for inflation using the Consumer Price 
Index. Individuals with no W-2 are coded as having zero earnings; 33.1 percent of 
individuals have zero wage earnings at age 28 in our sample.
Total Income.—To obtain a more comprehensive definition of income, we define 
total income as the sum of W-2 wage earnings and household self-employment earn-
ings (as reported on the 1040). For non-filers, we define total income as just W-2 
wage earnings; those with no W-2 income are coded as having zero total income. 
In our sample, 29.6 percent of individuals have zero total income.10 We show that 
­similar results are obtained using this alternative definition of income, but use W-2 
7 A few classrooms contain students at different grade levels because of retentions or split-level classroom struc-
tures. To avoid dropping a subset of students within a classroom, we include every classroom which has at least one 
student who would graduate school during or before 2008–2009 if she progressed at the normal pace. That is, we 
include all classrooms in which mi​n​i​ (12 + school year − grad​e​i​ ) ≤ 2009.
8 Here and in what follows, the year refers to the tax year: i.e., the calendar year in which income is earned. In 
most cases, tax returns for tax year t are filed during the calendar year t + 1.
9 We find little or no correlation between match rates and teacher VA in the various subsamples we use in our 
analysis and obtain very similar estimates of teachers’ impacts on long-term outcomes when restricting the sample 
to school-grade-subject cells with above-median match rates (see online Appendix Table 7).
10 According to the Current Population Survey, 27.2 percent of the noninstitutionalized population between the 
ages of 25 and 29 was not employed in 2011. The nonemployment rate in our sample may differ from this figure 
because it includes the institutionalized population in the denominator and applies to a relatively low-income urban 
public school district.


--- Page 10 ---
2642
THE AMERICAN ECONOMIC REVIEW
September 2014
wage earnings as our baseline measure because it (i) is unaffected by the endogene-
ity of tax filing and (ii) provides a consistent definition of individual (rather than 
household) income for both filers and non-filers.
College Attendance.—We define college attendance as an indicator for having one 
or more 1098-T forms filed on one’s behalf. Title IV institutions—all colleges and 
universities as well as vocational schools and other postsecondary institutions eli-
gible for federal student aid—are required to file 1098-T forms which report tuition 
payments or scholarships received for every student. Because the 1098-T forms are 
filed directly by colleges independent of whether an individual files a tax return, we 
have complete records on college attendance for all individuals. The 1098-T data 
are available from 1999 to 2011. Comparisons to other data sources indicate that 
1098-T forms capture college enrollment accurately (see online Appendix B).
College Quality.—We construct an earnings-based index of college quality based 
on data from the universe of tax returns (not just the students from our school dis-
trict). Using the population of all current US citizens born in 1979 or 1980, we 
group individuals by the higher education institution they attended at age 20. We 
pool individuals who were not enrolled in any college at age 20 together in a sepa-
rate “no college” category. For each college or university (including the “no college” 
group), we then compute the mean W-2 earnings of the students when they are age 
31 (in 2010 and 2011). Among colleges attended by students in the school district 
studied in this paper, the average value of our earnings index is $44,048 for four-
year colleges and $30,946 for two-year colleges. For students who did not attend 
college, the mean earnings level is $17,920.
In online Appendix B, we analyze the robustness of the college quality index to 
alternative specifications, such as measuring earnings and college attendance at dif-
ferent ages and defining the index based on total income instead of W-2 earnings. 
We find that rankings of college quality are very stable across cohorts and are robust 
to alternative specifications provided that earnings are measured after age 28 (online 
Appendix Figure 1, online Appendix Table 2).
Neighborhood Quality.—We use data from 1040 forms to identify each household’s 
zip code of residence in each year. For non-filers, we use the zip code of the address to 
which the W-2 form was mailed. If an individual did not file and has no W-2 in a given 
year, we impute current zip code as the last observed zip code. We construct a measure 
of a neighborhood’s socioeconomic status (SES) using data on the percentage of col-
lege graduates in the individual’s zip code from the 2000 census.
Retirement Savings.—We measure retirement savings using contributions to 
401(k) accounts reported on W-2 forms from 1999 to 2011. We define saving for 
retirement as an indicator for contributing to a 401(k) at age 28.
Teenage Birth.—We define a woman as having a teenage birth if she ever claims 
a dependent who was born while she was between the ages of 13 and 19 (as of 
December 31 in the year the child was born). This measure is an imperfect proxy for 
having a teenage birth because it only covers children who are claimed as ­dependents 


--- Page 11 ---
2643
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
by their mothers and includes any other dependents who are not biological chil-
dren but were born while the mother was a teenager. Despite these limitations, our 
proxy for teenage birth is closely aligned with estimates based on the American 
Community Survey (ACS). In our core sample, 15.8 percent of women have teenage 
births, compared with 14.6 percent in the 2003 ACS. The unweighted correlation 
between state-level teenage birth rates in our data and the ACS is 0.80.
Parent Characteristics.—We also use the tax data to obtain information on five 
parent characteristics which we use as controls. Students were linked to parents 
based on the earliest 1040 form filed between tax years 1996 and 2011 on which 
the student was claimed as a dependent. We identify parents for 94.8 percent of the 
observations in the analysis dataset conditional on being matched to the tax data.11
We define parental household income as mean Adjusted Gross Income (capped 
at $117,000, the ninety-fifth percentile in our sample) between 2005 and 2007 for 
the primary filer who first claimed the child.12 Parents are assigned an income of 
zero in years when they did not file a tax return. We define parental marital status, 
home ownership, and 401(k) saving as indicators for whether the first primary filer 
who claims the child ever files a joint tax return, makes a mortgage interest payment 
(based on data from 1040s for filers and 1099s for non-filers), or makes a 401(k) 
contribution (based on data from W-2s) between 2005 and 2007. Lastly, we define 
mother’s age at child’s birth using data from Social Security Administration (SSA) 
records on birth dates for parents and children. For single parents, we define the 
mother’s age at child’s birth using the age of the filer who first claimed the child, 
who is typically the mother but is sometimes the father or another relative.13 When 
a child cannot be matched to a parent, we define all parental characteristics as zero, 
and we always include a dummy for missing parents in regressions which include 
parent characteristics.
C. Summary Statistics
The linked school district and tax record analysis dataset has one row per student 
per subject (math or English) per school year, as illustrated in online Appendix 
Table 1. Each observation in the analysis dataset contains the student’s test score in 
the relevant subject test, demographic information, and class and teacher assignment 
if available. Each row also includes all the students’ available adult outcomes (e.g., 
college attendance and earnings at each age). We organize the data in this format so 
that each row contains information on a treatment by a single teacher conditional on 
predetermined characteristics, facilitating the estimation of (5). We account for the 
11 The remaining students are likely to have parents who did not file tax returns in the early years of the sample 
when they could have claimed their child as a dependent, making it impossible to link the children to their parents. 
Note that this definition of parents is based on who claims the child as a dependent, and thus may not reflect the 
biological parent of the child.
12 Because the children in our sample vary in age by over 25 years whereas the tax data start only in 1996, we cannot 
measure parent characteristics at the same age for all children. For simplicity, we instead measure parent characteris-
tics at a fixed time. Measuring parent income at other points in time yields very similar results (not reported).
13 We set the mother’s age at child’s birth to missing for 78,007 observations in which the implied mother’s age 
at birth based on the claiming parent’s date of birth is below 13 or above 65, or where the date of birth is missing 
entirely from SSA records.


--- Page 12 ---
2644
THE AMERICAN ECONOMIC REVIEW
September 2014
fact that each student appears multiple times in the dataset by clustering standard 
errors as described in Section IIIA.
After imposing the sample restrictions described above, the linked analy-
sis sample contains 6.8 million student-subject-year observations (covering 1.1 
million students) which we use to study teachers’ long-term impacts.14 Table 1 
reports summary statistics for this sample. Note that the summary statistics are 
­student-subject-year means and thus weight students who are in the district for a 
longer period of time more heavily, as does our empirical analysis. On average, 
each student has 6.25 subject-school year observations.
The mean test score in the analysis sample is positive and has a standard deviation 
below one because we normalize the test scores in the full population which includes 
students in special education classrooms and schools (who typically have lower test 
scores). The mean age at which students are observed in school is 11.7 years. In addi-
tion, 77.1 percent of students are eligible for free or reduced price lunches.
The availability of data on adult outcomes naturally varies across cohorts. There 
are more than 5.9 million observations for which we observe college attendance 
at age 20. We observe earnings at age 25 for 2.3 million observations and at age 
28 for 1.3 million observations. Because many of these observations at later ages 
are from older cohorts of students who were in middle school in the early 1990s, 
we were not able to obtain information on teachers. As a result, there are only 1.6 
million student-subject-school year observations for which we see both teacher VA 
and earnings at age 25, 750,000 at age 28, and only 220,000 at age 30. The oldest 
age at which the sample is large enough to obtain informative estimates of teachers’ 
impacts on earnings turns out to be age 28. Mean individual earnings at age 28 is 
$20,885, while mean total income is $21,272 (in 2010 US$).
For students whom we are able to link to parents, the mean parent household 
income is $40,808, while the median is $31,834. Though our sample includes more 
low-income households than would a nationally representative sample, it still includes 
a substantial number of higher-income households, allowing us to analyze the impacts 
of teachers across a broad range of the income distribution. The standard deviation 
of parent income is $34,515, with 10 percent of parents earning more than $100,000.
D. Cross-Sectional Correlations
Online Appendix Tables 3–6 report coefficients from OLS regressions of various 
adult outcomes on test scores. Both math and English test scores are highly posi-
tively correlated with earnings, college attendance, and neighborhood quality, and 
are negatively correlated with teenage births. In the cross section, a one standard 
deviation increase in test score is associated with a $7,700 (36 percent) increase in 
earnings at age 28. Conditional on the student- and class-level controls ​X​it​ which 
we define in Section IIIA below, a one standard deviation increase in the current test 
score is associated with a $2,600 (12 percent) increase in earnings on average.
14 For much of the analysis in our first paper, we restricted attention to the subset of observations in the core sample 
which have lagged scores and other controls needed to estimate the baseline VA model. Because we do not control for 
individual-level variables in most of the specifications in this paper, we do not impose that restriction here.


--- Page 13 ---
2645
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Table 1—Summary Statistics for Linked Analysis Dataset
Mean
SD
Observations
Variable
(1)
(2)
(3)
Student data:
Class size (not student-weighted)
28.2
5.8
240,459
Number of subject-school years per student
6.25
3.18
1,083,556
Test score (SD)
0.12
0.91
6,035,726
Female
50.4%
6,762,896
Age (years)
11.7
1.6
6,762,679
Free lunch eligible (1999–2009)
77.1%
3,309,198
Minority (Black or Hispanic)
72.1%
6,756,138
English language learner
4.9%
6,734,837
Special education
3.1%
6,586,925
Repeating grade
2.7%
6,432,281
Matched to tax data
89.2%
6,770,045
Matched to parents (cond. on match to tax data)
94.8%
6,036,422
Adult outcomes:
Annual wage earnings at age 20
5,670
7,733
5,939,022
Annual wage earnings at age 25
17,194
19,889
2,321,337
Annual wage earnings at age 28
20,885
24,297
1,312,800
Total income at age 28
21,780
24,281
1,312,800
In college at age 20
35.6%
5,939,022
In college at age 25
16.5%
2,321,337
More than four years of college, ages 18–22
22.7%
4,514,758
College quality at age 20
26,408
13,461
5,934,570
Contributed to a 401(k) at age 28
19.1%
1,312,800
Percent college graduates in zip at age 28
13.7%
929,079
Had a child while a teenager (for women)
14.3%
3,032,170
Owned a house at age 25
4.3%
2,321,337
Married at age 25
11.3%
2,321,337
Parent characteristics:
Annual household income
40,808
34,515
5,720,657
Owned a house
34.8%
5,720,657
Contributed to a 401(k)
31.3%
5,720,657
Married
42.2%
5,720,657
Age at child birth
28.3
7.8
5,615,400
Notes: All statistics reported are for the linked analysis dataset described in Section II, which 
includes students from classrooms in which at least one student would graduate high school in or 
before 2009 if progressing at a normal pace. The sample has one observation per student-subject-
school year. Student data are from the administrative records of a large urban school district in the 
US. Adult outcomes and parent characteristics are from 1996–2011 federal income tax data. All 
monetary values are expressed in real 2010 US$. All ages refer to the age of an individual as of 
December 31 within a given year. Test score refers to standardized scale score in math or English. 
Free lunch is an indicator for receiving free or reduced-price lunches. We link students to their 
parents by finding the earliest 1040 form from 1996 to 2011 on which the student is claimed as a 
dependent. We are unable to link 10.8 percent of observations to the tax data; the summary statis-
tics for adult outcomes and parent characteristics exclude these observations. Wage earnings are 
measured from W-2 forms; we assign 0’s to students with no W-2s. Total income includes both 
W-2 wage earnings and self-employment income reported on the 1040. College attendance is 
measured from 1098-T forms. College quality is the average W-2 earnings at age 31 for students 
who attended a given college at age 20 (see Section IIB for more details). 401(k) contributions 
are reported on W-2 forms. Zip code of residence is determined from the address on a 1040 (or 
W-2 for non-filers); percent college graduates in zip is based on the 2000 Census. We measure 
teen births for female students as an indicator for claiming a dependent who was born fewer than 
20 years after the student herself was born. We measure home ownership from the payment of 
mortgage interest, reported on either the 1040 or a 1099 form. We measure marriage by the filing 
of a joint return. Conditional on linking to the tax data, we are unable to link 5.2 percent of obser-
vations to a parent; the summary statistics for parents exclude these observations. Parent income 
is average adjusted gross income during the three tax-years between 2005 and 2007. For parents 
who do not file, household income is defined as zero. Parent age at child birth is the difference 
between the age of the mother (or father if single father) and the student. All parent indicator vari-
ables are defined in the same way as the equivalent for the students and are equal to 1 if the event 
occurs in any year between 2005 and 2007.


--- Page 14 ---
2646
THE AMERICAN ECONOMIC REVIEW
September 2014
Online Appendix Figure  2 presents binned scatter plots of selected outcomes 
versus test scores both with and without controls. The unconditional relationship 
between scores and outcomes is S-shaped, while the relationship conditional on 
prior scores and other covariates is almost perfectly linear. We return to these results 
below and show that the causal impacts of teacher VA on earnings and other out-
comes are commensurate to what one would predict based on these correlations.
III.  Research Design 1: Cross-Class Comparisons
Our first method of estimating teachers’ long-term impacts builds on our finding 
that conditioning on prior test scores and other observables is adequate to obtain 
unbiased estimates of teachers’ causal impacts on test scores (Chetty, Friedman, 
and Rockoff 2014). Given this result, one may expect that comparing the long-term 
outcomes of students assigned to different teachers conditional on the same control 
vector will yield unbiased estimates of teachers’ long-term impacts. The next sub-
section formalizes the identification assumption and methodological details of this 
approach. We then present results for three sets of impacts: college attendance, earn-
ings, and other outcomes such as teenage birth.
A. Methodology
We begin by constructing earnings (or other long-term outcome) residuals 
​Y​it​ = ​Y​ i​ ∗​ − ​  β​
 Y​X​it​, estimating ​  β​
 Y​ using within-teacher variation as in (3). We then 
regress students’ earnings residuals on their teachers’ normalized VA ​  
m​jt​ , pooling 
all grades and subjects:
(6)  	
​Y​it​  =  α  +  κ ​  
m​jt​  +  ​η​ it​ ′ ​ .
Note that we do not residualize ​  
m​jt​ with respect to the controls ​X​it​ when estimating 
(6). In a partial regression, one residualizes ​  
m​jt​ typically with respect to ​X​it​ because 
the OLS regression of ​Y​ i​ ∗​ on ​X​it​ used to construct earnings residuals yields an esti-
mate of ​β​ Y​ which is biased by the correlation between and ​  
m​jt​ and ​X​it​. This problem 
does not arise here because we estimate ​β​ Y​ from a regression with teacher fixed 
effects, so the variation in ​X​it​ used to identify ​β​ Y​ is orthogonal to the variation in 
VA across teachers.15 Hence, regressing ​Y​it​ directly on ​  
m​jt​ identifies the relationship 
between ​Y​it​ and ​  
m​jt​ conditional on ​X​it​.
Recall that each student appears in our dataset once for every subject-year with 
the same level of ​Y​it​ but different values of ​  
m​jt​. Hence, κ represents the mean impact 
of having a higher VA teacher for a single grade between grades 4–8. We present 
results on heterogeneity in impacts across grades and subgroups in Section V.
15 Teacher fixed effects account for correlation between ​X​it​ and mean teacher VA. If ​X​it​ is correlated with fluc-
tuations in teacher VA across years due to drift, one may still obtain biased estimates of ​β​ Y​. This problem is 
modest because only 20 percent of the variance in ​  
m​jt​ is within teacher. Moreover, we obtain similar results when 
estimating ​  
m​jt​ and ​β​ Y​ from regressions without teacher fixed effects and implementing a standard partial regression 
(Chetty, Friedman, and Rockoff 2011b). We estimate ​β​ Y​ using within-teacher variation here for consistency with 
our approach to estimating teacher VA in the first paper; see Section IB of that paper for further details.


--- Page 15 ---
2647
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Estimating (6) using OLS yields an unbiased estimate of κ under the following 
assumption.
Assumption 2 (Selection on Observables): Test-score value-added estimates are 
orthogonal to unobserved determinants of earnings conditional on ​X​it​ :
(7)  	
Cov​( ​  
m​jt​, ​η​ it​ ′ ​ )​  =  0.
While we cannot be certain that conditioning on observables fully accounts for 
differences in student characteristics across teachers, as required by Assumption 2, 
the quasi-experimental evidence reported in the next section supports this assump-
tion. In particular, quasi-experimental estimates of the impacts of teacher quality on 
test scores and college attendance are very similar to the estimates obtained from 
(6).16 This result supports the validity of the selection on observables assumption 
not only for test scores and college attendance but also for other outcomes as well, 
as any selection effects would presumably be manifested in all of these outcomes.
Four methodological issues arise in estimating (6): (i) estimating test-score VA ​  
m​jt​; 
(ii) specifying a control vector ​X​it​; (iii) calculating the standard error on κ; and (iv) 
accounting for outliers in ​  
m​jt​. The remainder of this subsection addresses these four 
issues. Note that our methodology parallels closely that in our companion paper. In 
particular, we use the same VA estimates and control vectors and calculate standard 
errors in the same way. We did not address outliers in our first paper because they 
only affect our analysis of long-term impacts, as we explain below.
Estimating Test-Score VA.—We define normalized VA ​  
m​jt​ = ​  
μ​jt​/​σ​μ​, where ​  
μ​jt​ 
is the baseline estimate of test-score VA for teacher j in year t constructed in our 
companion paper.17 We define ​σ​μ​ as the standard deviation of teacher effects for 
the corresponding subject and school-level using the estimates in Table 2 of our 
companion paper: 0.163 for math and 0.124 for English in elementary school and 
0.134 for math and 0.098 for English in middle school. With this scaling, a one unit 
increase in ​  
m​jt​ corresponds to a teacher who is rated one standard deviation higher 
in the distribution of true teacher quality for her subject and school-level. Note that 
because ​  
μ​jt​ is shrunk toward the sample mean to account for noise in VA estimates, 
SD(​  
μ​jt​) < ​σ​μ​ and hence the standard deviation of the normalized VA measure ​  
m​jt​ is 
less than one. We demean ​  
m​jt​ within each of the four subject (math versus English) 
by school level (elementary versus middle) cells in the estimation sample in (6) so 
that κ is identified purely from variation within the subject-by-school-level cells.
Importantly, the VA estimates ​  
m​jt​ are predictions of teacher quality in year t based 
on test score data from all years excluding year t. For example, when predicting 
teachers’ effects on the outcomes of students they taught in 1995, we estimate ​  
m​j, 1995​ 
based on residual test scores from students in all years of the sample except 1995. 
To maximize precision, the VA estimates are based on data from all years for which 
16 The test score results are reported in our companion paper (Chetty, Friedman, and Rockoff 2014). We do not 
have adequate precision to implement the quasi-experimental design for earnings or within specific subgroups, 
which is why the cross-sectional estimates remain valuable.
17 Unless otherwise specified, the independent variable in all the regressions and figures in this paper is normal-
ized test-score VA ​  
m​jt​. For simplicity, we refer to this measure as value-added or VA below.


--- Page 16 ---
2648
THE AMERICAN ECONOMIC REVIEW
September 2014
school district data with teacher assignments are available (1991–2009), not just the 
subset of older cohorts for which we observe long-term outcomes.
Using a leave-year-out estimate of VA is necessary to obtain unbiased estimates of 
teachers’ long-term impacts because of correlated errors in students’ test scores and 
later outcomes. Intuitively, if a teacher is randomly assigned unobservably high ability 
students, her estimated VA will be higher. The same unobservably high ability stu-
dents are likely to have high levels of earnings ​η​ it​ ′ ​ , generating a mechanical correlation 
between VA and earnings even if teachers have no causal effect (κ = 0). The leave-
year-out approach eliminates this correlated estimation error bias because ​  
m​jt​ is esti-
mated using a sample which excludes the observations on the left-hand side of (6).18
Control Vectors.—We construct residuals ​Y​it​ using separate models for each of 
the four subject-by-school-level cells. Within each of these groups, we regress raw 
outcomes ​Y​ i​ ∗​ on a vector of covariates ​X​it​ with teacher fixed effects, as in (3), and 
compute residuals ​Y​it​. We partition the control vector ​X​it​ which we used to construct 
our baseline VA estimates into two components: student-level controls ​X​ it​ 
I ​ that vary 
across students within a class; and classroom-level controls ​X​ct​ that vary only at 
the classroom level. The student-level control vector ​X​ it​ 
I ​ includes cubic polynomi-
als in prior-year math and English scores, interacted with the student’s grade level 
to permit flexibility in the persistence of test scores as students age. We also con-
trol for the following student level characteristics: ethnicity, gender, age, lagged 
­suspensions and absences, and indicators for grade repetition, free or reduced-price 
lunch, special education, and limited English. The class-level controls ​X​ct​ consist of 
the following elements: (i) class size and class-type indicators (honors, remedial); 
(ii) cubics in class and school-grade means of prior-year test scores in math and 
English (defined based on those with non-missing prior scores) each interacted with 
grade; (iii) class and school-year means of all the individual covariates ​X​ it​ 
I ​; and 
(iv) grade and year dummies.
In our baseline analysis, we control only for the class-level controls ​X​ct​ when 
estimating the residuals ​Y​it​. Let ​Y​ct​ = ​Y​ c​ 
∗​ − ​β​C​X​ct​ denote the residual of mean out-
comes ​Y​ c​ 
∗​ in class c in year t, where ​β​C​ is estimated at the class level using within-
teacher variation across classrooms as in (3), weighting by class size. We estimate 
the impact of teacher VA on mean outcomes using a class-level OLS regression 
analogous to (6), again weighting by class size:
(8)  	
​Y​ct​  =  α  +  ​κ​C​ ​  
m​jt​  +  ​η​ ct​ ′ ​ .
The point estimate ​  κ​C​ in (8) is identical to ​  κ​ in (6) because teacher VA varies only 
at the classroom level. Formally, ​  κ​C​ = ​  κ​ because deviations of individual-level 
controls (​X​ it​ 
I ​ − ​X​ct​) and outcomes (​Y​ i​ ∗​ − ​Y​ ct​ 
∗ ​) from class means are uncorrelated 
with ​  
m​jt​ .19 Omitting individual-level controls allows us to implement our analysis 
on a dataset collapsed to classroom means, reducing computational costs.
18 This problem does not arise when estimating the impacts of treatments such as class size because the treatment is 
observed; here, the size of the treatment (teacher VA) must itself be estimated, leading to correlated estimation errors.
19 In practice, the identity ​  κ​ = ​  κ​C​ does not hold exactly because the class means ​X​ct​ are defined using all obser-
vations with non-missing data for the relevant variable. Some students are not matched to the tax data and hence 
are missing ​Y​ i​ ∗​, while other students are missing some of the individual-level covariates ​X​ it​ 
I ​ (e.g., prior-year test 


--- Page 17 ---
2649
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Standard Errors.—The dependent variable in (6) has a correlated error structure 
because students within a classroom face common class-level shocks and because 
our analysis dataset contains repeat observations on students in different grades. One 
natural way to account for these two sources of correlated errors would be to clus-
ter standard errors by both student and classroom (Cameron, Gelbach, and Miller 
2011). Unfortunately, two-way clustering of this form requires running regres-
sions on student-level data and thus was computationally infeasible at the Internal 
Revenue Service. We instead cluster standard errors at the school by cohort level 
when estimating (8) at the class level, which adjusts for correlated errors across 
classrooms and repeat student observations within a school. The more conservative 
approach of clustering by school increases standard errors by 30 percent (for earn-
ings), but does not affect our hypothesis tests at conventional levels of statistical 
significance (online Appendix Table 7).20
Outliers.—In our baseline specifications, we exclude classrooms taught by teach-
ers whose estimated VA ​  
m​jt​ falls in the top 1 percent for their subject and school level 
(above 2.03 in math and 1.94 in English in elementary school and 1.93 in math and 
1.19 in English in middle school). We do so because these teachers’ impacts on test 
scores appear suspiciously consistent with testing irregularities indicative of test 
manipulation. Jacob and Levitt (2003) develop a proxy for cheating which measures 
the extent to which a teacher generates very large test score gains which are followed 
by very large test score losses for the same students in the subsequent grade. Jacob 
and Levitt show that this proxy for cheating is highly correlated with unusual answer 
sequences which reveal test manipulation directly. Teachers in the top 1 percent of our 
estimated VA distribution are significantly more likely to show suspicious patterns 
of test score gains followed by steep losses, as defined by Jacob and Levitt’s proxy 
(see online Appendix Figure 3).21 We therefore trim the top 1 percent of outliers in 
all the specifications reported in the main text. We investigate how trimming at other 
cutoffs affects our results in online Appendix Table 8. The qualitative conclusion that 
teacher VA has long-term impacts is not sensitive to trimming, but including teachers 
in the top 1 percent reduces our estimates of teachers’ impacts on long-term outcomes 
by 10–30 percent. In contrast, excluding the bottom 1 percent of the VA distribution 
has little impact on our estimates, consistent with the view that test manipulation is 
responsible for the results in the upper tail. Directly excluding teachers who have sus-
pect classrooms based on Jacob and Levitt’s proxy for cheating yields similar results 
scores). As a result, ​X​ct​ does not exactly equal the mean of ​X​ it​ 
I ​ within classrooms in the final estimation sample. To 
verify that the small discrepancies between ​X​ct​ and ​X​ it​ 
I ​ do not affect our estimates of κ, we show in online Appendix 
Table 7 that the inclusion of individual controls ​X​ it​ 
I ​ has little impact on the point estimates of κ by estimating (6) 
for a selected set of specifications on the individual data.
20 In online Appendix Table 7 of Chetty, Friedman, and Rockoff (2011b), we evaluated the robustness of our 
results to other forms of clustering for selected specifications. We found that school-cohort clustering yields more 
conservative confidence intervals than more computationally intensive techniques such as two-way clustering by 
student and classroom.
21 Online Appendix Figure 3 plots the fraction of classrooms which are in the top 5 percent according to Jacob 
and Levitt’s proxy, defined in the notes to the figure, versus our leave-out-year measure of teacher value-added. On 
average, classrooms in the top 5 percent according to the Jacob and Levitt measure have test score gains of 0.47 
standard deviations in year t followed by mean test score losses of 0.42 standard deviations in the subsequent year. 
Stated differently, teachers’ impacts on future test scores fade out much more rapidly in the very upper tail of the 
VA distribution. Consistent with this pattern, these exceptionally high VA teachers also have very little impact on 
their students’ long-term outcomes.


--- Page 18 ---
2650
THE AMERICAN ECONOMIC REVIEW
September 2014
to trimming on VA itself. Because we trim outliers, our baseline estimates should be 
interpreted as characterizing the relationship between VA and outcomes below the 
ninety-ninth percentile of VA.
B. College Attendance
We begin by analyzing the impact of teachers’ test-score VA on college atten-
dance at age 20, the age at which college attendance rates are maximized in our 
sample. Panel A of Figure 1 plots residual college attendance rates for students in 
school year t versus ​  
m​jt​, the leave-year-out estimate of their teacher’s VA in year t. 
To construct this binned scatter plot, we first residualize college attendance rates 
with respect to the class-level control vector ​X​ct​ separately within each subject by 
school-level cell, using within-teacher variation to estimate the coefficients on the 
controls as described above. We then divide the VA estimates ​  
m​jt​ into 20 equal-
sized groups (vingtiles) and plot the mean of the college attendance residuals in 
each bin against the mean of ​  
m​jt​ in each bin. Finally, we add back the mean college 
attendance rate in the estimation sample to facilitate interpretation of the scale.22 
Note that this binned scatter plot provides a nonparametric representation of the 
conditional expectation function but does not show the underlying variance in the 
individual-level data. The regression coefficient and standard error reported in this 
and all subsequent figures are estimated on the class-level data using (8), with stan-
dard errors clustered by school-cohort.
Panel A of Figure 1 shows that being assigned to a higher VA teacher in a sin-
gle grade raises a student’s probability of attending college significantly. The null 
hypothesis that teacher VA has no effect on college attendance is rejected with a 
t-statistic above 11 ( p < 0.001). On average across subjects and grades, a one stan-
dard deviation increase in a teacher’s true test score VA in a single grade increases 
the probability of college attendance at age 20 by κ = 0.82 percentage points, rela-
tive to a mean college attendance rate of 37.2 percent.23
We evaluate the robustness of this estimate to alternative control vectors in Table 2. 
Column 1 of Table 2 replicates the specification with the baseline control vector ​X​ct​ 
in panel A of Figure 1 as a reference. Column 2 replicates column 1, adding parent 
controls ​P ​ ct​ 
∗ ​ to the control vector. The parent characteristics ​P​ ct​ 
∗ ​ consist of classroom 
means of the following variables: mother’s age at child’s birth; indicators for parent’s 
401(k) contributions and home ownership; and an indicator for the parent’s marital 
status interacted with a quartic in parent’s household income.24 The estimate in col-
umn 2 is quite similar to that in column 1. Column 3 of Table 2 replicates column 1, 
adding class means of twice-lagged test scores ​A​ c, t−2​ 
∗ 
​ to the control vector instead of 
22 In this and all subsequent scatter plots, we also demean ​  
m​jt​ within subject-by-school-level groups to isolate varia-
tion within these cells as in the regressions, and then add back the unconditional mean of ​  
m​jt​ in the estimation sample.
23 These and all other estimates reported below reflect the value of a one standard deviation improvement in 
actual teacher VA ​m​jt​, as shown in Section I. Being assigned to a teacher with higher estimated VA yields smaller 
gains because of noise in ​  
m​jt​ and drift in teacher quality, an issue we revisit in Section VI.
24 We code the parent characteristics as zero for the 5.2 percent of students whom we matched to the tax data but 
were unable to link to a parent, and include an indicator for having no parent matched to the student. We also code 
mother’s age at child’s birth as zero for the small number of observations where we match parents but do not have 
data on parents’ ages, and include an indicator for such cases.


--- Page 19 ---
2651
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
parent characteristics. Again, the coefficient does not change appreciably.25 Both parent 
­characteristics and twice-lagged test scores are strong ­predictors of college attendance 
rates even conditional on the baseline controls ​X​ct​, with F-statistics exceeding 300. 
Hence, the fact that controlling for these variables does not significantly affect the esti-
mates of κ supports the identification assumption of selection on observables in (7).
25 The sample in column 3 has fewer observations than in column 1 because twice-lagged test scores are not 
observed in fourth grade. Replicating the specification in column 1 on exactly the estimation sample used in col-
umn 3 yields an estimate of 0.81 percent (0.09).
Figure 1. Effects of Teacher Value-Added on College Outcomes
Notes: These figures are drawn using the linked analysis sample, pooling all grades and subjects, with one observation 
per student-subject-school year. Panels A and B are binned scatter plots of college attendance rates and college qual-
ity versus normalized teacher VA ​  
m​jt​. These plots correspond to the regressions in columns 1 and 4 of Table 2 and use 
the same sample restrictions and variable definitions. To construct these binned scatter plots, we first residualize the 
y-axis variable with respect to the baseline class-level control vector (defined in the notes to Table 2) separately within 
each subject by school-level cell, using within-teacher variation to estimate the coefficients on the controls as described 
in Section IA. We then divide the VA estimates ​  
m​jt​ into 20 equal-sized groups (vingtiles) and plot the means of the 
y-variable residuals within each bin against the mean value of ​  
m​jt​ within each bin. Finally, we add back the uncondi-
tional mean of the y variable in the estimation sample to facilitate interpretation of the scale. The solid line shows the 
best linear fit estimated on the underlying microdata using OLS. The coefficients show the estimated slope of the best-
fit line, with standard errors clustered at the school-cohort level reported in parentheses. In panel C, we replicate the 
regression in column 1 of Table 2 (depicted in panel A), varying the age of college attendance from 18 to 28, and plot 
the resulting coefficients. The dashed lines show the boundaries of the 95 percent confidence intervals for the effect of 
value-added on college attendance at each age, with standard errors clustered by school-cohort. The coefficients and 
standard errors from the regressions underlying panel C are reported in online Appendix Table 9.
Panel A. College attendance at age 20
36
36.5
37
37.5
38
Percent in college at age 20
−1.5
−1
−0.5
0
0.5
1
Normalized teacher value-added 
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Coef. = 0.82%
Panel B. College quality at age 20
26,400
26,600
26,800
27,000
27,200
College quality at age 20 ($)
Coef. = $299
(21)
Panel C. Impact of teacher value-added on college
attendance by age
−0.25
0
0.25
0.5
0.75
1
18
20
22
24
26
28
Impact of 1 SD of VA 
on college attendance rate
Age
^
(mjt)
Normalized teacher value-added ^
(mjt)
(0.07)
Point estimate
95% Ci


--- Page 20 ---
2652
THE AMERICAN ECONOMIC REVIEW
September 2014
College Quality.—We use the same set of specifications to analyze whether high-
VA teachers also improve the quality of colleges that their students attend, as mea-
sured by the earnings of students who previously attended the same college (see 
Section IIB). Students who do not attend college are included in this analysis and 
assigned the mean earnings of individuals who do not attend college. Panel B of 
Figure 1 plots the earnings-based index of quality for college attended at age 20 
versus teacher VA, using the same baseline controls ​X​ct​ and technique as in panel 
A. Again, there is a highly significant relationship between the quality of colleges 
students attend and the quality of the teachers they had in grades 4–8 (t = 14.4, 
p < 0.001). A one standard deviation improvement in teacher VA raises college 
quality by $299 (or 1.11 percent) on average, as shown in column 4 of Table 2. 
Columns 5 and 6 replicate column 4, adding parent characteristics and lagged test 
score gains to the baseline control vector. As with college attendance, the inclusion 
of these controls has only a modest effect on the point estimates.
Table 2— Impacts of Teacher Value-Added on College Attendance
College
at
age 20
College
at
age 20
College
at
age 20
College 
quality
at age 20
College 
quality
at age 20
College 
quality
at age 20
High
quality
college
Four or more 
years of 
college, ages 
18–22
(%)
(%)
(%)
($)
($)
($)
(%)
(%)
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
Teacher VA
0.82
0.71
0.74
298.63
265.82
266.17
0.72
0.79
(0.07)
(0.06)
(0.09)
(20.74)
(18.31)
(26.03)
(0.05)
(0.08)
Mean of
37.22
37.22
37.09
26,837
26,837
26,798
13.41
24.59
  dep. var.
Baseline
X
X
X
X
X
X
X
X
  controls
Parent chars. 
X
X
  controls
Lagged score
X
X
  controls
Observations
4,170,905
4,170,905
3,130,855
4,167,571
4,167,571
3,128,478
4,167,571
3,030,878
Notes: Each column reports coefficients from an OLS regression, with standard errors clustered by school-cohort 
in parentheses. The regressions are estimated on the linked analysis sample (as described in the notes to Table 1). 
Teacher value-added is estimated using data from classes taught by a teacher in other years, following the proce-
dure described in Section IIIA. The dependent variable in columns 1–3 is an indicator for college attendance at age 
20. The dependent variable in columns 4–6 is the earnings-based index of college quality. See notes to Table 1 and 
Section II for more details on the construction of these variables. The dependent variable in column 7 is an indi-
cator for attending a high-quality college, defined as quality greater than the median college quality among those 
attending college, which is $43,914. The dependent variable in column 8 is an indicator for attending four or more 
years of college between the ages of 18 and 22. All columns control for the baseline class-level control vector, which 
includes: class size and class-type indicators; cubics in class and school-grade means of lagged own- and cross-
subject scores, interacted with grade level; class and school-year means of student-level characteristics including 
ethnicity, gender, age, lagged suspensions and absences, and indicators for grade repetition, special education, free 
or reduced-price lunch, and limited English; and grade and year dummies. Columns 2 and 5 additionally control 
for class means of parent characteristics, including mother’s age at child’s birth, indicators for parent’s 401(k) con-
tributions and home ownership, and an indicator for the parent’s marital status interacted with a quartic in parent’s 
household income. Columns 3 and 6 include the baseline controls and class means of twice-lagged test scores. We 
use within-teacher variation to identify the coefficients on all controls as described in Section IA; the estimates 
reported are from regressions of outcome residuals on teacher VA with school by subject level fixed effects.


--- Page 21 ---
2653
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
The $299 estimate in column  4 combines intensive and extensive margin 
responses because it includes the effect of increased college attendance rates on 
projected earnings. Isolating intensive margin responses is more complicated 
because students who are induced to go to college by a high-VA teacher will tend 
to attend ­lower-quality colleges, pulling down mean earnings conditional on atten-
dance. We take two approaches to overcome this selection problem and identify 
­intensive-margin effects. First, we define colleges with earnings-based quality above 
the student-weighted median in our sample ($43,914) as high quality. We regress 
this high-quality college indicator on teacher VA in the full sample, including stu-
dents who do not attend college, and find that a one standard deviation increase in 
teacher VA raises the probability of attending a high-quality college by 0.72 per-
centage points, relative to a mean of 13.41 percent (column 7 of Table 2). This 
increase is most consistent with an intensive margin effect, as students would be 
unlikely to jump from not going to college at all to attending a high-quality col-
lege. Second, we derive a lower bound on the intensive margin effect by assuming 
that those who are induced to attend college attend a college of average quality. 
The mean college quality conditional on attending college is $41,756, while the 
quality for all those who do not attend college is $17,920. This suggests that at 
most (41, 756 − 17, 920) × 0.82 percent = $195 of the $299 impact is due to the 
extensive margin response, confirming that teachers improve the quality of colleges 
which students attend.
Finally, we analyze the impact of teacher quality on the number of years in college. 
Column 8 replicates the baseline specification in column 1, replacing the depen-
dent variable with an indicator variable for attending college in at least four years 
between 18 and 22. A one standard deviation increase in teacher quality increases 
the fraction of students who spend four or more years in college by 0.79 percent-
age points (3.2 percent of the mean).26 While we cannot directly measure college 
completion in our data, this finding suggests that higher-quality teachers increase 
not just attendance but also college completion rates.
Panel C of Figure 1 plots the impact of a one standard deviation improvement in 
teacher quality on college attendance rates at all ages from 18 to 28. We run separate 
regressions of college attendance at each age on teacher VA, using the same speci-
fication as in column 1 of Table 2. As one would expect, teacher VA has the largest 
impacts on college attendance rates before age 22. However, the impacts remain 
significant even in the mid-20s, perhaps because of increased attendance of gradu-
ate or professional schools. These continued impacts on higher education affect our 
analysis of earnings impacts, to which we now turn.
C. Earnings
The correlation between annual earnings and lifetime income rises rapidly as 
individuals enter the labor market and begins to stabilize only in their late 20s. We 
26 The magnitude of the four-year attendance impact (0.79 pp) is very similar to the magnitude of the single-year 
attendance impact (0.82 pp). Since the students who are on the margin of attending for one year presumably do 
not all attend for four years, this suggests that better teachers increase the number of years that students spend in 
college on the intensive margin.


--- Page 22 ---
2654
THE AMERICAN ECONOMIC REVIEW
September 2014
therefore begin by analyzing the impacts of teacher VA on earnings at age 28, the 
oldest age at which we have a sufficiently large sample of students to obtain precise 
estimates. Although individuals’ earnings trajectories remain quite steep at age 28, 
earnings levels at age 28 are highly correlated with earnings at later ages (Haider and 
Solon 2006), a finding we confirm within the tax data in online Appendix Figure 4.
Panel A of Figure 2 plots individual (W-2) wage earnings at age 28 against VA ​  
m​jt​, 
conditioning on the same set of classroom-level controls as above. Being assigned 
to a higher value-added teacher has a significant impact on earnings, with the null 
hypothesis of κ = 0 rejected with p < 0.01. A one standard deviation increase in 
teacher VA in a single grade increases earnings at age 28 by $350, 1.65 percent of 
mean earnings in the regression sample.
Columns 1–3 of Table 3 evaluate the robustness of this estimate to the inclusion 
of parent characteristics and lagged test score gains. These specifications mirror col-
umns 1–3 of Table 2, but use earnings at age 28 as the dependent variable. As with 
college attendance, controlling for these additional observable characteristics has 
relatively small effects on the point estimates, supporting the identification assump-
tion in (7). The smallest of the three estimates implies that a one standard deviation 
increase in teacher VA raises earnings by 1.34 percent.
To interpret the magnitude of this 1.34 percent impact, consider the lifetime earn-
ings gain from having a one standard deviation higher VA teacher in a single grade. 
Assume that the percentage gain in earnings remains constant at 1.34 percent over 
the life cycle and that earnings are discounted at a 3 percent real rate (i.e., a 5 per-
cent discount rate with 2 percent wage growth) back to age 12, the mean age in our 
Panel A. Earnings at age 28
20,500
21,000
21,500
22,000
Earnings at age 28 ($)
Coef. = $350
(92)
Panel B. Impact of teacher value-added
on earnings by age
−1
0
1
2
3
Impact of 1 SD of VA
on earnings (%)
20
22
24
26
28
Age of earnings measurement
Normalized teacher value-added 
Point estimate
95% Ci
−1.5
−0.5
0
0.5
1
1.5
−1
^
(mjt)
Figure 2. Effect of Teacher Value-Added on Earnings
Notes: Panel A is a binned scatter plot of earnings at age 28 versus normalized teacher VA ​  
m​jt​. This plot corresponds 
to the regression in column 1 of Table 3 and uses the same sample restrictions and variable definitions. See notes to 
Figure 1 for details on the construction of binned scatter plots. In panel B, we replicate the regression in column 1 
of Table 3 (depicted in panel A), varying the age at which earnings are measured from 20 to 28. We then plot the 
resulting coefficients expressed as a percentage of mean wage earnings in the regression sample at each age. The 
dashed lines show the boundaries of the 95 percent confidence intervals for the effect of value-added on earnings at 
each age, with standard errors clustered by school-cohort. The coefficients and standard errors from the regressions 
underlying panel B are reported in online Appendix Table 9.


--- Page 23 ---
2655
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
sample. Under these assumptions, the mean present value of lifetime earnings at age 
12 in the US population is approximately $522,000.27 Hence, the financial value 
of having a one standard deviation higher VA teacher (i.e., a teacher at the eighty-
fourth percentile instead of the median) is 1.34 percent × $522, 000 ≃ $7, 000 per 
grade. The undiscounted lifetime earnings gain (assuming a 2 percent growth rate 
but 0 percent discount rate) is approximately $39,000 per student.
A second benchmark is the increase in earnings from an additional year of school-
ing, which is around 9  percent (Gunderson and Oreopoulos 2010, Oreopoulos 
and Petronijevic 2013). Having a teacher in the first percentile of the value-added 
distribution (2.33 standard deviations below the mean) is equivalent to miss-
ing ​ 2.33 × 1.34 percent 
 
__ 
 
9 percent 
​ = one-third of the school year when taught by a teacher of 
average quality.
A third benchmark is the cross-sectional relationship between test scores and 
earnings. A one standard deviation increase in teacher quality raises end-of-year 
scores by 0.13 standard deviations of the student test score distribution on aver-
age across grades and subjects. A one standard deviation increase in student test 
scores, controlling for the student- and class-level characteristics ​X​it​, is associated 
with a 12 percent increase in earnings at age 28 (online Appendix Table 3, column 3, 
row 2). The predicted impact of a one standard deviation increase in teacher VA 
27 We calculate this number using the mean wage earnings of a random sample of the US population in 2007 to obtain 
an earnings profile over the life cycle, and then inflate these values to 2010 US$. See Chetty et al. (2011) for details.
Table 3—Impacts of Teacher Value-Added on Earnings
Earnings
at age 28
Earnings
at age 28
Earnings
at age 28
Working
at age 28
Total income
at age 28
Wage growth 
ages 22–28
($)
($)
($)
(%)
($)
($)
(1)
(2)
(3)
(4)
(5)
(6)
Teacher VA
349.84
285.55
308.98
0.38
353.83
286.20
 
(91.92)
(87.64)
(110.17)
(0.16)
(88.62)
(81.86)
 
 
 
 
 
 
 
Mean of dep. var.
21,256
21,256
21,468
68.09
22,108
11,454
 
 
 
 
 
 
Baseline controls
X
X
X
X
X
X
 
 
 
 
 
 
Parent chars. 
 
X
 
 
 
 
  controls
 
 
 
 
 
 
Lagged score
 
 
X
 
 
 
  controls
 
 
 
 
 
 
Observations
650,965
650,965
510,309
650,965
650,965
650,943
Notes: Each column reports coefficients from an OLS regression, with standard errors clustered by school-cohort in 
parentheses. The regressions are estimated on the linked analysis sample (as described in the notes to Table 1). There is 
one observation for each student-subject-school year. Teacher value-added is estimated using data from classes taught 
by a teacher in other years, following the procedure described in Section IIIA. The dependent variable in columns 1–3 
is the individual’s wage earnings reported on W-2 forms at age 28. The dependent variable in column 4 is an indicator 
for having positive wage earnings at age 28. The dependent variable in column 5 is total income (wage earnings plus 
self-employment income). The dependent variable in column 6 is wage growth between ages 22 and 28. All columns 
control for the baseline class-level control vector; column 2 additionally controls for parent characteristics, while col-
umn 3 additionally controls for twice-lagged test scores (see notes to Table 2 for details). We use within-teacher varia-
tion to identify the coefficients on all controls as described in Section IA; the estimates reported are from regressions 
of outcome residuals on teacher VA with school by subject level fixed effects.


--- Page 24 ---
2656
THE AMERICAN ECONOMIC REVIEW
September 2014
on earnings is therefore 0.13 × 12 percent = 1.55 percent, similar to the observed 
impact of 1.34 percent.
Extensive Margin Responses and Other Sources of Income.—The increase in wage 
earnings comes from a combination of extensive and intensive margin responses. In 
column 4 of Table 3, we regress an indicator for having positive W-2 wage earnings 
on teacher VA using the same specification as in column 1. A one standard devia-
tion increase in teacher VA raises the probability of working by 0.38 percent. If the 
marginal entrant into the labor market were to take a job that paid the mean earnings 
level in the sample ($21,256), this extensive margin response would raise mean 
earnings by $81. Since the marginal entrant most likely has lower earnings than the 
mean, this implies that the extensive margin accounts for at most 81/350 = 23 per-
cent of the total earnings increase due to better teachers.
Column 5 of Table 3 replicates the baseline specification in column 1 using total 
income (as defined in Section II) instead of wage earnings. Reassuringly, the point 
estimate of teachers’ impacts changes relatively little with this broader income defini-
tion, which includes self-employment and other sources of income. We therefore use 
wage earnings—which provides an individual rather than household measure of earn-
ings and is unaffected by the endogeneity of filing—for the remainder of our analysis.
Earnings Trajectories.—Next, we analyze how teacher VA affects the trajectory of 
earnings by examining wage earnings impacts at each age from 20 to 28. We run sepa-
rate regressions of wage earnings at each age on teacher VA using the same specifica-
tion as in column 1 of Table 3. Panel B of Figure 2 plots the coefficients from these 
regressions (which are reported in online Appendix Table 9), divided by average earn-
ings at each age to obtain percentage impacts. The impact of teacher quality on earnings 
rises almost monotonically with age. At early ages, the impact of higher VA is negative 
and statistically significant, consistent with our finding that higher VA teachers induce 
their students to go to college. As these students enter the labor force, they have steeper 
earnings trajectories than students who had lower VA teachers in grades 4–8. Earnings 
impacts become positive at age 23, become statistically significant at age 24, and grow 
through age 28, where the earnings impact reaches 1.65 percent, as in Figure 2A.
An alternative way to state the result in panel B of Figure 2 is that better teachers 
increase the growth rate of students’ earnings in their 20s. In column 6 of Table 3, 
we verify this result directly by regressing the change in earnings from age 22 to 
age 28 on teacher VA. As expected, a one standard deviation increase in teacher VA 
increases earnings growth by $286 (2.5 percent) over this period. This finding sug-
gests that teachers’ impacts on lifetime earnings could be larger than the 1.34 per-
cent impact observed at age 28.
D. Other Outcomes
In this subsection, we analyze the impacts of teacher VA on other outcomes, start-
ing with our “teenage birth” measure, which is an indicator for filing a tax return 
and claiming a dependent who was born while the mother was a teenager (see 
Section IIB). Column 1 of Table 4 analyzes the impact of teacher VA on the fraction 
of female students who have a teenage birth. Having a one standard deviation higher 


--- Page 25 ---
2657
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
VA teacher in a single year from grades 4–8 reduces the probability of a teen birth by 
0.61 ­percentage points, a reduction of roughly 4.6 percent, as shown in panel A of 
Figure 3. This impact is similar to the raw cross-sectional correlation between scores 
and teenage births (online Appendix Table 3), echoing our results on earnings and 
college attendance.
Column 2 of Table 4 analyzes the impact of teacher VA on the socioeconomic 
status of the neighborhood in which students live at age 28, measured by the percent 
of college graduates living in that neighborhood. A one standard deviation increase 
in teacher VA raises neighborhood SES by 0.25 percentage points (1.8 percent of 
the mean) by this metric, as shown in panel B of Figure 3. Column 3 of Table 4 
shows that a one standard deviation increase in teacher VA increases the likelihood 
of saving money in a 401(k) at age 28 by 0.55 percentage points (or 2.8 percent of 
the mean), as shown in panel C of Figure 3.
Fade-Out of Test Score Impacts.—The final set of outcomes we consider are 
teachers’ impacts on test scores in subsequent grades. Figure 4 plots the impacts 
of teacher VA on test scores in subsequent years; see online Appendix Table 10 for 
the underlying coefficients. To construct this figure, we residualize raw test scores 
​A​ i, t+s​ 
∗  ​ with respect to the class-level controls ​X​ct​ using within-teacher variation and 
then regress the residuals ​A​i, t+s​ on ​  
μ​jt​. We scale teacher VA in units of student test-
score standard deviations in these regressions—by using ​  
μ​jt​ as the independent 
variable instead of ​  
m​jt​ = ​  
μ​jt​/​σ​0​—to facilitate interpretation of the regression coef-
ficients, which are plotted in Figure 4. The coefficient at s = 0 is not statistically 
distinguishable from 1, as shown in our companion paper. Teachers’ impacts on 
Table 4—Impacts of Teacher Value-Added on Other Outcomes (percent)
Teenage
birth
Percent college grad
in zip at age 28
Have 401(k)
at age 28
(1)
(2)
(3)
Teacher VA
−0.61
0.25
0.55
(0.06)
(0.04)
(0.16)
Mean of dep. var.
13.24
13.81
19.81
Baseline controls
X
X
X
Observations
2,110,402
468,021
650,965
Notes: Each column reports coefficients from an OLS regression, with standard errors clus-
tered by school-cohort in parentheses. The regressions are estimated on the linked analysis 
sample (as described in the notes to Table 1). There is one observation for each student-­subject-
school year. Teacher value-added is estimated using data from classes taught by a teacher in 
other years, following the procedure described in Section IIIA. The dependent variables in col-
umn 1–3 are an indicator for having a teenage birth, the fraction of residents in an individual’s 
zip code of residence at age 28 with a college degree or higher, and an indicator for whether an 
individual made a contribution to a 401(k) plan at age 28 (see notes to Table 1 and Section II 
for more details). Column 1 includes only female students. All regressions include the baseline 
class-level control vector (see notes to Table 2 for details). We use within-teacher variation to 
identify the coefficients on all controls as described in Section IA; the estimates reported are 
from regressions of outcome residuals on teacher VA with school by subject level fixed effects.


--- Page 26 ---
2658
THE AMERICAN ECONOMIC REVIEW
September 2014
impacts on test scores fade out rapidly in subsequent years and appear to stabilize 
at approximately 25 percent of the initial impact after three to four years.28 This 
result aligns with existing ­evidence that ­improvements in education raise con-
temporaneous scores, then fade out in later scores, only to reemerge in adulthood 
(Deming 2009; Heckman et al. 2010b; Chetty et al. 2011).
IV.  Research Design 2: Teacher Switching Quasi-Experiments
The estimates in the previous section rely on the assumption that the unobserved 
determinants of students’ long-term outcomes are uncorrelated with teacher quality 
conditional on observables. In this section, we estimate teachers’ long-term impacts 
using a quasi-experimental design which relaxes and helps validate this identifica-
tion assumption.
28 Prior studies (e.g., Kane and Staiger 2008, Jacob, Lefgren, and Sims 2010, Rothstein 2010, Cascio and Staiger 
2012) document similar fade-out after one or two years but have not determined whether test score impacts continue 
to deteriorate after that point. The broader span of our dataset allows us to estimate test score persistence more 
precisely. For instance, Jacob, Lefgren, and Sims (2010) estimate one-year persistence using 32,422 students and 
two-year persistence using 17,320 students. We estimate one-year persistence using more than 5.6 million student-
year-subject observations and four-year persistence using more than 1.3 million student-year-subject observations.
Panel A. Women with teenage births
12.5
13
13.5
14
14.5
Percent of women
with teenage births
Coef. = −0.61%
Panel B. Neighborhood quality at age 28
13.4
13.6
13.8
14
14.2
Percent college graduates
in zip at age 28
Coef. = 0.25%
Panel C. Retirement savings at age 28
19
19.5
20
20.5
Percent saving for
retirement at age 28
Coef. = 0.55%
Normalized teacher value-added 
Normalized teacher value-added 
Normalized teacher value-added 
−1.5
−0.5
0
0.5
1
1.5
−1
−1.5
−0.5
0
0.5
1
1.5
−1
−1.5
−0.5
0
0.5
1
1.5
−1
^
(mjt)
^
(mjt)
^
(mjt)
(0.06)
(0.04)
(0.16)
Figure 3. Effects of Teacher Value-Added on Other Outcomes in Adulthood
Notes: These three figures are binned scatter plots corresponding to columns 1–3 of Table 4 and use the same sample 
restrictions and variable definitions. See notes to Figure 1 for details on the construction of these binned scatter plots.


--- Page 27 ---
2659
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
A. Methodology
Adjacent cohorts of students within a school are exposed to different teachers fre-
quently. We exploit this teacher turnover to obtain a quasi-experimental estimate of 
teachers’ long-term impacts. To understand our research design, consider a school 
with three fourth-grade classrooms. Suppose one of the teachers leaves the school in 
1995 and is replaced by a teacher whose VA estimate is 0.3 higher, so that the mean 
test-score VA of the teaching staff rises by 0.3/3 = 0.1. If the distribution of unob-
served determinants of students’ long-term outcomes does not change between 1994 
and 1995, the change in mean college attendance rates between the 1994 and 1995 
cohorts of students will reveal the impact of a 0.1 improvement in fourth grade teach-
ers’ test-score VA. More generally, we can estimate teachers’ long-term impacts by 
comparing the change in mean student outcomes across cohorts to the change in mean 
VA driven by teacher turnover provided that student quality is stable over time.
To formalize this approach, let ​  
m​ jt​ 
−​{ t, t−1 }​​ denote the test-score VA estimate for 
teacher j in school year t constructed as in our companion paper using data from all 
years except t − 1 and t. Similarly, let ​  
m​ j, t−1​ 
−​{ t, t−1 }​​ denote the VA estimate for teacher 
j in school year t − 1 based on data from all years except t − 1 and t. Let ​Q​sgt​ 
denote the student-weighted mean of ​  
m​ jt​ 
−​{ t, t−1 }​​ across teachers in school s in grade 
g, which is the average estimated quality of teachers in a given school-grade-year 
0
0.2
0.4
0.6
0.8
1
Impact of current teacher VA on test scores
0
1
2
3
4
Years after current school year
Point estimate
95% Ci
Figure 4. Effects of Teacher Value-Added on Future Test Scores
Notes: This figure shows the effect of current teacher VA on test scores at the end of the cur-
rent and subsequent school years. To construct this figure, we regress end-of-grade test scores in 
year t + s on teacher VA ​  μ​jt​ in year t, varying s from 0 to 4. As in our companion paper, we scale 
teacher VA in units of student test-score standard deviations and include all students in the school 
district data in these regressions, without restricting to the older cohorts that we use to study out-
comes in adulthood. We control for the baseline class-level control vector (defined in the notes 
to Table 2), using within-teacher variation to identify the coefficients on controls as described in 
Section IA. The dashed lines depict 95 percent confidence intervals on each regression coeffi-
cient, with standard errors clustered by school-cohort. The coefficients and standard errors from 
the underlying regressions are reported in online Appendix Table 10.


--- Page 28 ---
2660
THE AMERICAN ECONOMIC REVIEW
September 2014
cell; define ​Q​sg, t−1​ analogously.29 Let Δ​Q​sgt​ =  ​Q​sgt​ − ​Q​sg, t−1​ denote the change in 
mean teacher value-added from year t − 1 to year t in grade g in school s. Define 
mean changes in student outcome residuals Δ​Y​sgt​ analogously. Note that because 
we exclude both years t and t − 1 when estimating VA, the variation in Δ​Q​sgt​ is 
driven purely by changes in the teaching staff and not by changes in teachers’ VA 
estimates.30 As above, this leave-out technique ensures that changes in Δ​Y​sgt​ are not 
spuriously correlated with Δ​Q​sgt​ due to estimation error in VA.
We estimate teachers’ long-term impacts by regressing changes in mean outcomes 
across cohorts on changes in mean test-score VA:
(9)  	
Δ​Y​sgt​  =  α  +  κΔ​Q​sgt​  +  Δ​η​ sgt​ 
′  ​ .
Note that this specification is the same as the quasi-experimental specification we 
used to estimate the degree of bias in VA estimates in our companion paper, except 
that we use long-term outcomes as the dependent variable instead of test scores. The 
coefficient in (9) identifies the effect of a one standard deviation improvement in 
teacher quality as defined in (6) under the following assumption.
Assumption 3 (Teacher Switching as a Quasi-Experiment): Changes in teacher 
quality across cohorts within a school-grade are orthogonal to changes in other 
determinants of student outcomes Δ​η​ sgt​ 
′  ​ across cohorts:
(10)  	
Cov​( Δ​Q​sgt​, Δ​η​ sgt​ 
′  ​ )​  =  0.
This assumption could potentially be violated by endogenous student or teacher 
sorting. In practice, student sorting at an annual frequency is minimal because of the 
costs of changing schools. During the period we study, most students would have 
to move to a different neighborhood to switch schools, which families would be 
unlikely to do simply because a single teacher leaves or enters a given grade. While 
endogenous teacher sorting is plausible over long horizons, the sharp changes we 
analyze are likely driven by idiosyncratic shocks such as changes in staffing needs, 
maternity leaves, or the relocation of spouses. Moreover, in our first paper, we pres-
ent direct evidence supporting (10) by showing that both prior scores and contem-
poraneous scores in the other subject (e.g., English) are uncorrelated with changes 
in mean teacher quality in a given subject (e.g., math). We also present additional 
evidence supporting (10) below.
If observable characteristics ​X​it​ are also orthogonal to changes in teacher quality 
across cohorts (i.e., satisfy Assumption 3), we can implement (9) simply by regress-
ing the change in raw outcomes Δ​Y​ sgt​ 
∗ ​ on Δ​Q​sgt​. We therefore begin with regressions 
29 In our baseline specifications, we impute teacher VA as the sample mean (0) for students for whom we have 
no leave-out-year VA estimate ​  
m​ jt​ 
−​{ t, t−1 }​​, either because we have no teacher information or because the teacher did 
not teach in the district outside of years ​{ t − 1, t }​. We show below that we obtain similar results when restricting to 
the subset of school-grade-subject-year cells with no missing data on teacher VA. See Section V of our companion 
paper for additional discussion on the effects of this imputation.
30 Part of the variation in Δ​Q​sgt​ comes from drift. Even for a given teacher, predicted VA will change because 
our forecast of VA varies across years. Because the degree of drift is small across a single year, drift accounts for 
5.5 percent of the variance in Δ​Q​sgt​. As a result, isolating the variation due purely to teacher switching using an 
instrumental variables specification yields very similar results (not reported).


--- Page 29 ---
2661
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
of Δ​Y​ sgt​ 
∗ ​ on Δ​Q​sgt​ and then confirm that changes in observable characteristics ​X​it​ 
across cohorts are uncorrelated with Δ​Q​sgt​.
B. Results
Panel A of Figure 5 presents a binned scatter plot of changes in mean college 
attendance rates Δ​Y​ sgt​ 
∗ ​ against changes in mean teacher value-added Δ​Q​sgt​ across 
cohorts. We include year fixed effects (demeaning both the x and y variables by 
school year), so that the estimate is identified purely from differential changes in 
teacher value-added across school-grade-subject cells over time. The corresponding 
regression coefficient, which is based on estimating (9) with year fixed effects but 
no other controls, is reported in column 1 of panel A of Table 5.
Changes in the quality of the teaching staff have significant impacts on 
changes in college attendance rates across consecutive cohorts of students in a 
­school-grade-subject cell. The null hypothesis that κ = 0 is rejected with p < 0.01. 
The point estimate implies that a one standard deviation improvement in teacher 
quality raises college attendance rates by 0.86 percentage points, with a standard 
error of 0.23. This estimate is very similar to the estimates of 0.71–0.82  per-
centage points obtained from the first research design in Table  2. However, the 
quasi-experimental estimate is much less precise because it exploits less variation.
This analysis identifies teachers’ causal impacts provided that (10) holds. One 
natural concern is that improvements in teacher quality may be correlated with other 
improvements in a school—such as better resources in other dimensions—which 
also contribute to students’ long-term success and thus lead us to overstate teachers’ 
true impacts. To address this concern, column 2 of panel A of Table 5 replicates the 
baseline specification in column 1 including school by year fixed effects instead 
of just year effects. In this specification, the only source of identifying variation 
comes from differential changes in teacher quality across subjects and grades within 
a school in a given year. The coefficient on Δ​Q​sgt​ changes very little relative to the 
baseline estimate that pools all sources of variation. Column 3 further accounts for 
secular trends in subject- or grade-specific quality by controlling for the change in 
mean teacher VA in the prior and subsequent year as well as cubics in the change 
in prior-year mean own-subject and other-subject scores across cohorts. Controlling 
for these variables has little impact on the estimate. This result shows that fluc-
tuations in teacher quality relative to trend in specific grades generate significant 
changes in the affected students’ college attendance rates.
In the preceding specifications, we imputed the sample mean of VA (0) for 
classrooms for which we could not calculate actual VA. This generates downward 
bias in our estimates because we mismeasure the change in teacher quality Δ​Q​sgt​ 
across cohorts. Column 4 of panel A replicates column 2, limiting the sample to 
­school-grade-subject-year cells in which we can calculate the ­leave-two-year-out 
mean for all teachers in the current and preceding year. As expected, the point 
estimate of a one standard deviation increase in teacher quality increases, but the 
confidence interval is significantly wider because the sample size is considerably 
smaller.
Finally, we further evaluate (10) using a series of placebo tests. In column 5 of 
panel A of Table 5, we replicate column 2, replacing the change in actual ­college 


--- Page 30 ---
2662
THE AMERICAN ECONOMIC REVIEW
September 2014
Panel A. Change in college attendance across cohorts versus change in mean teacher VA
−0.2
0
0.2
0.4
0.6
0.8
−0.5
0
0.5
Change in college attendance rate across cohorts (%)
Change in mean normalized teacher VA across cohorts
Coef. = 0.86%
Panel B. Change in college quality across cohorts versus change in mean teacher VA
−150
−100
−50
0
50
100
Change in college quality across cohorts ($)
Change in mean normalized teacher VA across cohorts
Coef. = $198
 
0.5
0
−0.5
(0.23)
(60)
Figure 5. Effects of Changes in Teaching Staff across Cohorts on College Outcomes
Notes: These two figures plot changes in mean college attendance rates (measured in percentage points) and col-
lege quality across adjacent cohorts within a school-grade-subject cell against changes in mean teacher VA across 
those cohorts. These plots correspond to the regressions in column 1 of panels A and B of Table 5 and use the same 
sample restrictions and variable definitions. To construct these binned scatter plots, we first demean both the x- 
and y-axis variables by school year to eliminate any secular time trends. We then divide the observations into 20 
equal-size groups (vingtiles) based on their change in mean VA and plot the means of the y variable within each bin 
against the mean change in VA within each bin, weighting by the number of students in each school-grade-subject-
year cell. Finally, we add back the unconditional (weighted) mean of the x and y variables in the estimation sample.


--- Page 31 ---
2663
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
attendance with the change in predicted college attendance based on parent 
­characteristics. We predict college attendance using an OLS regression of ​Y​ it​ 
∗ ​ on the 
same five parent characteristics ​P​ it​ 
∗ ​ used in Section IIIB, with no other control vari-
ables. Changes in mean teacher VA have no effect on predicted college attendance 
rates, ­supporting the assumption that changes in the quality of the teaching staff are 
unrelated to changes in student quality at an annual level.
In panel A of Figure 6, we present an alternative set of placebo tests based on the 
sharp timing of the change in teacher quality. To construct this figure, we replicate 
Table 5—Impacts of Teacher Value-Added on College Outcomes: Quasi-Experimental Estimates
College
attendance (%)
Predicted college
attendance (%)
(1)
(2)
(3)
(4)
(5)
Panel A. College attendance at age 20
Teacher VA
0.86
0.73
0.67
1.20
0.02
(0.23)
(0.25)
(0.26)
(0.58)
(0.06)
Year FE
X
School × year FE
X
X
X
X
Lagged score controls
X
Lead and lag changes
X
  in teacher VA
Number of school× grade
33,167
33,167
26,857
8,711
33,167
  × subject × year cells
Sample:
Full sample
Full sample
Full sample
No imputed 
scores
Full sample
College
quality ($)
Predicted college
quality ($)
Panel B. College quality at age 20
Teacher VA
197.64
156.64
176.51
334.52
2.53
(60.27)
(63.93)
(64.94)
(166.85)
(18.30)
Year FE
X
School × year FE
X
X
X
X
Lagged score controls
X
Lead and lag changes
X
  in teacher VA
Number of school × grade
33,167
33,167
26,857
8,711
33,167
  × subject × year cells
Sample:
Full sample
Full sample
Full sample
No imputed 
scores
Full sample
Notes: Each column reports coefficients from an OLS regression, with standard errors clustered by school-cohort in 
parentheses. The regressions are estimated on the linked analysis sample (as described in the notes to Table 1), col-
lapsed to school-grade-year-subject means. The independent variable for each regression is the difference in mean 
teacher value-added between adjacent school-grade-year-subject cells, where we estimate teacher value-added 
using data which omits both years (see Section IVA for more details). Similarly, dependent variables are defined as 
changes in means across consecutive cohorts at the school-grade-year-subject level. In panel A, the dependent vari-
able is college attendance at age 20; in panel B, the dependent variable is the earnings-based index of college qual-
ity (see Table 1 for details). In column 1 we regress the mean change in the dependent variable on the mean change 
in teacher value-added, controlling only for year fixed-effects. Column 2 replicates column 1 including school-year 
fixed effects. In column 3, we add a cubic in the change in mean lagged scores to the specification in column 2, as 
well as controls for the lead and lag change in mean teacher value-added. In column 4, we restrict the sample to cells 
with no imputed VA; other columns impute the sample mean of 0 for classes with missing VA. Column 5 replicates 
column 2, except that the dependent variable is the predicted value from an individual-level regression of the origi-
nal dependent variable on the vector of parent characteristics defined in the notes to Table 2.


--- Page 32 ---
2664
THE AMERICAN ECONOMIC REVIEW
September 2014
Panel A. Effects of changes in mean teacher VA on college attendance
−0.5
0
0.5
1
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
Coef. at 0 = 1.0
Coef. at +1 equals Coef.at 0: p = 0.009
Coef. at −1 equals Coef.at 0: p = 0.050
Impact of 1 SD change in leads
or lags of mean VA (%)
Lead or lag of change in mean VA
Panel B. Effects of changes in mean teacher VA on college quality
−100
0
100
200
300
Coef. at 0 = 285
Coef. at +1 equals Coef. at 0: p = 0.006
Coef. at −1 equals Coef. at 0: p = 0.034
Impact of 1 SD change in leads
or lags of mean VA ($)
Lead or lag of change in mean VA
(0.3)
(85)
Figure 6. Timing of Changes in Teacher Quality and College Outcomes
Notes: These figures evaluate whether the timing of changes in teacher quality across cohorts 
aligns with the timing of changes in college outcomes. The point at 0 represents the treatment 
effect of changes in teacher quality on changes in college outcomes for a given group of stu-
dents; the other points are placebo tests that show the impacts of changes in teacher quality for 
previous and subsequent cohorts on the same set of students. To construct panel A, we regress 
the change in mean college attendance between adjacent cohorts within a school-grade-subject 
cell on the change in mean teacher quality across those cohorts as well as four lags and four 
leads of the change in mean teacher quality within the same school-grade-subject. The regres-
sion also includes year fixed effects. Panel A plots the coefficients from this regression. We 
report the point estimate and standard error on the own-year change in mean teacher quality 
(corresponding to the value at 0). We also report p-values from hypothesis tests for the equal-
ity of the own-year coefficient and the one-year lead or one-year lag coefficients. These stan-
dard errors and p-values account for clustering at the school-cohort level. Panel B replicates 
panel A, replacing the dependent variable with changes in mean college quality across adja-
cent cohorts.


--- Page 33 ---
2665
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
the specification in column 1 but include changes in mean teacher VA for the four 
preceding and subsequent cohorts as placebo effects (as well as year fixed effects):
(11)  	
Δ​Y​ sgt​ 
∗ ​  =  ​α​t​  +  ​∑​ 
n= −4
​ 
4
  ​ ​κ​n​Δ​Q​sg, t+n​  +  ​φ​t​ .
Panel A of Figure  6 plots the vector of coefficients κ = (​κ​−4​, … , ​κ​0​, … , ​κ​4​), 
which represent the impacts of changes in the quality of teaching staff at different 
horizons on changes in college attendance rates at time 0. As one would expect, ​
κ​0​ is positive and highly significant while all the other coefficients are near 0 and 
statistically insignificant. That is, contemporaneous changes in teacher quality have 
significant effects on college attendance rates, but past or future changes have no 
impact, as they do not affect the current cohort of students directly. The placebo 
tests in panel A support strongly the view that the changes in teacher VA have causal 
effects on college attendance rates.31
Panel B of Figure 5, panel B of Figure 6, and panel B of Table 5 replicate the 
preceding analysis using the earnings-based index of college quality as the outcome. 
We find that improvements in teachers’ test-score VA across cohorts lead to sharp 
changes in the quality of colleges that students attend across all of the specifica-
tions. The point estimate in the baseline specification (column 1) implies that a one 
standard deviation improvement in teacher VA raises college quality by $197 (s.e. 
= $60), which is not statistically distinguishable from the estimates of $266–$299 
obtained using the first research design. We find no evidence that predicted college 
quality based on parent characteristics is correlated with changes in teacher qual-
ity. In addition, changes in teacher VA again affect college quality in the year of 
the change rather than in preceding or subsequent years, as shown in panel B of 
Figure 6.
We used specifications analogous to those in Table 5 to investigate the impacts 
of teaching quality on other outcomes, including earnings at age 28. Unfortunately, 
our sample size for earnings at age 28 is roughly one-seventh the size of the sample 
available to study college attendance at age 20. This is both because we have fewer 
cohorts of students who are currently old enough to be observed at age 28 in the tax 
data and because we have data on teacher assignments for much fewer schools in 
the very early years of our school district data. Because of the considerably smaller 
sample, we obtain very imprecise and fragile estimates of the impacts of teacher 
quality on earnings using the quasi-experimental design.32 Hence, we are forced to 
rely on estimates from the cross-class design in Table 3 to gauge earnings impacts. 
31 In Figure 3 of our companion paper, we directly use event studies around the entry and exit of teachers in 
the top and bottom 5 percent to demonstrate the impacts of VA on test scores. We do not have adequate power to 
identify the impacts of these exceptional teachers on college attendance using such event studies. In the cross-cohort 
regression that pools all teaching staff changes, the t-statistic for college attendance is 3.78 (column 1 of panel A of 
Table 5 in this paper). The corresponding t-statistic for test scores is 34.0 (column 2 of Table 5 of the first paper). 
We have much less power here both because the college attendance is only observed for the older half of our sample 
and because college is a much noisier outcome than end-of-grade test scores.
32 For instance, estimating the specification in column 1 of Table 5 with earnings at age 28 as the dependent vari-
able yields a confidence interval of (−$581, $665), which contains both 0 and values nearly twice as large as the 
estimated earnings impacts based on our first research design.


--- Page 34 ---
2666
THE AMERICAN ECONOMIC REVIEW
September 2014
However, given that the cross-class and quasi-experimental designs yield very simi-
lar estimates of ­teachers’ impacts on test scores, college attendance, and college 
quality, we expect the cross-class design to yield unbiased estimates of earnings 
impacts as well.
V.  Heterogeneity of Teachers’ Impacts
In this section, we analyze whether teachers’ impacts vary across demographic 
groups, subjects, and grades. Because analyzing subgroup heterogeneity requires 
considerable statistical precision, we use the first research design—comparisons 
across classrooms conditional on observables—which as noted above relies on a 
stronger identification assumption but is bolstered by generating results which are 
similar to the quasi-experimental design in the full sample. We estimate impacts on 
college quality at age 20 (rather than earnings at age 28) to maximize precision and 
obtain a quantitative metric based on projected earnings gains.
A. Demographic Groups
In panel A of Table 6, we study the heterogeneity of teachers’ impacts across 
demographic subgroups. Each value reported in the first row of the table is a coef-
ficient estimate from a separate regression of college quality on teacher VA con-
ditional on controls. To be conservative, we include both student characteristics ​
X​ct​ and parent characteristics ​P​ ct​ 
∗ ​ in the control vector throughout this section and 
estimate specifications analogous to column 5 of Table 2 on various subsamples. 
Columns 1 and 2 consider heterogeneity by gender. Columns 3 and 4 consider het-
erogeneity by parental income, dividing students into groups above and below the 
median level of parent income in the sample. Columns 5 and 6 split the sample into 
minority and non-minority students.
Two lessons emerge from panel A of Table 6. First, the point estimates of the 
impacts of teacher VA are larger for females than males, although we cannot reject 
equality of the impacts ( p = 0.102). Second, the impacts are larger for higher-
income and non-minority households in absolute terms. For instance, a one stan-
dard deviation increase in VA raises college quality by $190 for children whose 
parents have below-median income, compared with $380 for those whose parents 
have above-median income. However, the impacts are more similar as a percentage 
of mean college quality: 0.80 percent for low-income students versus 1.25 percent 
for high-income students.
The larger absolute impact for high socioeconomic students could be driven by 
two channels: a given increase in teacher VA could have larger impacts on the test 
scores of high SES students or a given increase in scores could have larger long-
term impacts. The second row of coefficient estimates of panel A of Table 6 shows 
that a one standard deviation increase in teacher VA raises test scores by approxi-
mately 0.13 standard deviations on average in all the subgroups, consistent with 
the findings of Lockwood and McCaffrey (2009). In contrast, the cross-sectional 
correlation between scores and college quality is significantly larger for higher SES 
students (online Appendix Table 5). Although not conclusive, these findings sug-
gest that the heterogeneity in teachers’ long term impacts is driven by the second 


--- Page 35 ---
2667
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
­mechanism, namely that high SES students’ earnings are more sensitive to test score 
gains.33 Overall, the heterogeneity in treatment effects on college quality suggests 
that teacher quality—at least as measured by test-score VA—is complementary to 
family inputs and resources. This result implies that higher income families should 
be willing to pay more for high-VA teachers.
B. Subjects: Math versus English
Panel B of Table 6 analyzes differences in teachers’ impacts across subjects. 
For these regressions, we split the sample into elementary (columns 1–3) schools 
and middle (columns 4–5) schools. This distinction is important because students 
33 Importantly, the relationship between college quality and test scores conditional on prior characteristics ​X​it​ 
is linear throughout the test score distribution (online Appendix Figure 2b). Hence, the heterogeneity is not due to 
non-linearities in the relationship between scores and college outcomes but rather the fact that the same increase in 
scores translates to a bigger change in college outcomes for high-SES families.
Table 6— Heterogeneity in Impacts of Teacher Value-Added
College quality at age 20 ($)
Female
Male
Low income High income
Minority
Non-minority
(1)
(2)
(3)
(4)
(5)
(6)
Panel A. Impacts by demographic group
Teacher VA
290.65
237.93
190.24
379.89
215.51
441.08
(23.61)
(21.94)
(19.63)
(27.03)
(17.09)
(42.26)
Mean of dep. var.
27,584
26,073
23,790
30,330
23,831
33,968
Impact as percent of mean
1.05%
0.91%
0.80%
1.25%
0.90%
1.30%
Dep. var.: Test score (SD)
Teacher VA
0.135
0.136
0.128
0.129
0.136
0.138
(0.001)
(0.001)
(0.001)
(0.001)
(0.001)
(0.001)
Mean of dep. var.
0.196
0.158
−0.003
0.331
−0.039
0.651
College quality at age 20 ($)
Elementary school
Middle school
(1)
(2)
(3)
(4)
(5)
Panel B. Impacts by subject
Math teacher VA
207.81
106.34
265.59
(21.77)
(28.50)
(43.03)
English teacher VA
258.16
189.24
521.61
(25.42)
(33.07)
(63.67)
Control for average VA
  in other subject
X
X
Notes: In the first row of estimates in panel A, we replicate the specification in column 5 of Table 2 within vari-
ous population subgroups. In columns 1 and 2, we split the sample between males and females; in columns 3 and 
4, we split the sample based on the median parent household income (which is $31,905); in columns 5 and 6, we 
split the sample based on whether a student belongs to an ethnic minority (Black or Hispanic). In the second row 
of estimates in panel A, we replicate all of the regressions from the first row replacing college quality with score 
as the dependent variable. In panel B, we split the sample into elementary schools (where the student is taught by 
the same teacher for both math and English) and middle schools (which have different teachers for each subject). 
Columns 1 and 2 replicate the specification in column 5 of Table 2, splitting the sample by subject. In column 3, we 
regress college quality on measures of math teacher value-added and English teacher value-added together in a data-
set reshaped to have one row per student by school year. We restrict the sample so that the number of teacher-year 
observations is identical in columns 1–3. Columns 4 and 5 replicate column 5 of Table 2 for middle schools with an 
additional control for the average teacher value-added in the other subject for students in a given class.


--- Page 36 ---
2668
THE AMERICAN ECONOMIC REVIEW
September 2014
have the same teacher for both subjects in elementary school but not middle 
school.
In column 1, we replicate the baseline specification in column 5 of Table 2, 
restricting the sample to math classrooms in elementary school. Column 2 repeats 
this specification for English. In column 3, we include each teacher’s math and 
English VA together in the same specification, reshaping the dataset to have one 
row for each student-year (rather than one row per student-subject-year, as in 
previous regressions). Because a given teacher’s math and English VA are highly 
correlated (r = 0.6), the magnitude of the two subject-specific coefficients drops 
by an average of 40 percent when included together in a single regression for 
elementary school. Intuitively, when math VA is included by itself in elementary 
school, it partly picks up the effect of having better teaching in English as well.
We find that a one standard deviation increase in teacher VA in English has larger 
impacts on college quality than a one standard deviation improvement in teacher 
VA in math. This is despite the fact that the variance of teacher effects in terms of 
test scores is larger in math than English. In Table 2 of our companion paper, we 
estimated that the standard deviation of teacher effects on student test scores in 
elementary school is 0.124 in English and 0.163 in math. Using the estimates from 
column 3 of panel B in Table 6, this implies that an English teacher who raises her 
students’ test scores by one standard deviation raises college quality by ​ 189/0.124 
_ 
106/0.163 ​  
= 2.3 times as much as a math teacher who generates a commensurate test score 
gain. Hence, the returns to better performance in English are especially large, 
although it is much harder for teachers to improve students’ achievement in 
English (e.g., Hanushek and Rivkin 2010; Kane et al. 2013).
We find a similar pattern in middle school. In column 4 of panel B, we repli-
cate the baseline specification for the subset of observations in math in middle 
school. We control for teacher VA in English when estimating this specification 
by residualizing college quality ​Y​ it​ 
∗ ​ with respect to the student and parent class-
level control vectors ​X​ct​ and ​P​ it​ 
∗ ​ as well as ​  
m​jt​ in English using a regression with 
math teacher fixed effects as in (3). Column 5 of panel B in Table 6 replicates 
the same regression for observations in English in middle school, controlling for 
math teacher VA. A one standard deviation improvement in English teacher qual-
ity raises college quality by roughly twice as much as a one standard deviation 
improvement in math teacher quality. Even though teachers have much smaller 
impacts on English test scores than math test scores, the small improvements 
that good teachers generate in English are associated with substantial long-term 
impacts.
C. Impacts of Teachers by Grade
We estimate the impact of a one standard deviation improvement in teacher qual-
ity in each grade g ∈ [4, 8] on college quality (​κ​g​) by estimating the specification 
in column 5 of Table 2 but interacting ​  
m​jt​ with grade dummies. Because the school 
­district data system did not cover many middle schools in the early and mid 1990s, 
we cannot analyze the impacts of teachers in grades 6–8 for more than half the 
students who are in grade 4 before 1994. To obtain a more balanced sample for 


--- Page 37 ---
2669
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
comparisons across grades, we restrict attention to cohorts who would have been in 
grade 4 during or after 1994 in this subsection.34
The series in circles in Figure 7 plots the estimates of ​κ​g​, which are also reported 
in online Appendix Table 11. We find that teachers’ long-term impacts are large 
and significant in all grades. Although the estimates in each grade have relatively 
wide confidence intervals, there is no systematic trend in the impacts. This pattern 
is consistent with the cross-sectional correlations between test scores and adult out-
comes, which are also relatively stable across grades (online Appendix Table 6). 
One issue which complicates cross-grade comparisons is that teachers spend almost 
the entire school day with their students in elementary school (grades 4–5 as well 
as 6 in some schools), but only their subject period (math or English) in middle 
school (grades 7–8). If teachers’ skills are correlated across subjects—as is the case 
with math and English value-added, which have a correlation of 0.6 for elementary 
34 Restricting the sample in the same way does not affect the conclusions above about heterogeneity across sub-
jects or demographic groups, because these groups are balanced across cohorts.
0
200
400
600
800
4
5
6
7
8
Impact of 1 SD of VA on college quality ($)
Grade
Reduced-form coefficients
95% Ci for reduced-form
Net of teacher tracking
Figure 7. Impacts of Teacher Value-Added on College Quality by Grade
Notes: This figure plots the impact of a one standard deviation increase in teacher VA in each 
grade from 4 to 8 on our earnings-based index of college quality (defined in the notes to 
Table 1). The upper (circle) series shows the reduced-form effect of improved teacher quality 
in each grade, including both the direct impact of the teacher on earnings and the indirect effect 
through improved teacher quality in future years. To generate this series, we replicate column 5 
of Table 2, interacting VA with grade dummies. We restrict the sample to cohorts who would 
have been in fourth grade during or after 1994 to obtain a balanced sample across grades. 
The dots in the series plot the coefficients on each grade interaction. The dashed lines show 
the boundaries of the 95 percent confidence intervals for the reduced-form effects, clustering 
the standard errors by school-cohort. The lower (triangle) series plots the impact of teachers 
in each grade on college quality netting out the impacts of increased future teacher quality. 
We net out the effects of future teachers using the tracking coefficients reported in Appendix 
Table 12 and solving the system of equations in Section VC. Online Appendix Table 11 reports 
the reduced-form effects and net-of-future-teachers effects plotted in this figure.


--- Page 38 ---
2670
THE AMERICAN ECONOMIC REVIEW
September 2014
school ­teachers—then a high-VA teacher should have a greater impact on earnings 
in elementary school than middle school because they spend more time with the 
student. Hence, the fact that high-VA math and English teachers continue to have 
substantial impacts in middle school indicates that education has substantial returns 
well beyond early childhood.
Tracking and Net Impacts.—The reduced-form estimates of ​κ​g​ reported above 
include the impacts of being tracked to a better teacher in subsequent grades, as 
discussed in Section I. While a parent may be interested in the reduced-form impact 
of teacher VA in grade g, a policy that raises teacher quality in grade g will not allow 
every child to get a better teacher in grade g + 1. We now turn to identifying teach-
ers’ net impacts ​˜ κ ​g​ in each grade, holding fixed future teachers’ test-score VA.
Because we have no data after grade 8, we can only estimate teachers’ net effects 
holding fixed teacher quality up to grade 8. We therefore set ​˜ κ ​8​ = ​κ​8​. We recover ​˜ κ ​g​ 
from estimates of ​κ​g​ by subtracting out the impacts of future teachers on earnings 
iteratively. The net impact of a seventh grade teacher is her reduced-form impact ​κ​ 7​ 
minus her indirect impact via tracking to a better eighth grade teacher:
(12)  	
​˜ κ ​ 7​  =  ​κ​ 7​  −  ​ρ​ 78​ ​˜ κ ​ 8​,
where ​ρ​ 78​ is the extent to which teacher VA in grade 7 increases teacher VA in 
grade 8 conditional on controls. If we observed each teacher’s true VA, we could 
estimate ​ρ​ 78​ using an OLS regression that parallels (8) with the future teacher’s true 
VA as the dependent variable:
(13)  	
​m​j, ​t​i​(8)​  =  α  +  ​ρ​ 78​ ​  
m​j, ​t​i​(7)​  +  ​γ​1​X​ct​  +  ​γ​2​P​ ct​ 
∗ ​  +  ​η​ ct 78​ 
μ  ​.
Since true VA is unobserved, we substitute VA estimates ​  
m​j, ​t​i​ (8)​ for ​m​j, ​t​i​ (8)​ on the 
left-hand side of (13). This yields an attenuated estimate of ​ρ​78​ because ​  
m​j, ​t​i​ (8)​ is 
shrunk toward zero to account for estimation error (see Section IB of our companion 
paper). If all teachers taught the same number of classes and had the same number 
of students, the shrinkage factor would not vary across observations. In this case, we 
could identify ​ρ​ 78​ by using ​  
m​j, ​t​i​ (8)​ as the dependent variable in (13) and multiply-
ing the estimate of ​ρ​ 78​ by SD(​m​jt​)/SD(​  
m​jt​) = 1/SD(​  
m​jt​). In the sample for which 
we observe college attendance, the standard deviation of teacher VA estimates is 
SD(​  
m​jt​) = 0.61. We therefore multiply the estimate of ​ρ​78​ and all the other tracking 
coefficients ​ρ​g​g​ ′​​ by 1/SD(​  
m​jt​) = 1.63. This simple approach to correcting for the 
attenuation bias is an approximation because the shrinkage factor does vary across 
observations. However, as we discuss below, the magnitude of the tracking coef-
ficients is small and hence further adjusting for the variation in shrinkage factors is 
unlikely to affect our conclusions.
We estimate ​ρ​ 78​ in three steps. First, we residualize ​  
m​j, ​t​i​ (8)​ with respect to the 
controls by regressing ​  
m​j, ​t​i​(8)​ on ​X​ct​ and ​P​ ct​ 
∗ ​ with grade 7 teacher fixed effects, as in 
(3). Second, we run a univariate OLS regression of the residuals on ​  
m​j, ​t​i​( 7 )​​. Finally, 
we multiply this coefficient by 1/SD(​  
m​jt​) = 1.63 to obtain an estimate of ​  ρ​ 78​. Using 
our estimate of ​  ρ​ 78​, we apply (12) to identify ​˜ κ ​7​ from the reduced-form estimates of 


--- Page 39 ---
2671
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
​κ​g​ in Figure 7. Iterating backwards, we calculate ​κ​6​ by estimating ​  ρ​68​ and ​  ρ​67​ and so 
on until we obtain the full set of net impacts. We show formally that this procedure 
recovers net impacts ​˜ κ ​g​ in online Appendix C.
The series in triangles in Figure 7 plots the estimates of the net impacts ​˜ κ ​g​. The 
differences between the net impacts and reduced-form impacts are modest: the mean 
difference between ​κ​g​ and ​˜ κ ​g​ is 19.6 percent in grades 4–7. This is because the track-
ing coefficients ​ρ​g, ​g​ ′​​ are small. A one standard deviation increase in current teacher 
quality typically raises future teacher quality by about 0.05 standard deviations or 
less in elementary school and by about 0.25 standard deviations in middle school 
(online Appendix Table 12). The greater degree of tracking in middle school is con-
sistent with the availability of subject-specific honors classes in grades 7 and 8.
These results suggest that the vast majority of the reduced-form impacts esti-
mated above reflect a teacher’s own direct impact rather than the impacts of being 
tracked to better teachers in later grades. However, we caution that this approach to 
calculating teachers’ net impacts has three important limitations. First, it assumes 
that all tracking to future teachers occurs exclusively via teachers’ ­test-score VA. 
We allow students who have high-VA teachers in grade g to be tracked to higher 
test-score VA (​m​jt​) teachers in grade g + 1, but not to teachers with higher earn-
ings VA ​μ​ jt​ 
Y ​. We are forced to make this strong assumption because we have no way 
to estimate teacher impacts on earnings that are orthogonal to VA, as discussed in 
Section I. Second, ​ ˜ κ​g​ does not net out potential changes in other factors besides 
teachers, such as peer quality or parental inputs. Hence, ​ ˜ κ​g​ cannot be interpreted as 
the structural impact of teacher quality holding fixed all other inputs in a general 
model of the education production function (e.g., Todd and Wolpin 2003). Finally, 
our approach assumes that teacher effects are additive across grades. We cannot 
identify complementarities in teacher VA across grades because our identification 
strategy forces us to condition on lagged test scores, which are endogenous to the 
prior teacher’s quality. It would be very useful to relax these assumptions in future 
work to obtain a better understanding of how the sequence of teachers a child has 
affects her outcomes in adulthood.
VI.  Policy Analysis
In this section, we use our estimates to predict the potential earnings gains from 
selecting and retaining teachers on the basis of their VA. We make four assumptions 
in our calculations. First, we assume that the percentage impact of a one standard 
deviation improvement in teacher VA on earnings observed at age 28 is constant at 
b = 1.34 percent (Table 3, column 2) over the life cycle.35 Second, we ignore gen-
eral equilibrium effects which may reduce wage rates if all children are better edu-
cated. Third, we follow Krueger (1999) and discount earnings gains at a 3 ­percent 
real annual rate (consistent with a 5  percent discount rate and 2  percent wage 
growth) back to age 12, the average age in our sample. Under this assumption, the 
present value of earnings at age 12 for the average individual in the US ­population 
35 We have inadequate precision to estimate wage earnings impacts separately by subject and grade level. We 
therefore assume that a one standard deviation improvement in teacher VA raises earnings by 1.34 percent in all 
subjects and grade levels in the calculations that follow.


--- Page 40 ---
2672
THE AMERICAN ECONOMIC REVIEW
September 2014
is $522, 000 in 2010 dollars, as noted above. Finally, we assume that teacher 
VA ​m​jt​ is normally distributed.
To quantify the value of improving teacher quality, we evaluate Hanushek’s 
(2009, 2011) proposal to replace teachers whose VA ratings are in the bottom 5 per-
cent of the distribution with teachers of average quality. To simplify exposition, we 
calculate these impacts for elementary school teachers, who teach one classroom 
per year. We first calculate the earnings gains from selecting teachers based on their 
true test-score VA ​m​jt​ and then calculate the gains from selecting teachers based on 
VA estimates ​  
m​jt​. Selection on true VA is informative as a benchmark for the poten-
tial gains from improving teacher quality, but is not a feasible policy because we 
only observe estimates of teacher VA in practice. Hence, it is useful to compare the 
gains from policies that select teachers based on VA estimates using a few years of 
performance data with the maximum attainable gain if one were to select teachers 
based on true VA.
Selection on True VA.—Elementary school teachers teach both math and English 
and therefore have two separate VA measures on which they could be evaluated. 
For simplicity, we assume that teachers are evaluated based purely on their VA in 
one subject (say math) and VA in the other subject is ignored.36 Consider a stu-
dent whose teacher’s true math VA is Δ​m​σ​ standard deviations below the mean. 
Replacing this teacher with a teacher of mean quality (for a single school year) 
would raise the student’s expected earnings by
(14)  	
G  =  Δ​m​σ​  ×  $522, 000  ×  b.
Under the assumption that ​m​jt​ is normally distributed, a teacher in the bottom 5 per-
cent of the true VA distribution is on average 2.063 standard deviations below the 
mean teacher quality. Therefore, replacing a teacher in the bottom 5 percent of the 
VA distribution with an average teacher generates a present value lifetime earnings 
gain per student of
  	
G  =  2.063  ×  $522, 000  ×  1.34%  =  $14, 500.
For a class of average size (28.2), the total NPV (net present value) earnings impact 
from this replacement is ​G​C​ = $407, 000. The undiscounted cumulative life-
time earnings gains from deselection are 5.5 times larger than these present value 
gains ($80,000 per student and $2.25 million per classroom), as shown in online 
Appendix Table 13.37 These simple calculations show that the potential gains from 
36 School districts typically average math and English VA ratings to calculate a single measure of teacher per-
formance in elementary schools (e.g., DC Public Schools 2012). In online Appendix D, we show that the gains 
from evaluating teachers based on mean math and English VA are only 12 percent larger than the gains from using 
information based on only one subject because math and English VA estimates are highly correlated. Therefore, the 
results we report below slightly underestimate the true gains from selection on mean VA.
37 These calculations do not account for the fact that deselected teachers may be replaced by rookie teachers, 
who have lower VA. In our sample, mean test score residuals for students taught by first-year teachers are on 
average 0.05 standard deviations lower (in units of standardized student test scores) than those taught by more 
experienced teachers. Given that the median teacher remains in the district for approximately ten years, account-
ing for the effect of inexperience in the first year would reduce the expected benefits of deselection over a typical 


--- Page 41 ---
2673
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
improving the quality of teaching—whether using selection based on VA, teacher 
training, or other policies—are quite large.38
Selection on Estimated VA.—In practice, we can only select teachers on the basis 
of estimated VA ​  
m​jt​. This reduces the gains from selection for two reasons: (1) esti-
mation error in VA and (2) drift in teacher quality over time. To quantify the impact 
of these realities, suppose we use test score data from years t = 1, … , n to estimate 
teacher VA in school year n + 1. The gain in year n + 1 from replacing the bottom 
5 percent of teachers based on VA estimated using the preceding n years of data is
(15)  	 G(n)  =  −E​[ ​m​j, n+1​ | ​  
m​j, n+1​  <  ​F​ ​  
m​j, n+1​​ 
−1  ​(0.05) ]​  ×  $522, 000  ×  b,
where E​[ ​m​j, n+1​ | ​  
m​j, n+1​ < ​F​ ​  
m​j, n+1​​ 
−1  ​(0.05) ]​ denotes the expected value of ​m​j, n+1​ 
conditional on the teacher’s estimated VA falling below the fifth  percentile. We 
calculate this expected value separately for math and English using Monte Carlo 
simulations as described in online Appendix D.39
Panel A of Figure 8 plots the mean gain per classroom ​G​C​(n) = 28.2 × G(n), 
averaging over math and English, for n = 1, … , 10. The values underlying this 
figure are reported in online Appendix Table 13. The gain from deselecting teach-
ers based on true VA, ​G​C​ = $407, 000, is shown by the horizontal line in the figure. 
The gains from deselecting teachers based on estimated VA are significantly smaller 
because of noise in VA estimates and drift in teacher quality.40 With one year of data, 
the expected gain per class is $226,000, 56 percent of the gain from selecting on 
true VA. The gains grow fairly rapidly with more data in the first three years, but the 
­marginal value of ­additional information is small. With three years of test score data, 
the gain is $266,000, but the gain increases to only $279,000 after ten years. After 
three years, waiting for one more year would increase the gain by $4,000 but has an 
expected cost of $266,000. The marginal gains from obtaining one more year of data 
are outweighed by the expected cost of having a low VA teacher on the staff even 
after the first year (Staiger and Rockoff 2010). Adding data from prior classes yields 
relatively little information about current teacher quality both because of decreasing 
returns to additional observations and drift.41
horizon by ​ 
0.05/10 
_ 
 
2.063 × σ(​m​jt​) ​ = 2 ­percent, where σ(​m​jt​) = 0.14 is the mean standard deviation of teacher effects across 
elementary school subjects in our data (see Table 2 of the first paper).
38 Moreover, these calculations do not include the non-monetary returns to a better education (Oreopoulos and 
Salvanes 2010), such as lower teenage birth rates.
39 Without drift, the formula in (15) reduces to r(n​)​ 1/2​ × 2.063 × $522, 000 × b, where r(n) denotes the reli-
ability of the VA estimate using n years of data, which is straightforward to calculate analytically. The original 
working paper version of our study (Chetty, Friedman, and Rockoff 2011b) used this version of the formula and an 
estimate of b based on a model that did not account for drift.
40 In panel B of Appendix Table 13, we distinguish these two factors by eliminating estimation error and predict-
ing current VA based on past VA instead of past scores. Without estimation error, ​G​C​(1) = $340, 000. Hence, drift 
and estimation error each account for roughly half of the difference between ​G​C​(1) and ​G​C​.
41 We also replicated the simulations using VA estimates that do not account for drift. When the estimation win-
dow n is short, drift has little impact on the weights placed on test scores across years. As a result, drift-unadjusted 
measures yield rankings of teacher quality that are very highly correlated with our measures and thus produce 
similar gains. For instance, selection based on three years of data using VA estimates that do not adjust for drift 
yields gains that are 98 percent as large as those reported above. Hence, while accounting for drift is important for 
evaluating out-of-sample forecasts accurately, it may not be critical for practical policy applications for VA.


--- Page 42 ---
2674
THE AMERICAN ECONOMIC REVIEW
September 2014
Panel A. Earnings impacts in first year after deselection
Gain from deselection on true VA = $406,988
0
100
200
300
400
Lifetime earnings gain per class 
($1,000s)
0
2
4
6
8
10
Number of years used to estimate VA
Panel B. Earnings impact over time
Average 10 year gain = $184,234
Average 10 year gain = $246,744
0
100
200
300
400
Lifetime earnings gain per class
 ($1,000s)
1
2
3
4
5
6
7
8
9
10
11
12
13
School years since teacher was hired
Deselected on estimated VA in year 4
Deselected on true VA in year 4
Figure 8. Earnings Impacts of Deselecting Low Value-Added Teachers
Notes: This figure analyzes the impacts of replacing teachers with VA in the bottom 5 percent with teachers of 
average quality on the present value of lifetime earnings for a single classroom of average size (28.2 students). In 
panel A, the horizontal line shows the hypothetical gain from deselecting the bottom 5 percent of teachers based on 
their true VA in the current school year. The series in circles plots the gains from deselecting teachers on estimated 
VA versus the number of years of prior test score data used to estimate VA. Panel A shows gains for the school year 
immediately after deselection; panel B shows the gains in subsequent school years, which decay over time due to 
drift in teacher quality. The lower series in panel B (in circles) plots the earnings gains in subsequent school years 
from deselecting teachers based on their VA estimate at t = 4, constructed using the past three years of data. The 
first point in this series (at t = 4) corresponds to the third point in panel A by construction. The upper series (in tri-
angles) shows the hypothetical gains obtained from deselecting the bottom 5 percent of teachers based on their true 
VA at t = 4; the first dot in this series matches the value in the horizontal line in panel A. For both series in panel B, 
we also report the unweighted mean gain over the first ten years after deselection. All values in these figures are 
based on our estimate that a one standard deviation increase in true teacher VA increases earnings by 1.34 percent 
(column 2 of Table 3). All calculations assume that teachers teach one class per year and report mean values for 
math and English teachers, which are calculated separately. We calculate earnings gains using Monte Carlo simu-
lations based on our estimates of the teacher VA process as described in Section VI. All values in these figures and 
their undiscounted equivalents are reported in online Appendix Table 13.


--- Page 43 ---
2675
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Drift in Quality over Subsequent School Years.—The values in panel A of Figure 8 
reflect the gains in the first year after the deselection of teachers, based on ​  
m​j, n+1​ in 
school year n + 1. Now consider the impacts of such a policy on the earnings of 
students in a subsequent school year n + m:
  	
G(m, n)  =  −E​[ ​m​j, n+m​ | ​  
m​j, n+1​  <  ​F​ ​  
m​j, n+1​​ 
−1  ​(0.05) ]​  ×  $522, 000  ×  b,
where E​[ ​m​j, n+m​ | ​  
m​j, n+1​ < ​F​ ​  
m​j, n+1​​ 
−1  ​(0.05) ]​ denotes the mean VA of teachers in year 
n + m conditional on having estimated VA in year n + 1 below the fifth percentile.
The lower series in panel B of Figure 8 plots ​G​C​(m, 3) = 28.2 × G(m, 3), the 
gains per class in school year m from deselecting teachers based on their estimated 
VA for year 4 (​  
m​j, 4​), constructed using the first three years of data. The first point 
in this series coincides with the value of $266,000 in panel A reported for n = 3. 
Because teacher quality drifts over time, the gains fall in subsequent school years, 
as some of the teachers who were deselected based on their predicted VA in school 
year n would have reverted toward the mean in subsequent years. Deselection based 
on VA estimates at the end of year three generates an average gain of $184,000 per 
classroom per year over the subsequent ten years, the median survival time in the 
district for teachers who have taught for three years.
The upper series in panel B of Figure 8 plots the analogous gains when teachers 
are deselected based on their true VA ​m​j, 4​ in year four instead of their estimated 
VA ​  
m​j, 4​. The first point in this series coincides with the maximum attainable gain 
of $407,000 shown in panel A. The gains again diminish over time because of drift 
in teacher quality. The average present value gain from deselection based on true 
VA over the subsequent ten years is approximately $250,000 per classroom. This 
corresponds to an undiscounted lifetime earnings gain per classroom of students of 
approximately $1.4 million.
The reason that estimation error and drift do not heavily erode the gains from 
deselection of low VA teachers is that very few of the teachers rated in the bottom 
5 percent turn out to be high-VA teachers. For example, among math teachers in 
elementary school, 3.2 percent of the teachers whose estimated VA based on three 
years of test score data (​  
m​j, 4​ ) is in the bottom 5 percent have true VA ​m​j, 4​ above 
the median. Nevertheless, because VA estimates are not perfect predictors of ​m​jt​, 
there is still substantial room to use other measures—such as principal evalua-
tions or student surveys—to complement VA estimates and improve predictions 
of teacher quality.42
Costs of Teacher Selection.—The calculations above do not account for the costs 
associated with a policy which deselects teachers with the lowest performance rat-
ings. First, they ignore downstream costs that may be required to generate earnings 
gains, most notably the cost associated with higher college attendance rates. Second, 
and more importantly, they ignore the fact that teachers need to be compensated for 
the added employment risk they face from such an evaluation system. Rothstein 
42 Of course, these other measures will also be affected by drift and estimation error. For instance, classroom 
observations have significant noise and may capture transitory fluctuations in teacher quality (Kane et al. 2013).


--- Page 44 ---
2676
THE AMERICAN ECONOMIC REVIEW
September 2014
(2013) estimates the latter cost using a structural model of the labor market for 
teachers. Rothstein estimates that a policy which fires teachers if their estimated VA 
after three years falls below the fifth percentile would require a mean salary increase 
of 1.4 percent to equilibrate the teacher labor market.43 In our sample, mean teacher 
salaries were approximately $50,000, implying that annual salaries would have to 
be raised by approximately $700 for all teachers to compensate them for the addi-
tional risk. Based on our calculations above, the deselection policy would generate 
NPV gains of $184,000 per teacher deselected, or $9,250 for all teachers on average 
(because only 1 out of 20 teachers would actually be deselected). Hence, the esti-
mated gains from this policy are more than ten times larger than the costs. Together 
with the preceding results, Rothstein’s (2013) findings imply that deselecting low-
VA teachers could be a very cost effective policy if the signal quality of VA does not 
fall substantially when used for personnel evaluation.44
Retention of High-VA Teachers.—An alternative approach to improving teacher 
quality that may impose lower costs on teachers is to increase the retention of high-VA 
teachers by paying them bonuses. Using Monte Carlo simulations analogous to those 
above, we estimate that retaining a teacher at the ninety-fifth percentile of the estimated 
VA distribution (using three years of data) for an extra year would yield present value 
earnings gains in the subsequent school year of $522, 000 × 28.2 × 1.34 percent × 
E​[ ​m​j, n+1​ | ​  
m​j, n+1​ = ​F​ ​  
m​j, n+1​​ 
−1  ​(0.95) ]​ = $212, 000. In our data, roughly 9 percent of teach-
ers in their third year do not return to the school district for a fourth year. The attrition 
rate is unrelated to teacher VA, consistent with the findings of Boyd et al. (2008). 
Clotfelter et al. (2008) estimate that a $1,800 bonus payment in North Carolina reduces 
attrition rates by 17 percent. Based on these estimates, a one-time bonus payment of 
$1,800 to high-VA teachers who return for a fourth year would increase retention rates 
in the next year by 1.5 percentage points and generate an average benefit of $3,180. 
The expected benefit of offering a bonus to even an excellent (ninety-fifth percen-
tile) teacher is only modestly larger than the cost because one must pay bonuses to 
(100 − 9)/1.5 ≈ 60 additional teachers for every extra teacher retained.
Replacing ineffective teachers is more cost-effective than attempting to retain 
high-VA teachers because most teachers stay for the following school year and are 
relatively inelastic to salary increases. Of course, increasing the salaries of ­high-VA 
teachers could attract more talented individuals into teaching to begin with or 
increase teacher effort. The preceding calculations do not account for these effects.
VII.  Conclusion
Our first paper (Chetty, Friedman, and Rockoff 2014) showed that existing test-
score value-added measures are a good proxy for a teacher’s ability to raise students’ 
test scores. This paper has shown that the same VA measures are also an informative 
43 In the working paper version of his study, Rothstein (2013) calculates the wage gains needed to compensate 
teachers for a policy that deselects teachers below the twentieth percentile after two years. Jesse Rothstein kindly 
provided the corresponding estimates for the policy analyzed here in personal correspondence.
44 Even if there is erosion, as long as the signal quality of VA is a continuous function of its weight in evaluation 
decisions, one would optimally place non-zero weight on VA, because the net gains would fall from the initial level 
of $184,000 in proportion to the weight on VA.


--- Page 45 ---
2677
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
proxy for teachers’ long-term impacts. Although these findings are encouraging for 
the use of value-added metrics, two important issues must be resolved before one can 
determine how VA should be used for policy.
First, using VA measures to evaluate teachers could induce responses such as teach-
ing to the test or cheating, eroding the signal in VA measures (e.g., Jacob 2005, Neal 
and Schanzenbach 2010).45 One can estimate the magnitude of such effects by repli-
cating the analysis in this paper in a district that evaluates teachers based on their VA. 
If behavioral responses substantially reduce the signal quality of VA, policymakers 
may need to develop metrics which are more robust to such responses, as in Barlevy 
and Neal (2012). For instance, districts may also be able to use data on the persistence 
of test score gains to identify test manipulation and develop a more robust estimate of 
teacher quality, as in Jacob and Levitt (2003).
Second, one should compare the long-term impacts of evaluating teachers on the 
basis of VA to other metrics, such as principal evaluations or classroom observation. 
One can adapt the methods developed in this paper to evaluate these other measures 
of teacher quality. When a teacher who is rated highly by principals enters a school, 
do subsequent cohorts of students have higher college attendance rates and earnings? 
What fraction of a teacher’s long-term impact is captured by test-score VA versus 
other measures of teacher quality? By answering these questions, ultimately one could 
estimate the optimal weighting of available metrics to identify teachers who are most 
successful in improving students’ long-term outcomes.
More generally, there are many aspects of teachers’ long-term impacts that remain 
to be explored and would be helpful in designing education policy. For example, in 
this paper we only identified the impact of a single teacher on long-term outcomes. 
Are teachers’ impacts additive over time? Do good teachers complement or ­substitute 
for each other across years? Similarly, it would be useful to go beyond the mean treat-
ment effects that we have estimated here and determine whether some teachers are 
especially effective in improving lower-tail outcomes or producing stars.
Whether or not VA is ultimately used as a policy tool, our results show that par-
ents should place great value on having their child in the classroom of a high value-
added teacher. Consider a teacher whose true VA is one standard deviation above the 
mean who is contemplating leaving a school. Each child would gain approximately 
$39,000 in total (undiscounted) lifetime earnings from having this teacher instead 
of the median teacher. With an annual discount rate of 5 percent, the parents of a 
classroom of average size should be willing to pay this teacher $200,000 ($7,000 per 
parent) to stay and teach their children during the next school year. Hence, the most 
important lesson of this study is that improving the quality of teaching—whether via 
the use of value-added metrics or other policy levers—is likely to have substantial 
economic and social benefits.
REFERENCES
Aaronson, Daniel, Lisa Barrow, and William Sander. 2007. “Teachers and Student Achievement in 
Chicago Public High Schools.” Journal of Labor Economics 25 (1): 95–135.
45 As we noted above, even in the low-stakes regime we study, some unusually high-VA teachers have test score 
impacts consistent with test manipulation. If such behavior becomes more prevalent when VA is used to evaluate 
teachers, the predictive content of VA as a measure of true teacher quality could be compromised.


--- Page 46 ---
2678
THE AMERICAN ECONOMIC REVIEW
September 2014
Baker, Eva L., Paul E. Barton, Linda Darling-Hammond, Edward Haertel, Helen F. Ladd, Robert 
L. Linn, Diane Ravitch, Richard Rothstein, Richard J. Shavelson, and Lorrie A. Shepard. 2010. 
“Problems with the Use of Student Test Scores to Evaluate Teachers.” Economic Policy Institute 
Briefing Paper 278.
Barlevy, Gadi, and Derek Neal. 2012. “Pay for Percentile.” American Economic Review 102 (5): 
1805–31.
Boyd, Donald, Pamela Grossman, Hamilton Lankford, Susanna Loeb, and James Wyckoff. 2008. 
“Who Leaves? Teacher Attrition and Student Achievement.” National Bureau of Economic 
Research Working Paper 14022.
Cameron, A. Colin, Jonah B. Gelbach, and Douglas L. Miller. 2011. “Robust Inference with Multi-
way Clustering.” Journal of Business and Economic Statistics 29 (2): 238–49.
Carrell, Scott E. and James E. West. 2010. “Does Professor Quality Matter? Evidence from Random 
Assignment of Students to Professors.” Journal of Political Economy 118 (3): 409–32.
Cascio, Elizabeth U., and Douglas O. Staiger. 2012. “Knowledge, Tests, and Fadeout in Educational 
Interventions.” National Bureau of Economic Research Working Paper 18038.
Chamberlain, Gary E. 2013. “Predictive Effects of Teachers and Schools on Test Scores, College 
Attendance, and Earnings.” Proceedings of the National Academy of Sciences 110 (43): 17176–82.
Chetty, Raj, John N. Friedman, Nathaniel Hilger, Emmanuel Saez, Diane Whitmore Schanzenbach, 
and Danny Yagan. 2011. “How Does Your Kindergarten Classroom Affect Your Earnings? Evi-
dence from Project STAR.” Quarterly Journal of Economics 126 (4): 1593–660.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2011a. “New Evidence on the Long-Term 
Impacts of Tax Credits.” Internal Revenue Service, Statistics of Income White Paper.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2011b. “The Long Term Impacts of Teach-
ers: Teacher Value-Added and Student Outcomes in Adulthood.” National Bureau of Economic 
Research Working Paper 17699.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2014. “Measuring the Impacts of Teach-
ers I: Evaluating Bias in Teacher Value-Added Estimates.” American Economic Review 104 (9): 
2593–632.
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2014. “Measuring the Impacts of Teach-
ers II: Teacher Value-Added and Student Outcomes in Adulthood: Dataset.” American Economic 
Review. http://dx.doi.org/10.1257/aer.104.9.2633.
Clotfelter, Charles, Elizabeth Glennie, Helen Ladd, and Jacob Vigdor. 2008. “Would Higher Salaries 
Keep Teachers in High-Poverty Schools? Evidence from a Policy Intervention in North Carolina.” 
Journal of Public Economics 92 (5–6): 1352–70.
Corcoran, Sean P. 2010. Can Teachers be Evaluated by Their Students’ Test Scores? Should they Be? 
The Use of Value-Added Measures of Teacher Effectiveness in Policy and Practice. Providence: 
Annenberg Institute for School Reform at Brown University.
Deming, David. 2009. “Early Childhood Intervention and Life-Cycle Development: Evidence from 
Head Start.” American Economic Journal: Applied Economics 1 (3): 111–34.
District of Columbia Public Schools. 2012. “IMPACT: The District of Columbia Public Schools 
Effectiveness Assessment System for School-Based Personnel.” http://www.nctq.org/docs/
IMPACT.pdf (accessed June 2014).
Gordon, Robert, Thomas J. Kane, and Douglas O. Staiger. 2006. “Identifying Effective Teachers 
Using Performance on the Job.” The Brookings Institution, Hamilton Project Discussion Paper 
2006-01.
Gunderson, Morley K., and Philip Oreopoulos. 2010. “Returns to Education in Developed Coun-
tries.” In International Encyclopedia of Education. Vol. 2, edited by Penelope Peterson, Eva 
Baker, and Barry McGaw, 298–304. Oxford: Elsevier Ltd.
Haider, Steven, and Gary Solon. 2006. “Life-Cycle Variation in the Association Between Current 
and Lifetime Earnings.” American Economic Review 96 (4): 1308–20.
Hanushek, Eric A. 1971. “Teacher Characteristics and Gains in Student Achievement: Estimation 
Using Micro Data.” American Economic Review 61 (2): 280–8.
Hanushek, Eric A. 2009. “Teacher Deselection.” In Creating a New Teaching Profession, edited by 
Dan Goldhaber and Jane Hannaway, 165–80. Washington, DC: Urban Institute Press.
Hanushek, Eric A. 2011. “The Economic Value of Higher Teacher Quality.” Economics of Educa-
tion Review 30 (3): 466–79.
Hanushek, Eric A., and Steven G. Rivkin. 2010. “Generalizations about Using Value-Added Mea-
sures of Teaching Quality.” American Economic Review 100 (2): 267–71.
Heckman, James J., Lena Malofeeva, Rodrigo Pinto, and Peter A. Savelyev. 2010a. “Understand-
ing the Mechanisms Through Which an Influential Early Childhood Program Boosted Adult Out-
comes.” Unpublished.


--- Page 47 ---
2679
Chetty et al. : Long-Term Impacts of Teachers
VOL. 104 NO. 9
Heckman, James J., Seong Hyeok Moon, Rodrigo Pinto, Peter A. Savelyev, and Adam Yavitz. 2010b. 
“Analyzing Social Experiments as Implemented: A Reexamination of the Evidence from the 
HighScope Perry Preschool Program.” Quantitative Economics 1 (1): 1–46.
Internal Revenue Service (IRS). 2010. Document 6961: Calendar Year Projections of Information 
and Withholding Documents for the United States and IRS Campuses 2010-2018. Washington, 
DC: IRS Office of Research, Analysis, and Statistics. 
Jackson, C. Kirabo. 2013 “Non-Cognitive Ability, Test Scores, and Teacher Quality: Evidence from 
9th Grade Teachers in North Carolina.” National Bureau of Economic Research Working Paper 
18624.
Jacob, Brian A. 2005. “Accountability, Incentives and Behavior: The Impact of High-Stakes Testing 
in the Chicago Public Schools.” Journal of Public Economics 89 (5–6): 761–96. 
Jacob, Brian A., Lars Lefgren, and David P. Sims. 2010. “The Persistence of Teacher-Induced Learn-
ing Gains.” Journal of Human Resources 45 (4): 915–43.
Jacob, Brian A., and Steven D. Levitt. 2003. “Rotten Apples: An Investigation Of The Prevalence 
And Predictors Of Teacher Cheating.” The Quarterly Journal of Economics 118 (3): 843–77.
Kane, Thomas J., Daniel F. McCaffrey, Trey Miller, and Douglas O. Staiger. 2013. Have We Identi-
fied Effective Teachers? Validating Measures of Effective Teaching Using Random Assignment. 
Seattle: Bill & Melinda Gates Foundation.
Kane, Thomas J., and Douglas O. Staiger. 2008. “Estimating Teacher Impacts on Student Achieve-
ment: An Experimental Evaluation.” National Bureau of Economic Research Working Paper 
14607.
Krueger, Alan B. 1999. “Experimental Estimates of Education Production Functions.” Quarterly 
Journal of Economics 114 (2): 497–532.
Lockwood, J.R., and Daniel F. McCaffrey. 2009. “Exploring Student-Teacher Interactions in Longi-
tudinal  Achievement Data.” Education Finance and Policy 4 (4): 439–67.
Murnane, Richard J. 1975. The Impact of School Resources on the Learning of Inner City Children. 
Cambridge: Ballinger Publishing Company.
Neal, Derek A., and Diane Whitmore Schanzenbach. 2010. “Left Behind by Design: Proficiency 
Counts and Test-Based Accountability.” Review of Economics and Statistics 92 (2): 263–83.
Oreopoulos, Philip, and Uros Petronijevic. 2013. “Making College Worth It: A Review of Research 
on the Returns to Higher Education.” National Bureau of Economic Research Working Paper 
19053.
Oreopoulos, Philip, and Kjell G. Salvanes. 2010. “Priceless: The Nonpecuniary Benefits of School-
ing.” Journal of Economic Perspectives 25 (1): 159–84.
Rivkin, Steven. G., Eric. A. Hanushek, and John F. Kain. 2005. “Teachers, Schools and Academic 
Achievement.” Econometrica 73 (2): 417–58.
Rockoff, Jonah E. 2004. “The Impact of Individual Teachers on Student Achievement: Evidence 
from Panel Data.” American Economic Review 94 (2): 247–52.
Rothstein, Jesse. 2010. “Teacher Quality in Educational Production: Tracking, Decay, and Student 
Achievement.” Quarterly Journal of Economics 125 (1): 175–214.
Rothstein, 
Jesse. 
2013. 
“Teacher 
Quality 
Policy 
When 
Supply 
Matters.” 
http://eml.berkeley.edu/~jrothst/workingpapers/.
Staiger, Douglas O., and Jonah E. Rockoff. 2010. “Searching for Effective Teachers with Imperfect 
Information.” Journal of Economic Perspectives 24 (3): 97–118.
Todd, Petra E., and Kenneth I. Wolpin. 2003. “On the Specification and Estimation of the Produc-
tion Function for Cognitive Achievement.” The Economic Journal 113 (485): F3–33.
US Census Bureau. 2010. “School Enrollment–Social and Economic Characteristics of Students: 
October 2008, Detailed.” http://www.census.gov/hhes/school/data/cps/index.html.
